# Analysis: Search Functionality & Chatbox Limitations

## Executive Summary

The current Streamlit app has **significant architectural limitations** that severely restrict what can be searched and analyzed:

1. **Search is limited to current week's briefing only** - cannot search across multiple briefings
2. **Chatbox receives only markdown summary** - missing full article data, analysis, and source information
3. **Critical data is lost in markdown parsing** - full article content, scores, entities, and metadata are discarded
4. **No access to cached article data** - the context_retriever module exists but is never used in the UI

This creates two distinct failure modes:
- **Search weakness**: Can only search the markdown summary, not the underlying data
- **Chatbox weakness**: Cannot provide deep analysis without full article context, evidence, and metadata

---

## Part 1: Current Search Implementation

### What Gets Passed to search_articles_with_llm()

**Location**: `app.py`, line 573
```python
response = search_articles_with_llm(
    user_input, 
    briefing.get("content", ""),  # <-- THE FULL MARKDOWN BRIEFING
    st.session_state.language
)
```

### What "content" Contains

The `briefing.get("content")` is the **full markdown briefing file** (examples from actual briefing):

```
# üìä AI Ë°å‰∏öÂë®ÂàäÁÆÄÊä•
## 2025Âπ¥Á¨¨43Âë® (10Êúà20-26Êó•) | ËØ≠‰πâÈáçÂ§çÊ∂àÈô§ÊäÄÊúØÊîØÊåÅ

---

## üìà ÊâßË°åÊëòË¶Å
Êú¨Âë®AIË°å‰∏öÂú®ÈáëËûçÂ∫îÁî®„ÄÅÊï∞ÊçÆÂàÜÊûêÂíåÊú∫Âô®Â≠¶‰π†È¢ÜÂüüÂèñÂæóÈáçË¶ÅËøõÂ±ï...

**1. AIÊäïËµÑÂàÜÊûêÁ≥ªÁªü (AI PB: A Grounded Generative Agent for Personalized Investment Insight)**

‰∏ÄÈ°πÊñ∞ÁöÑÁ†îÁ©∂‰ªãÁªç‰∫ÜAI PBÁ≥ªÁªüÔºåËøôÊòØ‰∏Ä‰∏™‰∏∫‰∏™‰∫∫ÊäïËµÑËÄÖÊèê‰æõ‰∏™ÊÄßÂåñÊäïËµÑÊ¥ûÂØüÁöÑÁîüÊàêÂºèAI‰ª£ÁêÜ„ÄÇ

**Êù•Ê∫ê**: ArXiv
**URL**: https://arxiv.org/abs/2510.20099
**ÂèëË°®Êó∂Èó¥**: 2025Âπ¥10Êúà

[... 4 more articles ...]

## üìä ÊñáÁ´†ÈááÈõÜ‰∏éÈáçÂ§çÊ∂àÈô§ÁªüËÆ°
[Technical stats table...]

## üî¨ ËØ≠‰πâÈáçÂ§çÊ∂àÈô§ÊäÄÊúØÁªÜËäÇ
[System architecture details...]
```

**Content Type**: Plain markdown with limited article information (title, source, URL, brief summary)

### Search Scope Limitation

**Current Behavior:**
```python
def search_articles_with_llm(query: str, briefing_content: str, lang: str = "en") -> str:
    """Use LLM to search and return matching articles"""
    
    system_prompt = f"""You are a helpful AI assistant that searches and retrieves relevant articles.
    
BRIEFING CONTENT:
{briefing_content}"""  # <-- ONLY THIS WEEK'S MARKDOWN IS INCLUDED
    
    response = st.session_state.provider_switcher.query(
        prompt=f"Search for articles related to: {query}",
        system_prompt=system_prompt,
        max_tokens=1024,
        temperature=0.7
    )
```

**Critical Limitation:**
- Only the **currently selected briefing** is passed to search
- The `get_available_briefings()` function returns list of all briefings (line 343)
- But the app **never searches multiple briefings** - only the selected one
- When user clicks "Archive" and selects a different date, search still only works on that single briefing

### Data Filtering Applied

The markdown contains:
- ‚úÖ Article titles
- ‚úÖ Brief 200-character summaries  
- ‚úÖ Source names
- ‚úÖ URLs
- ‚úÖ Publication dates

But markdown parsing (lines 243-307) **only extracts**:
```python
{
    "title": str,
    "summary": str[:200],  # TRUNCATED to 200 chars
    "url": str,
    "source": str
}
```

**Data Lost in Parsing**:
- ‚ùå Full article content (original extracted HTML)
- ‚ùå Paraphrased content (500-700 char analysis)
- ‚ùå Scoring data (impact, relevance, recency, credibility scores)
- ‚ùå Weighted score
- ‚ùå Article categories/classifications
- ‚ùå Entity information (companies, people, models mentioned)
- ‚ùå Fact-check status
- ‚ùå Publication timestamp (not date)
- ‚ùå Article ID

---

## Part 2: Current Chatbox Context

### What Gets Passed to answer_question_about_briefing()

**Location**: `app.py`, line 583
```python
response = answer_question_about_briefing(
    user_input, 
    briefing.get("content", ""),  # <-- SAME MARKDOWN BRIEFING
    st.session_state.language
)
```

### System Prompt Received by LLM

```python
system_prompt = f"""You are a helpful assistant that answers questions 
about an AI industry briefing.
The user will ask questions about the briefing content.
Be concise, accurate, and reference specific articles when relevant.

BRIEFING CONTENT:
{briefing_content}"""  # <-- FULL MARKDOWN
```

### What the LLM Actually Has Access To

**Advantages (what IS available)**:
- Full markdown briefing text with all sections
- Executive summary (2-3 sentences)
- Key insights (3-5 strategic takeaways)
- Article titles, sources, and URLs
- Brief summaries (truncated to 200 chars from markdown parsing)
- Meta information (statistics, sources list, system details)

**Critical Gaps (what is NOT available)**:
- ‚ùå **Full article content** - LLM only sees 200-char truncated summary
- ‚ùå **Detailed analysis** - No paraphrased content (500-700 chars with deep analysis)
- ‚ùå **Evidence & data** - Cannot cite specific statistics or numbers from original articles
- ‚ùå **Scoring context** - Doesn't know which articles are highest impact/credibility
- ‚ùå **Entity information** - Cannot understand which companies/models are mentioned
- ‚ùå **Article structure** - Doesn't know central argument, key findings, implications
- ‚ùå **Comparison data** - Cannot compare articles across weeks
- ‚ùå **Historical context** - Only has this week's data

### Example: What's Missing for Analysis

**User asks**: "Âì™ÁØáÊñáÁ´†ÂÖ≥‰∫éOpenAIÁöÑÊäïËµÑÊÉÖÂÜµÔºü"  
**Current context**: "ÊñáÁ´†Ê†áÈ¢ò: XXX, Êù•Ê∫ê: YYY, ÊëòË¶Å: [200Â≠óÊà™Êñ≠]"  
**Missing**: Full text that explains OpenAI's specific investments, amounts, strategic implications

**User asks**: "Êú¨Âë®ÈáçÁÇπÂ∫îËØ•ÂÖ≥Ê≥®Âì™‰∫õAIÂ∫îÁî®È¢ÜÂüüÔºü"  
**Current context**: Just markdown text about each category  
**Missing**: Article score breakdowns showing which applications scored highest across dimensions (impact, relevance, recency, credibility)

---

## Part 3: Data Loss in Markdown Parsing

### Actual Article Structure in Reports

The `report_formatter.py` shows articles have this structure:

```python
article = {
    'title': str,
    'url': str,
    'source': str,
    'content': str,                    # Full HTML content from scraper
    'summary': str,                    # Short summary
    'paraphrased_content': str,        # 500-700 char analysis
    'category': str,
    'date': str,
    'published_date': str,
    'source_id': str,
    'language': str,
    
    # Scores (multi-dimensional)
    'impact_score': float,
    'relevance_score': float,
    'recency_score': float,
    'credibility_score': float,
    'weighted_score': float,
    
    # Metadata
    'fact_check': str,                 # 'passed' or 'failed'
    'entities': {
        'companies': list,
        'models': list,
        'people': list,
        'locations': list,
        'other': list
    }
}
```

### What Markdown Parser Extracts vs. Discards

**Extraction Logic** (lines 243-307 of app.py):
```python
def parse_articles_from_markdown(content: str) -> List[Dict[str, str]]:
    articles = []
    for article in markdown:
        article_dict = {
            "title": extracted_title,
            "summary": extracted_summary[:200],    # TRUNCATION
            "url": extracted_url,
            "source": extracted_source
        }
        articles.append(article_dict)
    return articles
```

**Loss Matrix**:

| Data | Stored in Reports | In JSON Cache | In Markdown | Parser Extracts | App.py Uses |
|------|-------------------|----------------|-------------|-----------------|-------------|
| Title | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| Source | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| URL | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| Full content | ‚úÖ | ‚úÖ | ‚ùå | - | ‚ùå |
| Paraphrased (500-700) | ‚úÖ | ‚úÖ | ‚ùå | - | ‚ùå |
| Summary (200 char) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ truncates to 200 | ‚úÖ |
| Impact score | ‚úÖ | ‚úÖ | ‚ùå | - | ‚ùå |
| Relevance score | ‚úÖ | ‚úÖ | ‚ùå | - | ‚ùå |
| Recency score | ‚úÖ | ‚úÖ | ‚ùå | - | ‚ùå |
| Credibility score | ‚úÖ | ‚úÖ | ‚ùå | - | ‚ùå |
| Weighted score | ‚úÖ | ‚úÖ | ‚ùå | - | ‚ùå |
| Entities | ‚úÖ | ‚úÖ | ‚ùå | - | ‚ùå |
| Fact check | ‚úÖ | ‚úÖ | ‚ùå | - | ‚ùå |
| Category | ‚úÖ | ‚úÖ | ‚ùå | - | ‚ùå |

---

## Part 4: Unused Infrastructure

### context_retriever.py Exists But Is Unused

The codebase includes a sophisticated `ContextRetriever` class that:

```python
class ContextRetriever:
    def list_available_reports(self) -> List[Dict]:
        """Get all available briefings with metadata"""
        
    def load_report_by_date(self, date: str) -> Dict:
        """Load full report including ALL article data"""
        
    def search_by_keyword(self, keyword: str, date_from: str, date_to: str) -> List:
        """Search across multiple briefings with date range"""
        
    def search_by_entity(self, entity_name: str, entity_type: str, date_from: str, date_to: str) -> List:
        """Search for companies/people/models across multiple briefings"""
        
    def get_article_by_id(self, date: str, article_id: str) -> Dict:
        """Get full article with all fields"""
```

**Current Usage in app.py**: 0 times ‚ùå

This module could provide:
- ‚úÖ Multi-briefing search across date ranges
- ‚úÖ Entity-based search (find articles about "OpenAI" across all weeks)
- ‚úÖ Full article context with all metadata
- ‚úÖ Score-based filtering
- ‚úÖ Entity-type searches (find articles mentioning specific people/companies/models)

---

## Part 5: Architectural Limitations

### Search Limitations Summary

| Aspect | Current | Limitation |
|--------|---------|-----------|
| Briefing scope | Single week | Cannot cross-reference trends |
| Data available | 200-char summary + metadata | Missing deep analysis |
| Date range | Single week only | No historical search |
| Entity search | Not available | Cannot find articles about "OpenAI" across weeks |
| Score filtering | Not available | Cannot filter by impact/relevance |
| Search depth | Markdown text only | No access to full article content |

### Chatbox Limitations Summary

| Aspect | Current | Missing Impact |
|--------|---------|----------------|
| Article context | 200-char summary | Cannot answer "what specific evidence supports this?" |
| Analysis depth | Markdown text | Cannot perform deep analysis of article arguments |
| Comparison | Single week | Cannot compare trends across multiple weeks |
| Evidence citation | Limited | Cannot quote specific sections or statistics |
| Entity tracking | Not available | Cannot answer "which companies are mentioned most?" |
| Scoring context | Not available | Cannot answer "why is this the top article?" |
| Cross-briefing reference | Not possible | Cannot answer "has this topic appeared before?" |

---

## Part 6: Root Causes

### Why These Limitations Exist

1. **Design Choice**: App was built for **single-week display only**, not research/analysis
   - Streamlit app loads one briefing at a time
   - Archive selector loads different briefing file but doesn't aggregate

2. **Data Flow Bottleneck**: Using markdown as source of truth
   - Original data (in JSON cache) is rich and complete
   - Markdown export strips 80% of the data
   - App.py parses markdown, losing more data
   - LLM receives only 20% of available information

3. **Integration Gap**: context_retriever module built but never wired to UI
   - `utils/context_retriever.py` has all the multi-briefing search capability
   - `app.py` doesn't import or use it
   - No bridge between rich cached data and Streamlit interface

4. **Architecture Decision**: 70/30 split favors display over analysis
   - 70% of screen space for brief display
   - 30% for Q&A (but limited by available context)
   - No space for deep research or cross-briefing analysis

---

## Recommendations for Improvement

### High-Impact Improvements

#### 1. **Enable Multi-Briefing Search** (Would solve search scope issue)
```python
def search_articles_across_briefings(
    query: str, 
    date_from: str = None,
    date_to: str = None,
    lang: str = "en"
) -> str:
    """Search across multiple briefings using context_retriever"""
    retriever = ContextRetriever()
    
    # This is already implemented in context_retriever!
    articles = retriever.search_by_keyword(
        keyword=query,
        date_from=date_from,
        date_to=date_to
    )
    
    # Use full article data, not markdown summaries
    return format_search_results(articles)
```

**Impact**: Users can search "OpenAI" and get results from all weeks instead of just current

#### 2. **Load Full Article Context for Chatbox** (Would solve analysis depth)
```python
def get_full_briefing_context(briefing_date: str) -> str:
    """Load full article context instead of just markdown summary"""
    retriever = ContextRetriever()
    report = retriever.load_report_by_date(briefing_date)
    
    context = ""
    for article in report.get("articles", []):
        context += f"""
Article: {article['title']}
Source: {article['source']}
URL: {article['url']}
Impact: {article['impact_score']}/10
Relevance: {article['relevance_score']}/10
Credibility: {article['credibility_score']}/10

Full Analysis:
{article['paraphrased_content']}

Key Entities:
- Companies: {', '.join(article.get('entities', {}).get('companies', []))}
- Models: {', '.join(article.get('entities', {}).get('models', []))}
"""
    return context
```

**Impact**: Chatbox can provide detailed analysis with citations and score-based reasoning

#### 3. **Add Entity Search Tab** (Would solve "find mentions" queries)
```
UI Addition:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Mode: [Search] [Ask] [Find Entity]  ‚îÇ <-- NEW TAB
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Entity Type: [Company] [Model] [Person] [Location]
Entity Name: ___________________
Date Range: From ____ To ____

Results:
- Company "OpenAI" mentioned in 12 articles across 3 weeks
  - Week 43: 4 mentions (impact avg: 8.2)
  - Week 42: 5 mentions (impact avg: 7.8)
  - Week 41: 3 mentions (impact avg: 6.5)
```

**Implementation**: Use `context_retriever.search_by_entity()`

### Medium-Impact Improvements

#### 4. **Display Article Scores in UI**
```
Current:
**1. Article Title**
Summary text...

Improved:
**1. Article Title** [Impact: 8/10] [Credibility: 9/10]
Full analysis text (500-700 chars) instead of summary
Key findings: ...
Entities mentioned: Company1, Company2, Model1
```

#### 5. **Add "Trending Topics" Analysis**
```
Show topic evolution across weeks:
"Agentic AI" 
- Week 43: 3 mentions (impact: 8.2)
- Week 42: 2 mentions (impact: 7.5)
- Week 41: 1 mention (impact: 6.0)
```

#### 6. **Enable Date Range Search**
```python
# Instead of single-week search
response = search_articles_with_llm(query, briefing_content)

# Support multi-week search
response = search_articles_across_briefings(
    query=query,
    date_from="2025-10-01",
    date_to="2025-10-27"
)
```

### Quick Wins (Low-Effort, High-Value)

1. **Pass full paraphrased_content to LLM instead of truncated summary**
   - File: `app.py`, line 573 and 583
   - Change: Use `briefing.get("paraphrased_content")` if available
   - Effort: 2 lines
   - Gain: Better analysis depth immediately

2. **Include article scores in system prompt**
   - File: `app.py`, line 397-402
   - Change: Add "This article scored {score}/10 for impact" in context
   - Effort: 5 lines
   - Gain: LLM understands why articles were selected

3. **Load cache files alongside markdown**
   - File: `app.py`, line 309-341
   - Change: Check for JSON cache after loading markdown
   - Effort: 10 lines
   - Gain: All article metadata becomes available

---

## Conclusion

The current implementation prioritizes **report display** (70% of screen) over **analysis capability** (30% of screen). This creates two distinct weaknesses:

1. **Search is shallow**: Only markdown summaries are searchable, not full article content or metadata
2. **Chatbox lacks context**: Missing full article analysis, scores, and entity information needed for deep Q&A

**The good news**: The underlying infrastructure (`context_retriever.py`, cached JSON data) already supports multi-briefing search and deep analysis. The limitation is purely in the Streamlit app layer not using it.

**Recommended path forward**:
1. Start with quick wins (improve context passed to LLM)
2. Wire up context_retriever to enable multi-briefing search
3. Redesign UI to leverage richer data (show scores, enable entity search)
4. Add trending analysis across time dimensions

This would transform the tool from a "report viewer" into a "research assistant" without changing the underlying data pipeline.

