"""
BriefAI Streamlit Web Application - Unified Chat+Search Interface v3

Provides a professional interface for CEO to:
- View weekly AI industry briefings on the left (30%)
- Search articles or ask questions using unified LLM interface on the right (70%)
- All powered by Kimi/Moonshot with OpenRouter fallback
- Fully bilingual (Chinese/English)

Runs on Streamlit Cloud with automatic provider switching and rate limit handling.
"""

import streamlit as st
import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import os
from utils.provider_switcher import ProviderSwitcher
from utils.context_retriever import ContextRetriever

# ============================================================================
# TRANSLATIONS - UI TEXT IN ENGLISH AND MANDARIN CHINESE
# ============================================================================

TRANSLATIONS = {
    "page_title": {
        "en": "AI Industry Weekly Briefing",
        "zh": "AIË°å‰∏öÂë®Êä•"
    },
    "subtitle": {
        "en": "Executive Summary & Insights",
        "zh": "È´òÁÆ°ÊëòË¶Å‰∏éÊ¥ûÂØü"
    },
    "mode_search": {
        "en": "Search",
        "zh": "ÊêúÁ¥¢"
    },
    "mode_ask": {
        "en": "Ask Question",
        "zh": "ÊèêÈóÆ"
    },
    "unified_input_search": {
        "en": "Search articles...",
        "zh": "ÊêúÁ¥¢ÊñáÁ´†..."
    },
    "unified_input_ask": {
        "en": "Ask a question about the briefing...",
        "zh": "ÊèêÈóÆÂÖ≥‰∫éÁÆÄÊä•..."
    },
    "search_help": {
        "en": "Company/Model/Topic search powered by LLM",
        "zh": "Áî±LLMÈ©±Âä®ÁöÑÂÖ¨Âè∏/Ê®°Âûã/‰∏ªÈ¢òÊêúÁ¥¢"
    },
    "about_brief": {
        "en": "‚ÑπÔ∏è About This Brief",
        "zh": "‚ÑπÔ∏è ÂÖ≥‰∫éÊ≠§ÁÆÄÊä•"
    },
    "about_description": {
        "en": "This briefing features the top 10 AI industry articles this week, selected by impact and novelty.",
        "zh": "Ê≠§ÁÆÄÊä•Â±ïÁ§∫Êú¨Âë®ÊåâÂΩ±ÂìçÂíåÊñ∞È¢ñÊÄßÈÄâÊã©ÁöÑÂâç10ÁØáAIË°å‰∏öÊñáÁ´†„ÄÇ"
    },
    "report_date": {
        "en": "Report Date",
        "zh": "Êä•ÂëäÊó•Êúü"
    },
    "last_updated": {
        "en": "Last Updated",
        "zh": "ÊúÄÂêéÊõ¥Êñ∞"
    },
    "articles": {
        "en": "Articles",
        "zh": "ÊñáÁ´†"
    },
    "executive_summary": {
        "en": "üìä Executive Summary",
        "zh": "üìä È´òÁÆ°ÊëòË¶Å"
    },
    "language": {
        "en": "üåê Language",
        "zh": "üåê ËØ≠Ë®Ä"
    },
    "download": {
        "en": "‚¨áÔ∏è Download as Markdown",
        "zh": "‚¨áÔ∏è ‰∏ãËΩΩMarkdown"
    },
    "briefing": {
        "en": "Briefing",
        "zh": "ÁÆÄÊä•"
    },
    "download_tab": {
        "en": "Download",
        "zh": "‰∏ãËΩΩ"
    },
    "chat_error": {
        "en": "Error answering question",
        "zh": "ÂõûÁ≠îÈóÆÈ¢òÊó∂Âá∫Èîô"
    },
    "search_results_title": {
        "en": "üìÑ Search Results",
        "zh": "üìÑ ÊêúÁ¥¢ÁªìÊûú"
    },
    "no_results": {
        "en": "No articles matched your search",
        "zh": "Ê≤°ÊúâÊñáÁ´†‰∏éÊÇ®ÁöÑÊêúÁ¥¢ÂåπÈÖç"
    },
    "ai_response": {
        "en": "üí¨ AI Response",
        "zh": "üí¨ AIÂõûÂ§ç"
    },
    "your_question": {
        "en": "You",
        "zh": "‰Ω†"
    },
    "ai_assistant": {
        "en": "Assistant",
        "zh": "Âä©Êâã"
    },
    "multi_week_search": {
        "en": "Multi-Week Search",
        "zh": "Â§öÂë®ÊêúÁ¥¢"
    },
    "entity_search": {
        "en": "Entity Search",
        "zh": "ÂÆû‰ΩìÊêúÁ¥¢"
    },
    "date_range": {
        "en": "Date Range",
        "zh": "Êó•ÊúüËåÉÂõ¥"
    },
    "entity_type": {
        "en": "Entity Type",
        "zh": "ÂÆû‰ΩìÁ±ªÂûã"
    },
    "companies": {
        "en": "Companies/Models",
        "zh": "ÂÖ¨Âè∏/Ê®°Âûã"
    },
    "people": {
        "en": "People",
        "zh": "‰∫∫Áâ©"
    },
    "locations": {
        "en": "Locations",
        "zh": "Âú∞ÁÇπ"
    },
    "other": {
        "en": "Other",
        "zh": "ÂÖ∂‰ªñ"
    },
}

def t(key: str, lang: str = "en", **kwargs) -> str:
    """Get translated text"""
    text = TRANSLATIONS.get(key, {}).get(lang, TRANSLATIONS.get(key, {}).get("en", key))
    return text.format(**kwargs) if kwargs else text

# ============================================================================
# PAGE CONFIGURATION
# ============================================================================

st.set_page_config(
    page_title="AI Industry Weekly Briefing",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# Initialize session state for language and chat
if "language" not in st.session_state:
    st.session_state.language = "zh"
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "current_mode" not in st.session_state:
    st.session_state.current_mode = "search"
if "search_query" not in st.session_state:
    st.session_state.search_query = ""
if "search_results" not in st.session_state:
    st.session_state.search_results = None
if "selected_briefing" not in st.session_state:
    st.session_state.selected_briefing = None
if "provider_switcher" not in st.session_state:
    try:
        st.session_state.provider_switcher = ProviderSwitcher()
    except Exception as e:
        st.error(f"Failed to initialize LLM provider: {e}")
        st.session_state.provider_switcher = None

# ============================================================================
# CUSTOM STYLING
# ============================================================================

st.markdown("""
    <style>
    .main-title {
        font-size: 2.5em;
        font-weight: bold;
        margin-bottom: 0.3em;
        color: #1f77b4;
    }
    .subtitle-text {
        font-size: 1.1em;
        color: #555;
        margin-bottom: 1em;
    }
    .article-card {
        background-color: #f0f2f6;
        padding: 1.2em;
        border-radius: 0.5em;
        margin: 0.8em 0;
        border-left: 4px solid #1f77b4;
    }
    .article-title {
        font-size: 1.1em;
        font-weight: bold;
        color: #1f77b4;
        margin-bottom: 0.5em;
    }
    .article-meta {
        font-size: 0.85em;
        color: #666;
        margin-bottom: 0.5em;
    }
    .article-summary {
        font-size: 0.95em;
        line-height: 1.5;
        color: #333;
    }
    .article-url {
        font-size: 0.85em;
        color: #1f77b4;
        text-decoration: none;
    }
    .chat-message {
        padding: 1em;
        border-radius: 0.5em;
        margin: 0.8em 0;
    }
    .chat-user {
        background-color: #e8f4f8;
        margin-left: 1em;
    }
    .chat-ai {
        background-color: #f0f0f0;
        margin-right: 1em;
    }
    .mode-selector {
        margin: 1em 0;
        padding: 1em;
        background-color: #f9f9f9;
        border-radius: 0.5em;
    }
    .search-result-item {
        background-color: #fafafa;
        padding: 1em;
        margin: 0.5em 0;
        border-left: 3px solid #1f77b4;
        border-radius: 0.3em;
    }
    .relevance-score {
        display: inline-block;
        background-color: #1f77b4;
        color: white;
        padding: 0.2em 0.5em;
        border-radius: 0.25em;
        font-size: 0.85em;
        margin-top: 0.5em;
    }
    </style>
""", unsafe_allow_html=True)

# ============================================================================
# LOAD BRIEFING DATA
# ============================================================================

def parse_articles_from_markdown(content: str) -> List[Dict[str, str]]:
    """Parse articles from markdown briefing content
    Supports two formats:
    1. Old format: **1. Title** with numbered articles
    2. New format: ### „ÄêCategory„ÄëTitle with section headers
    """
    articles = []
    lines = content.split('\n')
    i = 0

    while i < len(lines):
        line = lines[i]

        # NEW FORMAT: Look for "### „ÄêCategory„ÄëTitle" pattern
        if line.startswith('###') and '„Äë' in line:
            # Extract title - everything after the „Äë
            if '„Äë' in line:
                title = line.split('„Äë', 1)[1].strip()
            else:
                title = line.replace('###', '').strip()

            summary = ""
            url = ""
            source = ""

            i += 1
            # Collect lines until we hit next article or end
            while i < len(lines):
                current_line = lines[i].strip()

                # Stop if we hit the next article (### marker)
                if current_line.startswith('###'):
                    break

                # Stop if we hit major sections like "## üîç", "## üìà"
                if current_line.startswith('##'):
                    break

                # Extract source - look for "**Êù•Ê∫ê**: value" or "**Êù•Ê∫ê**:" patterns
                if '**Êù•Ê∫ê**' in current_line and ':' in current_line:
                    # Extract everything after the colon
                    parts = current_line.split(':', 1)
                    if len(parts) > 1:
                        source = parts[1].strip()
                        # Clean up markdown formatting
                        source = source.split('|')[0].strip()
                elif current_line.startswith('**Êù•Ê∫ê'):
                    # Fallback pattern
                    parts = current_line.split(':', 1)
                    if len(parts) > 1:
                        source = parts[1].strip()

                # Extract URL - look for patterns like "**Êù•Ê∫êÈìæÊé•**: [text](url)"
                elif '**Êù•Ê∫êÈìæÊé•**' in current_line or 'URL:' in current_line:
                    if '[' in current_line and '](http' in current_line:
                        # Extract URL from markdown link
                        try:
                            url = current_line.split('](')[1].split(')')[0]
                        except:
                            pass
                    else:
                        parts = current_line.split(':', 1)
                        if len(parts) > 1:
                            url = parts[1].strip()

                # Extract score/rating if present
                elif '**ËØÑÂàÜ**' in current_line:
                    # Line like "**ËØÑÂàÜ**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **7.3/10** | ..."
                    pass  # Just skip for now

                # First non-empty non-metadata line is the summary
                elif summary == "" and current_line and not current_line.startswith('**') and not current_line.startswith('„Äê'):
                    # Skip lines that are just emojis or metadata
                    if current_line and not current_line.startswith('‚≠ê') and len(current_line) > 10:
                        summary = current_line[:200]  # Truncate to 200 chars

                i += 1

            if title.strip():
                articles.append({
                    "title": title.strip(),
                    "summary": summary if summary else "Êó†ÊëòË¶Å",
                    "url": url if url else "",
                    "source": source if source else ""
                })
            continue

        # OLD FORMAT: Look for article titles (numbered like "**1. Title**")
        elif line.startswith('**') and len(line) > 2 and line[2].isdigit() and '. ' in line:
            # Extract title - remove asterisks and number prefix
            title_raw = line.strip('*').strip()
            # Remove the number and dot prefix (e.g., "1. " or "10. ")
            if '. ' in title_raw:
                title = title_raw.split('. ', 1)[1]
            else:
                title = title_raw

            summary = ""
            url = ""
            source = ""

            i += 1
            # Collect lines until we hit next article or end
            while i < len(lines):
                current_line = lines[i].strip()

                # Stop if we hit the next article
                if current_line.startswith('**') and len(current_line) > 2 and current_line[2].isdigit():
                    break

                # Extract source
                if '**Êù•Ê∫ê**' in current_line and ':' in current_line:
                    source = current_line.split(':', 1)[1].strip()
                elif current_line.startswith('**Êù•Ê∫ê'):
                    parts = current_line.split(':', 1)
                    if len(parts) > 1:
                        source = parts[1].strip()

                # Extract URL
                elif '**URL**' in current_line and ':' in current_line:
                    url = current_line.split(':', 1)[1].strip()
                elif 'URL:' in current_line:
                    url = current_line.split(':', 1)[1].strip()

                # First non-empty non-metadata line is the summary
                elif summary == "" and current_line and not current_line.startswith('**'):
                    summary = current_line

                i += 1

            if title.strip():
                articles.append({
                    "title": title.strip(),
                    "summary": summary[:200] if summary else "Êó†ÊëòË¶Å",
                    "url": url if url else "",
                    "source": source if source else ""
                })
            continue

        i += 1

    return articles

def create_enriched_briefing_context(articles: List[Dict[str, str]]) -> str:
    """
    Create enriched context for LLM with full article information
    Includes article summaries, sources, URLs, and explanations
    """
    if not articles:
        return "No articles available."

    context_lines = []
    context_lines.append("# Êú¨Âë®Á≤æÈÄâÊñáÁ´†\n")

    for idx, article in enumerate(articles, 1):
        context_lines.append(f"## {idx}. {article.get('title', 'Untitled')}")
        context_lines.append("")

        if article.get('summary'):
            context_lines.append(article['summary'])
            context_lines.append("")

        if article.get('source') or article.get('url'):
            meta_parts = []
            if article.get('source'):
                meta_parts.append(f"Êù•Ê∫ê: {article['source']}")
            if article.get('url'):
                meta_parts.append(f"URL: {article['url']}")
            context_lines.append(" | ".join(meta_parts))
            context_lines.append("")

    context_lines.append("\n---\n")
    context_lines.append("‰ΩøÁî®ËØ¥Êòé:")
    context_lines.append("- ÂàÜÊûêÊñáÁ´†Êó∂ÔºåËØ∑ÂèÇËÄÉÂÆåÊï¥ÂÜÖÂÆπ")
    context_lines.append("- ÊâæÂá∫ÊØèÁØáÊñáÁ´†ÁöÑ‰∏≠ÂøÉËÆ∫ÁÇπÔºàcentral argumentÔºâ")
    context_lines.append("- ÊåáÂá∫ÊîØÊíëËÆ∫ÁÇπÁöÑÊï∞ÊçÆÂíåËØÅÊçÆÔºàdata and evidenceÔºâ")
    context_lines.append("- Â¶ÇÊûúÁî®Êà∑Ë¶ÅÊ±ÇÔºåÂèØ‰ª•‰ªéURLËé∑ÂèñÂÆåÊï¥ÊñáÁ´†ËøõË°åÊõ¥Ê∑±ÂÖ•ÂàÜÊûê")

    return "\n".join(context_lines)

def search_multi_week_with_context_retriever(
    keyword: str,
    date_from: Optional[str] = None,
    date_to: Optional[str] = None
) -> List[Dict[str, Any]]:
    """
    Search articles across multiple weeks using ContextRetriever

    Args:
        keyword: Search term
        date_from: Start date (YYYY-MM-DD) - optional
        date_to: End date (YYYY-MM-DD) - optional

    Returns:
        List of matching articles with report metadata
    """
    try:
        retriever = ContextRetriever()
        results = retriever.search_by_keyword(
            keyword=keyword,
            date_from=date_from,
            date_to=date_to,
            search_fields=["title", "full_content"]
        )
        return results
    except Exception as e:
        return []

def search_by_entity_with_context_retriever(
    entity_name: str,
    entity_type: Optional[str] = None,
    date_from: Optional[str] = None,
    date_to: Optional[str] = None
) -> List[Dict[str, Any]]:
    """
    Search articles by entity (company, model, person, location)
    across multiple weeks using ContextRetriever

    Args:
        entity_name: Name of entity to search for
        entity_type: Type of entity (companies, models, people, locations, other)
        date_from: Start date (YYYY-MM-DD) - optional
        date_to: End date (YYYY-MM-DD) - optional

    Returns:
        List of articles mentioning the entity with report metadata
    """
    try:
        retriever = ContextRetriever()
        results = retriever.search_by_entity(
            entity_name=entity_name,
            entity_type=entity_type,
            date_from=date_from,
            date_to=date_to
        )
        return results
    except Exception as e:
        return []

def format_multi_week_results(results: List[Dict[str, Any]], lang: str = "zh") -> str:
    """
    Format multi-week search results for display

    Args:
        results: List of articles from ContextRetriever
        lang: Language for display (zh/en)

    Returns:
        Formatted markdown string
    """
    if not results:
        return "Ê≤°ÊúâÊâæÂà∞ÂåπÈÖçÁöÑÊñáÁ´†„ÄÇ" if lang == "zh" else "No articles found."

    output_lines = []
    output_lines.append(f"## ÊâæÂà∞ {len(results)} ÁØáÁõ∏ÂÖ≥ÊñáÁ´†\n" if lang == "zh" else f"## Found {len(results)} relevant articles\n")

    # Group by date
    by_date = {}
    for article in results:
        date = article.get("report_date", "Unknown")
        if date not in by_date:
            by_date[date] = []
        by_date[date].append(article)

    # Display grouped by date
    for date in sorted(by_date.keys(), reverse=True):
        output_lines.append(f"### üìÖ {date}")
        output_lines.append("")

        for article in by_date[date]:
            output_lines.append(f"**{article.get('title', 'Untitled')}**")

            meta_parts = []
            if article.get('source'):
                meta_parts.append(f"Êù•Ê∫ê: {article['source']}" if lang == "zh" else f"Source: {article['source']}")
            if article.get('url'):
                meta_parts.append(f"[URL]({article['url']})")

            if meta_parts:
                output_lines.append(" | ".join(meta_parts))

            if article.get('credibility_score'):
                score = article.get('credibility_score')
                output_lines.append(f"ÂèØ‰ø°Â∫¶: {score}/10" if lang == "zh" else f"Credibility: {score}/10")

            output_lines.append("")

    return "\n".join(output_lines)

def load_latest_briefing() -> Optional[Dict[str, Any]]:
    """Load the latest briefing from data/reports directory"""
    reports_dir = Path("./data/reports")
    if not reports_dir.exists():
        return None

    # Look for both briefing_*.md and ai_briefing_*.md patterns
    markdown_files = sorted(reports_dir.glob("*briefing_*.md"), reverse=True)
    if not markdown_files:
        return None

    latest_file = markdown_files[0]

    # Try to find corresponding JSON file
    json_file = reports_dir / latest_file.stem / "data.json"

    # If JSON doesn't exist, look for data.json
    data_files = list(reports_dir.glob("**/data.json"))
    if data_files:
        with open(data_files[0], 'r', encoding='utf-8') as f:
            return json.load(f)

    # Load markdown and parse articles
    content = latest_file.read_text(encoding='utf-8')
    articles = parse_articles_from_markdown(content)

    # Fallback: create structure from markdown
    return {
        "date": latest_file.stem.replace("ai_briefing_", "").replace("briefing_", ""),
        "title": "AI Industry Weekly Briefing",
        "content": content,
        "articles": articles
    }

def get_available_briefings() -> List[Dict[str, Any]]:
    """Get list of all available briefings (for archive)"""
    reports_dir = Path("./data/reports")
    if not reports_dir.exists():
        return []

    briefings = []
    markdown_files = sorted(reports_dir.glob("*briefing_*.md"), reverse=True)

    for file in markdown_files:
        date_str = file.stem.replace("ai_briefing_", "").replace("briefing_", "")
        briefings.append({
            "date": date_str,
            "filename": file.name,
            "path": file
        })

    return briefings

def search_articles_with_llm(query: str, briefing_content: str, lang: str = "en") -> str:
    """Use LLM to search and return matching articles with detailed analysis"""
    if not st.session_state.provider_switcher:
        return t("chat_error", lang)

    try:
        system_prompt = f"""‰Ω†ÊòØ‰∏Ä‰ΩçAIË°å‰∏öÊêúÁ¥¢‰∏ìÂÆ∂„ÄÇÁî®Êà∑ÈúÄË¶ÅÊâæÂà∞‰∏éÂÖ∂Êü•ËØ¢Áõ∏ÂÖ≥ÁöÑÊñáÁ´†„ÄÇ

ÊêúÁ¥¢Ë¶ÅÊ±Ç:
1. ÊâæÂà∞ÊâÄÊúâ‰∏éÁî®Êà∑Êü•ËØ¢Áõ∏ÂÖ≥ÁöÑÊñáÁ´†
2. ÂØπÊØèÁØáÂåπÈÖçÁöÑÊñáÁ´†ËøõË°åËØ¶ÁªÜÂàÜÊûê
3. ËØ¥Êòé‰∏∫‰ªÄ‰πàËøôÁØáÊñáÁ´†‰∏éÊü•ËØ¢Áõ∏ÂÖ≥
4. Êèê‰æõÂÖ∑‰ΩìÁöÑËØÅÊçÆÊàñÊëòÂΩïÊîØÊåÅÊÇ®ÁöÑÂà§Êñ≠

ËøîÂõûÊ†ºÂºèÔºàÂØπÊØèÁØáÂåπÈÖçÁöÑÊñáÁ´†Ôºâ:
**[ÊñáÁ´†Ê†áÈ¢ò]**
Êù•Ê∫ê: [Êù•Ê∫ê]
URL: [ÈìæÊé•]
Áõ∏ÂÖ≥Â∫¶: [È´ò/‰∏≠/‰Ωé]
Áõ∏ÂÖ≥ÂéüÂõ†: [ÁÆÄË¶ÅËØ¥ÊòéËøôÁØáÊñáÁ´†‰∏∫‰ªÄ‰πà‰∏éÊü•ËØ¢Áõ∏ÂÖ≥ÔºåÂåÖÊã¨ÂÖ∑‰ΩìÁöÑÊï∞ÊçÆÊàñËØÅÊçÆ]

ÈáçË¶ÅÊèêÁ§∫:
- Â¶ÇÊûúÊâæ‰∏çÂà∞Áõ∏ÂÖ≥ÊñáÁ´†ÔºåÊòéÁ°ÆËØ¥Êòé
- ‰∏çË¶ÅÁºñÈÄ†‰∏çÂ≠òÂú®ÁöÑÊñáÁ´†
- ‰ΩøÁî®‰∏≠ÊñáÂõûÁ≠î
- Ê∑±ÂÖ•ÂàÜÊûêËÄå‰∏ç‰ªÖ‰ªÖËøîÂõûÊ†áÈ¢ò

‰ª•‰∏ãÊòØÊú¨Âë®ÁöÑÊñáÁ´†ÂÜÖÂÆπ:

{briefing_content}"""

        response = st.session_state.provider_switcher.query(
            prompt=f"Ê†πÊçÆ‰ª•‰∏ãÊü•ËØ¢ÊêúÁ¥¢ÊñáÁ´†: {query}",
            system_prompt=system_prompt,
            max_tokens=1024,
            temperature=0.7
        )
        return response
    except Exception as e:
        return f"{t('chat_error', lang)}: {str(e)}"

def answer_question_about_briefing(question: str, briefing_content: str, lang: str = "en") -> str:
    """Use LLM to answer questions about the briefing with deep analysis"""
    if not st.session_state.provider_switcher:
        return t("chat_error", lang)

    try:
        system_prompt = f"""‰Ω†ÊòØ‰∏Ä‰ΩçAIË°å‰∏öÂàÜÊûê‰∏ìÂÆ∂„ÄÇ‰Ω†ÈúÄË¶ÅÂõûÁ≠îÂÖ≥‰∫éAIË°å‰∏öÂë®Êä•ÁöÑÈóÆÈ¢ò„ÄÇ

ÂÖ≥ÈîÆËÅåË¥£:
1. ÂàÜÊûêÊñáÁ´†ÂÜÖÂÆπÔºåÊèêÂèñ‰∏≠ÂøÉËÆ∫ÁÇπÔºàCentral ArgumentÔºâ
2. ËØÜÂà´Âπ∂Ëß£ÈáäÊîØÊíëËÆ∫ÁÇπÁöÑÊï∞ÊçÆÂíåËØÅÊçÆÔºàData and EvidenceÔºâ
3. Â¶ÇÊûúÁî®Êà∑ÊèêÈóÆÂê´Ê∑∑ÔºåÂ∫îËØ•Êèê‰æõÂ§öËßíÂ∫¶ÁöÑÂàÜÊûê
4. ÂºïÁî®ÂÖ∑‰ΩìÁöÑÊñáÁ´†Ê†áÈ¢òÂíåÊù•Ê∫ê
5. Ê∑±ÂÖ•ÂàÜÊûêËÄåÈùû‰ªÖÈáçÂ§çÊëòË¶Å

ÂõûÁ≠îË¶ÅÊ±Ç:
- ÂáÜÁ°ÆÂºïÁî®ÊñáÁ´†ÂÜÖÂÆπ
- Êèê‰æõÂÖ∑‰ΩìÁöÑÊï∞ÊçÆ„ÄÅÊï∞Â≠óÊàñ‰∫ãÂÆû
- Ëß£ÈáäÂõ†ÊûúÂÖ≥Á≥ªÂíåÈÄªËæë
- ÂøÖË¶ÅÊó∂ÂèØ‰ª•‰ªéÂ§öÁØáÊñáÁ´†ÁªºÂêàÂàÜÊûê
- ‰ΩøÁî®‰∏≠ÊñáÂõûÁ≠îÔºå‰øùÊåÅ‰∏ì‰∏ö‰∏îÊòìÊáÇÁöÑËØ≠Ê∞î

‰ª•‰∏ãÊòØÊú¨Âë®ÁöÑÊñáÁ´†ÂÜÖÂÆπ:

{briefing_content}"""

        response = st.session_state.provider_switcher.query(
            prompt=question,
            system_prompt=system_prompt,
            max_tokens=1024,
            temperature=0.7
        )
        return response
    except Exception as e:
        return f"{t('chat_error', lang)}: {str(e)}"

# ============================================================================
# MAIN APP LAYOUT
# ============================================================================

# Header with title and language selector
col_title, col_lang = st.columns([0.85, 0.15])
with col_title:
    st.markdown(f"<div class='main-title'>{t('page_title', st.session_state.language)}</div>", unsafe_allow_html=True)
    st.markdown(f"<div class='subtitle-text'>{t('subtitle', st.session_state.language)}</div>", unsafe_allow_html=True)

with col_lang:
    st.session_state.language = st.selectbox(
        t('language', st.session_state.language),
        ["zh", "en"],
        format_func=lambda x: "‰∏≠Êñá" if x == "zh" else "English",
        index=0 if st.session_state.language == "zh" else 1,
        key="lang_selector",
        label_visibility="collapsed"
    )

# Archive section in sidebar
with st.sidebar:
    st.markdown("### üìö Archive / Â≠òÊ°£")
    available = get_available_briefings()

    if available:
        briefing_options = {b['date']: b['path'] for b in available}
        selected_date = st.selectbox(
            "Select briefing / ÈÄâÊã©ÁÆÄÊä•",
            options=list(briefing_options.keys()),
            index=0,
            label_visibility="collapsed"
        )

        if selected_date:
            st.session_state.selected_briefing = briefing_options[selected_date]

# Load briefing data - either from archive selection or latest
if st.session_state.selected_briefing:
    briefing_file = st.session_state.selected_briefing
    content = briefing_file.read_text(encoding='utf-8')
    articles = parse_articles_from_markdown(content)
    date_str = briefing_file.stem.replace("ai_briefing_", "").replace("briefing_", "")
    briefing = {
        "date": date_str,
        "title": "AI Industry Weekly Briefing",
        "content": content,
        "articles": articles
    }
else:
    briefing = load_latest_briefing()

if briefing is None:
    st.warning("No briefing data available. Please generate a briefing first.")
    st.stop()

# Main content: 70% briefing + 30% chat interface
left_col, right_col = st.columns([0.70, 0.30], gap="medium")

# ============================================================================
# LEFT COLUMN: BRIEFING DISPLAY
# ============================================================================
with left_col:
    # Executive summary section
    st.markdown(f"**{t('executive_summary', st.session_state.language)}**")

    if briefing.get("summary"):
        st.markdown(briefing["summary"])
    else:
        st.info(t('about_description', st.session_state.language))

    st.divider()

    # Briefing info
    if briefing.get("date"):
        st.caption(f"üìÖ {t('report_date', st.session_state.language)}: {briefing['date']}")

    st.divider()

    # Articles list
    st.markdown(f"**{t('articles', st.session_state.language)}**")

    if briefing.get("articles"):
        for idx, article in enumerate(briefing["articles"], 1):
            # Article title
            st.markdown(f"**{idx}. {article.get('title', 'Untitled')}**")

            # Article summary
            if article.get('summary'):
                st.markdown(article['summary'])

            # Source and URL in one line
            meta_info = []
            if article.get('source'):
                meta_info.append(f"Êù•Ê∫ê: {article['source']}")
            if article.get('url'):
                meta_info.append(f"[{article['url']}]({article['url']})")

            if meta_info:
                st.caption(" | ".join(meta_info))

            st.divider()
    else:
        st.info("No articles in this briefing")

    st.divider()

    # Download section
    if briefing.get("content"):
        briefing_text = briefing["content"]
        st.download_button(
            label=t('download', st.session_state.language),
            data=briefing_text,
            file_name=f"briefing_{briefing.get('date', 'report')}.md",
            mime="text/markdown"
        )

# ============================================================================
# RIGHT COLUMN: UNIFIED CHAT+SEARCH INTERFACE
# ============================================================================
with right_col:
    st.markdown("### ü§ñ AI Assistant")

    # Mode selector with support for three search types + ask
    st.markdown(f"<div class='mode-selector'>", unsafe_allow_html=True)
    mode_options = {
        "this_week": t('mode_search', st.session_state.language),
        "multi_week": t('multi_week_search', st.session_state.language),
        "entity": t('entity_search', st.session_state.language),
        "ask": t('mode_ask', st.session_state.language)
    }

    st.session_state.current_mode = st.radio(
        "Mode / Ê®°Âºè",
        list(mode_options.keys()),
        format_func=lambda x: mode_options[x],
        horizontal=True,
        label_visibility="collapsed",
        key="mode_selector"
    )
    st.markdown(f"</div>", unsafe_allow_html=True)

    # Additional inputs based on selected mode
    user_input = None
    search_params = {}

    if st.session_state.current_mode == "this_week":
        user_input = st.text_input(
            "Search / ÊêúÁ¥¢",
            placeholder=t('unified_input_search', st.session_state.language),
            key="search_input",
            label_visibility="collapsed"
        )
        st.caption(t('search_help', st.session_state.language))

    elif st.session_state.current_mode == "multi_week":
        col1, col2 = st.columns(2)
        with col1:
            # Default to last 4 weeks
            default_from = datetime.now() - timedelta(days=28)
            date_from = st.date_input(
                f"{t('date_range', st.session_state.language)} - {t('mode_search', st.session_state.language).split()[0]}",
                value=default_from,
                key="date_from",
                label_visibility="collapsed"
            )
        with col2:
            date_to = st.date_input(
                f"{t('date_range', st.session_state.language)} - {t('mode_ask', st.session_state.language)}",
                value=datetime.now(),
                key="date_to",
                label_visibility="collapsed"
            )

        user_input = st.text_input(
            "Multi-Week Search / Â§öÂë®ÊêúÁ¥¢",
            placeholder=t('unified_input_search', st.session_state.language),
            key="multiweek_search_input",
            label_visibility="collapsed"
        )
        st.caption(f"üîç {t('search_help', st.session_state.language)}")
        search_params['date_from'] = date_from
        search_params['date_to'] = date_to

    elif st.session_state.current_mode == "entity":
        col1, col2 = st.columns(2)
        with col1:
            entity_type = st.selectbox(
                t('entity_type', st.session_state.language),
                ["companies", "models", "people", "locations", "other"],
                format_func=lambda x: {
                    "companies": t('companies', st.session_state.language),
                    "models": "Models",
                    "people": t('people', st.session_state.language),
                    "locations": t('locations', st.session_state.language),
                    "other": t('other', st.session_state.language)
                }[x],
                key="entity_type_selector",
                label_visibility="collapsed"
            )
        with col2:
            default_from = datetime.now() - timedelta(days=28)
            date_from = st.date_input(
                f"{t('date_range', st.session_state.language)} - From",
                value=default_from,
                key="entity_date_from",
                label_visibility="collapsed"
            )

        date_to = st.date_input(
            f"{t('date_range', st.session_state.language)} - To",
            value=datetime.now(),
            key="entity_date_to",
            label_visibility="collapsed"
        )

        user_input = st.text_input(
            "Entity Search / ÂÆû‰ΩìÊêúÁ¥¢",
            placeholder="e.g., OpenAI, GPT-4, Yann LeCun / ‰æãÂ¶ÇÔºöOpenAI„ÄÅGPT-4„ÄÅYann LeCun",
            key="entity_search_input",
            label_visibility="collapsed"
        )
        st.caption(f"üîç {t('search_help', st.session_state.language)}")
        search_params['entity_type'] = entity_type
        search_params['date_from'] = date_from
        search_params['date_to'] = date_to

    else:  # Ask mode
        user_input = st.text_input(
            "Ask / ÊèêÈóÆ",
            placeholder=t('unified_input_ask', st.session_state.language),
            key="ask_input",
            label_visibility="collapsed"
        )

    st.divider()

    # Process user input and display results
    if user_input:
        if st.session_state.current_mode == "ask":
            # Ask mode: Question answering using current briefing
            enriched_context = create_enriched_briefing_context(briefing.get("articles", []))
            with st.spinner(f"üí≠ {t('mode_ask', st.session_state.language)}..." if st.session_state.language == "zh" else "Thinking..."):
                response = answer_question_about_briefing(user_input, enriched_context, st.session_state.language)

            st.markdown(f"**{t('ai_response', st.session_state.language)}**")
            if response and "Error" not in response:
                st.markdown(response)
            else:
                st.error(response)

        elif st.session_state.current_mode == "this_week":
            # This week search: Current implementation (Phase A)
            enriched_context = create_enriched_briefing_context(briefing.get("articles", []))
            with st.spinner(f"üîç {t('mode_search', st.session_state.language)}..."):
                response = search_articles_with_llm(user_input, enriched_context, st.session_state.language)

            st.markdown(f"**{t('search_results_title', st.session_state.language)}**")
            if response and "Error" not in response:
                st.markdown(response)
            else:
                st.warning(t('no_results', st.session_state.language))

        elif st.session_state.current_mode == "multi_week":
            # Multi-week search: Search across multiple briefings using ContextRetriever (Phase B)
            with st.spinner(f"üîç {t('multi_week_search', st.session_state.language)}..."):
                results = search_multi_week_with_context_retriever(
                    keyword=user_input,
                    date_from=search_params['date_from'].strftime("%Y-%m-%d"),
                    date_to=search_params['date_to'].strftime("%Y-%m-%d")
                )

            st.markdown(f"**{t('search_results_title', st.session_state.language)}**")
            if results:
                # Format and display results grouped by date
                formatted_results = format_multi_week_results(results, st.session_state.language)
                st.markdown(formatted_results)
            else:
                st.warning(t('no_results', st.session_state.language))

        elif st.session_state.current_mode == "entity":
            # Entity search: Search for specific companies, people, models, locations (Phase B)
            with st.spinner(f"üîç {t('entity_search', st.session_state.language)}..."):
                results = search_by_entity_with_context_retriever(
                    entity_name=user_input,
                    entity_type=search_params['entity_type'],
                    date_from=search_params['date_from'].strftime("%Y-%m-%d"),
                    date_to=search_params['date_to'].strftime("%Y-%m-%d")
                )

            st.markdown(f"**{t('entity_search', st.session_state.language)}: {user_input}**")
            if results:
                # Format and display results grouped by date
                formatted_results = format_multi_week_results(results, st.session_state.language)
                st.markdown(formatted_results)
            else:
                st.warning(t('no_results', st.session_state.language))

# ============================================================================
# FOOTER
# ============================================================================
st.divider()
st.caption(f"Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
