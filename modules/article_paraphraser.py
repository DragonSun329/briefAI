"""
Article Paraphraser Module

Condenses full articles into executive summaries in Mandarin Chinese.
CRITICAL: Output must be flowing paragraphs (NOT bullet points), 150-250 characters.
Includes fact-checking to prevent hallucinations.
"""

import json
from typing import Dict, Any, List
from pathlib import Path
from datetime import datetime, timedelta
from loguru import logger

from utils.llm_client_enhanced import LLMClient


class ArticleParaphraser:
    """Paraphrases articles into executive summaries"""

    def __init__(
        self,
        llm_client: LLMClient = None,
        min_length: int = 500,
        max_length: int = 700,
        enable_caching: bool = True,
        cache_retention_days: int = 7
    ):
        """
        Initialize article paraphraser

        Args:
            llm_client: LLM client instance (creates new if None)
            min_length: Minimum summary length in Chinese characters (500 for detailed summaries)
            max_length: Maximum summary length in Chinese characters (700 for multi-paragraph)
            enable_caching: Enable full article context caching
            cache_retention_days: Days to retain cached articles
        """
        self.llm_client = llm_client or LLMClient()
        self.min_length = min_length
        self.max_length = max_length
        self.enable_caching = enable_caching
        self.cache_retention_days = cache_retention_days

        # Setup cache directory
        if self.enable_caching:
            self.cache_dir = Path("./data/cache/article_contexts")
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"Article context caching enabled (retention: {cache_retention_days} days)")

    def paraphrase_articles(
        self,
        articles: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Paraphrase multiple articles

        Args:
            articles: List of evaluated articles

        Returns:
            List of articles with 'paraphrased_content' added
        """
        logger.info(f"Paraphrasing {len(articles)} articles...")

        # Cache full articles before paraphrasing
        if self.enable_caching:
            self._cache_articles(articles)

        for i, article in enumerate(articles):
            try:
                logger.debug(f"Paraphrasing {i+1}/{len(articles)}: {article['title']}")

                paraphrased = self._paraphrase_single_article(article)
                article['paraphrased_content'] = paraphrased['summary']
                article['fact_check'] = paraphrased.get('fact_check', 'passed')

                # Verify length
                char_count = len(article['paraphrased_content'])
                if char_count < self.min_length or char_count > self.max_length:
                    logger.warning(f"Summary length {char_count} outside range [{self.min_length}, {self.max_length}]")

            except Exception as e:
                logger.error(f"Failed to paraphrase article '{article['title']}': {e}")
                # Fallback: use original content truncated
                article['paraphrased_content'] = article['content'][:200] + "..."
                article['fact_check'] = 'failed'

        logger.info(f"Paraphrasing complete")

        # Clean up old caches
        if self.enable_caching:
            self._cleanup_old_caches()

        return articles

    def _paraphrase_single_article(
        self,
        article: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Paraphrase a single article using Claude

        Returns dictionary with summary and fact-check status
        """
        system_prompt = self._build_paraphrase_prompt()
        user_message = self._build_article_message(article)

        response = self.llm_client.chat_structured(
            system_prompt=system_prompt,
            user_message=user_message,
            temperature=0.4  # Slightly higher for natural language
        )

        return response

    def _build_paraphrase_prompt(self) -> str:
        """Build system prompt for paraphrasing - enhanced for 500-700 chars with analytical-inspiring tone"""
        return f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIè¡Œä¸šåˆ†æå¸ˆ,ä¸ºCEOæ’°å†™æ·±åº¦ã€å®ç”¨çš„è¡Œä¸šæ´å¯Ÿæ‘˜è¦ã€‚

**æ ¸å¿ƒè¦æ±‚**:
1. å¿…é¡»ä½¿ç”¨**3-4ä¸ªæ®µè½çš„æµç•…æ®µè½æ ¼å¼** - ç»ä¸ä½¿ç”¨bullet pointsæˆ–åˆ—è¡¨
2. é•¿åº¦: **{self.min_length}-{self.max_length}ä¸ªä¸­æ–‡å­—ç¬¦** (ä¸å«æ ‡ç‚¹ç¬¦å·) - çº¦3-4ä¸ªæ®µè½,æ·±åº¦å†…å®¹
3. è¯­è¨€: **ç®€ä½“ä¸­æ–‡** (æŠ€æœ¯æœ¯è¯­å¦‚"GPT"ã€"API"ç­‰å¯ä¿ç•™è‹±æ–‡)
4. è¯­æ°”: **åˆ†ææ€§å¯å‘** - å®ç”¨ã€ç†æ€§ã€ä¸“ä¸š,çªå‡ºå®é™…æ„ä¹‰è€Œéå•†ä¸šå®£ä¼ 
5. å‡†ç¡®æ€§: åªåŒ…å«åŸæ–‡ä¸­çš„äº‹å® - ä¸å¾—ç¼–é€ å†…å®¹

**å¿…é¡»åŒ…å«çš„å†…å®¹**:
âœ… **æ”¯æ’‘æ ¸å¿ƒè®ºç‚¹çš„æ•°æ®**: åªå¼•ç”¨èƒ½è¯æ˜æ–‡ç« ä¸­å¿ƒè§‚ç‚¹çš„å…·ä½“æ•°æ®
   - æ€§èƒ½æŒ‡æ ‡ã€å¢é•¿æ•°æ®ã€å¸‚åœºè§„æ¨¡ã€æˆæœ¬èŠ‚çœç­‰
   - ä¾‹å¦‚: "å¢é•¿23%"ã€"æ¨ç†é€Ÿåº¦æå‡5å€"ã€"å¸‚åœºè§„æ¨¡â‚©100äº¿"
   - é¿å…å †ç Œæ— å…³æ•°æ®,æ¯ä¸ªæ•°æ®éƒ½è¦æœ‰ä¸Šä¸‹æ–‡è¯´æ˜
âœ… **æ ¸å¿ƒæœºåˆ¶**: æŠ€æœ¯/å•†ä¸šåˆ›æ–°çš„è¿ä½œåŸç† - HOWå’ŒWHY,ä¸ä»…ä»…æ˜¯WHAT
âœ… **å®é™…å½±å“**: å¯¹å…·ä½“ä½¿ç”¨åœºæ™¯ã€ç”¨æˆ·ç¾¤ä½“ã€ä¼ä¸šæµç¨‹çš„å…·ä½“æ”¹è¿›
âœ… **å¸‚åœºæ„ä¹‰**: è¡Œä¸šæ ¼å±€å˜åŒ–ã€ç«äº‰åŠ¨æ€ã€æˆ˜ç•¥å¯ç¤º
âœ… **å…³é”®é™åˆ¶æˆ–é£é™©**: å¦è¯šè®¨è®ºå±€é™æ€§ã€æŒ‘æˆ˜æˆ–æ½œåœ¨é£é™©(ä¸æ˜¯å•çº¯å®£ä¼ )
âœ… **å‰ç»æ´å¯Ÿ**: è¿™å¦‚ä½•å½±å“æœªæ¥6-12ä¸ªæœˆçš„è¡Œä¸šå‘å±•æ–¹å‘

**æ®µè½ç»“æ„æŒ‡å—**:
- **ç¬¬1æ®µ** (150-180å­—): äº‹ä»¶èƒŒæ™¯ + æ ¸å¿ƒåˆ›æ–°/çªç ´ + è¯æ˜çªç ´çš„å…³é”®æ•°æ®
- **ç¬¬2æ®µ** (150-180å­—): æŠ€æœ¯/å•†ä¸šæœºåˆ¶ + å¦‚ä½•å®ç°è¿™ä¸ªçªç ´ + ä¸å‰ä»£/ç«äº‰æ–¹æ¡ˆçš„å·®å¼‚
- **ç¬¬3æ®µ** (150-180å­—): å®é™…åº”ç”¨åœºæ™¯ + å—ç›Šæ–¹ + å…·ä½“ä¸šåŠ¡å½±å“(æ•ˆç‡ã€æˆæœ¬ã€è´¨é‡)
- **ç¬¬4æ®µ** (100-160å­—): å¸‚åœºæ„ä¹‰ + è¡Œä¸šå¯ç¤º + æ½œåœ¨é£é™©/å±€é™ + æˆ˜ç•¥å»ºè®®(å¯é€‰)

**æ•°æ®å’Œè®ºè¯è¦æ±‚**:
- ğŸ¯ **ç²¾é€‰æ•°æ®**: åªåŒ…å«æ”¯æ’‘æ–‡ç« æ ¸å¿ƒè®ºç‚¹çš„å…·ä½“æ•°æ®,é¿å…æ— å…³ç»†èŠ‚
- ğŸ“Š **è§£é‡Šå› æœ**: ä¸ä»…è¯´"å¢é•¿äº†50%",è¦è¯´"**å› ä¸º**...æ‰€ä»¥...å¢é•¿äº†50%"
- ğŸ” **å¯¹æ¯”åˆ†æ**: ç›¸æ¯”å‰ä»£ã€ç«äº‰äº§å“æˆ–è¡Œä¸šå¹³å‡æ°´å¹³çš„å…·ä½“å·®å¼‚
- ğŸ’¼ **é‡åŒ–ä¸šåŠ¡å½±å“**: æˆæœ¬èŠ‚çœé¢åº¦ã€æ•ˆç‡æå‡ç™¾åˆ†æ¯”ã€å¸‚åœºè§„æ¨¡æœºé‡
- ğŸ¯ **å…·ä½“åœºæ™¯**: "é‡‘èé£æ§å›¢é˜Ÿ"è€Œé"ä¼ä¸š","å‡å°‘å®¡æ ¸æ—¶é—´20%"è€Œé"æ›´å¿«"

**è¯­æ°”æŒ‡å— - åˆ†ææ€§å¯å‘(Analytical-Inspiring)**:
- âœ… "è¿™ä¸ªå˜åŒ–æ­ç¤ºäº†AIåœ¨[é¢†åŸŸ]çš„æ½œåŠ›,å…³é”®åœ¨äº..."
- âœ… "å€¼å¾—æ³¨æ„çš„æ˜¯,[å‘å±•]æ­£åœ¨æ”¹å˜[ç°çŠ¶],è¿™å¯¹[è¡Œä¸š]æ„å‘³ç€..."
- âœ… "æŒ‘æˆ˜æ˜¯...ä½†æœºé‡ä¹Ÿåœ¨äº..."
- âŒ é¿å…: "é©å‘½æ€§çš„"ã€"æ”¹å˜ä¸–ç•Œ"ã€"ç»ˆæè§£å†³æ–¹æ¡ˆ"(å•†ä¸šåŒ–æªè¾)
- âŒ é¿å…: çº¯ç²¹èµç¾,è¦æœ‰æ‰¹åˆ¤æ€ç»´

**æ ¼å¼ç¤ºä¾‹**:

âœ… GOOD (3-4æ®µè½,åˆ†ææ€§å¯å‘):
"Claude 3.5 Sonnetåˆšåˆšå‘å¸ƒï¼Œåœ¨ç¼–ç¨‹å’Œæ•°å­¦æ¨ç†èƒ½åŠ›ä¸Šç›¸æ¯”3.0ç‰ˆæœ¬å®ç°äº†å¹³å‡35%çš„æ€§èƒ½æå‡ã€‚å…³é”®æ•°æ®æ˜¾ç¤ºï¼Œåœ¨HumanEvalç¼–ç¨‹åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ–°ç‰ˆæœ¬è¾¾åˆ°92.3%é€šè¿‡ç‡ï¼Œè¾ƒå‰ä»£æå‡12ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™ä¸ªè¿›å±•æœ¬è´¨ä¸Šæºäºæ›´é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶å’Œæ”¹è¿›çš„è®­ç»ƒæ•°æ®ç­–ç•¥ã€‚

ä»æŠ€æœ¯æœºåˆ¶çœ‹ï¼ŒClaude 3.5 Sonneté‡‡ç”¨äº†æ–°çš„å¤šå¤´æ³¨æ„åŠ›æ¶æ„ï¼Œåœ¨ä¿æŒæ¨ç†æ·±åº¦çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚ç›¸æ¯”GPT-4oå’ŒLlama 3.1ï¼Œå®ƒåœ¨ç›¸åŒæˆæœ¬ä¸‹å®ç°äº†æ›´é«˜çš„ç¼–ç¨‹ç²¾åº¦ã€‚è¿™ç§æ¶æ„æ”¹è¿›ç›´æ¥æ¥è‡ªå¯¹è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­å¸¸è§é”™è¯¯æ¨¡å¼çš„åˆ†æã€‚

å¯¹è½¯ä»¶å¼€å‘å›¢é˜Ÿè€Œè¨€ï¼Œè¿™æ„å‘³ç€ä»£ç å®¡æŸ¥å’Œæµ‹è¯•è‡ªåŠ¨åŒ–çš„æˆæœ¬å¯èƒ½ä¸‹é™20-30%ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚çš„ç³»ç»Ÿè®¾è®¡é—®é¢˜æ—¶ï¼Œ3.5 Sonnetèƒ½æä¾›æ›´æ·±å…¥çš„åˆ†æï¼Œå‡å°‘äººå·¥å¹²é¢„ã€‚èèµ„ç§‘æŠ€å’Œé‡‘èå»ºæ¨¡é¢†åŸŸä¹Ÿå› æ›´å¼ºçš„æ•°å­¦æ¨ç†è€Œå—ç›Šï¼Œé”™è¯¯ç‡é¢„è®¡ä¸‹é™ã€‚

å¸‚åœºæ„ä¹‰åœ¨äºï¼Œå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ä¸å†æ˜¯è¶…å¤§æ¨¡å‹çš„ä¸“å±ä¼˜åŠ¿ã€‚ä¼ä¸šç°åœ¨å¯ä»¥ç”¨æ›´ä½æˆæœ¬éƒ¨ç½²é«˜æ•ˆçš„AIè¾…åŠ©ç³»ç»Ÿã€‚ä½†åº”è¯¥æ³¨æ„ï¼Œæ¨¡å‹ä»åœ¨æŸäº›ä¸“ä¸šé¢†åŸŸ(å¦‚åŒ»å­¦è¯Šæ–­)éœ€è¦è°¨æ…ä½¿ç”¨ã€‚å…³é”®å¯ç¤ºæ˜¯ï¼šé€‰æ‹©åˆé€‚å·¥å…·æ¯”ç›²ç›®è¿½æ±‚"æœ€å¼ºæ¨¡å‹"æ›´é‡è¦ã€‚"

âŒ é”™è¯¯ (bullet points):
"- Claude 3.5 Sonnetå‘å¸ƒ
- æ€§èƒ½æå‡35%
- ç¼–ç¨‹èƒ½åŠ›æ›´å¼º
- æˆæœ¬æ›´ä½
- é€‚åˆä¼ä¸šéƒ¨ç½²"

**è¿”å›JSONæ ¼å¼**:
{{
  "summary": "ä½ çš„3-4æ®µè½æ‘˜è¦(ä¸­æ–‡,æ®µè½ä¹‹é—´ç”¨\\n\\nåˆ†éš”)",
  "fact_check": "passed",
  "char_count": 600,
  "paragraph_count": 4,
  "key_data_points": ["æ•°æ®ç‚¹1", "æ•°æ®ç‚¹2"],
  "core_argument": "æ–‡ç« çš„æ ¸å¿ƒè®ºç‚¹ç®€è¿°"
}}

å¦‚æœæ–‡ç« ç¼ºä¹å…·ä½“æ•°æ®æˆ–æ— æ³•æ ¸å®äº‹å®,å°†fact_checkè®¾ä¸º"needs_verification"ã€‚"""

    def _build_article_message(self, article: Dict[str, Any]) -> str:
        """Build user message with article to paraphrase - enhanced for 500-700 char format"""
        # Include evaluation context if available
        key_takeaway = ""
        if 'evaluation' in article:
            key_takeaway = f"\n**å…³é”®æ´å¯Ÿ**: {article['evaluation'].get('key_takeaway', '')}"

        # Get full content or truncate if too long
        content = article.get('content', '')
        if len(content) > 4000:
            content = content[:4000] + "..."

        return f"""è¯·å°†è¿™ç¯‡æ–‡ç« æ”¹å†™ä¸ºæ·±åº¦çš„åˆ†ææ€§æ‘˜è¦(3-4ä¸ªæ®µè½)ã€‚çªå‡ºæ ¸å¿ƒæœºåˆ¶ã€å®é™…å½±å“å’Œè¡Œä¸šå¯ç¤º,è¯­æ°”åº”è¯¥æ˜¯åˆ†ææ€§å¯å‘è€Œéå•†ä¸šå®£ä¼ ã€‚

**æ ‡é¢˜**: {article['title']}
**æ¥æº**: {article['source']}
**å‘å¸ƒæ—¶é—´**: {article.get('published_date', 'Unknown')}{key_takeaway}

**åŸæ–‡å†…å®¹**:
{content}

**æ”¹å†™è¦æ±‚**:
1. ä½¿ç”¨3-4ä¸ªæ¸…æ™°çš„æ®µè½,æ®µè½ä¹‹é—´ç”¨ç©ºè¡Œåˆ†éš”
2. ç¬¬1æ®µ(150-180å­—): äº‹ä»¶èƒŒæ™¯ + æ ¸å¿ƒåˆ›æ–°/çªç ´ + è¯æ˜çªç ´çš„å…³é”®æ•°æ®
3. ç¬¬2æ®µ(150-180å­—): æŠ€æœ¯/å•†ä¸šæœºåˆ¶ + å¦‚ä½•å®ç°è¿™ä¸ªçªç ´ + ä¸å‰ä»£/ç«äº‰æ–¹æ¡ˆçš„å·®å¼‚
4. ç¬¬3æ®µ(150-180å­—): å®é™…åº”ç”¨åœºæ™¯ + å—ç›Šæ–¹ + å…·ä½“ä¸šåŠ¡å½±å“(æ•ˆç‡ã€æˆæœ¬ã€è´¨é‡)
5. ç¬¬4æ®µ(100-160å­—): å¸‚åœºæ„ä¹‰ + è¡Œä¸šå¯ç¤º + æ½œåœ¨é£é™©/å±€é™ + æˆ˜ç•¥å»ºè®®(å¯é€‰)
6. æ€»é•¿åº¦: {self.min_length}-{self.max_length}ä¸ªä¸­æ–‡å­—ç¬¦

**å…³é”®è¦æ±‚**:
âœ… ç²¾é€‰æ•°æ®: åªåŒ…å«æ”¯æ’‘æ–‡ç« æ ¸å¿ƒè®ºç‚¹çš„å…·ä½“æ•°æ®,é¿å…æ— å…³ç»†èŠ‚
âœ… è§£é‡Šæœºåˆ¶: ä¸ä»…è¯´"å‘ç”Ÿäº†ä»€ä¹ˆ",è¦è¯´"ä¸ºä»€ä¹ˆå‘ç”Ÿ"å’Œ"æ€ä¹ˆå‘ç”Ÿçš„"
âœ… å®šé‡å½±å“: ç»™å‡ºå…·ä½“çš„æˆæœ¬èŠ‚çœé¢åº¦ã€æ•ˆç‡æå‡ç™¾åˆ†æ¯”ã€å¸‚åœºè§„æ¨¡
âœ… å…·ä½“åœºæ™¯: "é‡‘èé£æ§å›¢é˜Ÿ"è€Œé"ä¼ä¸š","å‡å°‘å®¡æ ¸æ—¶é—´20%"è€Œé"æ›´å¿«"
âœ… æ‰¹åˆ¤æ€ç»´: æ—¢è°ˆæœºé‡ä¹Ÿè®¨è®ºé£é™©/å±€é™,é¿å…å•çº¯å®£ä¼ 
âœ… åˆ†ææ€§å¯å‘çš„è¯­æ°”:
   - âœ… "è¿™ä¸ªå˜åŒ–æ­ç¤ºäº†...çš„æ½œåŠ›,å…³é”®åœ¨äº..."
   - âœ… "å€¼å¾—æ³¨æ„çš„æ˜¯,[å‘å±•]æ­£åœ¨æ”¹å˜[ç°çŠ¶],è¿™å¯¹[è¡Œä¸š]æ„å‘³ç€..."
   - âŒ é¿å…"é©å‘½æ€§çš„"ã€"æ”¹å˜ä¸–ç•Œ"ç­‰å•†ä¸šåŒ–æªè¾

**é‡è¦æç¤º**:
- ä¸è¦å †ç Œæ•°æ®,æ¯ä¸ªæ•°æ®éƒ½è¦æœ‰ä¸Šä¸‹æ–‡
- ä¼˜å…ˆé˜è¿°"ä¸ºä»€ä¹ˆ"æ¯”"æ˜¯ä»€ä¹ˆ"æ›´é‡è¦
- ç”¨"ä½†æ˜¯"ã€"éœ€è¦æ³¨æ„"ç­‰è¯è¯­å¹³è¡¡å®£ä¼ è¯­æ°”
- é¿å…bullet pointsæˆ–åˆ—è¡¨,ä¸€å®šè¦æ˜¯æ®µè½æ ¼å¼

ä»¥JSONæ ¼å¼è¿”å›æ‘˜è¦:
```json
{{
  "summary": "ä½ çš„3-4æ®µè½æ‘˜è¦(ä¸­æ–‡,æ®µè½ä¹‹é—´ç”¨\\n\\nåˆ†éš”)",
  "fact_check": "passedæˆ–needs_verification",
  "char_count": 600,
  "paragraph_count": 4,
  "key_data_points": ["æ•°æ®ç‚¹1", "æ•°æ®ç‚¹2"],
  "core_argument": "æ–‡ç« çš„æ ¸å¿ƒè®ºç‚¹ç®€è¿°"
}}
```"""

    def _cache_articles(self, articles: List[Dict[str, Any]]):
        """
        Cache full article contexts before paraphrasing

        Saves to: ./data/cache/article_contexts/YYYYMMDD.json
        Format includes full original content and extracted entities
        """
        try:
            # Generate cache filename based on current date
            cache_filename = datetime.now().strftime("%Y%m%d.json")
            cache_path = self.cache_dir / cache_filename

            # Prepare cached data
            cached_data = {
                "report_date": datetime.now().strftime("%Y-%m-%d"),
                "generation_time": datetime.now().isoformat(),
                "articles": []
            }

            # Process each article
            for i, article in enumerate(articles):
                cached_article = {
                    "id": f"{i+1:03d}",  # 001, 002, etc.
                    "title": article.get('title', ''),
                    "url": article.get('url', ''),
                    "source": article.get('source', ''),
                    "published_date": article.get('published_date', ''),
                    "full_content": article.get('content', ''),
                    "credibility_score": article.get('credibility_score', 0),
                    "relevance_score": article.get('relevance_score', 0),
                    "entities": article.get('entities', {}),
                    "evaluation": article.get('evaluation', {})
                }
                cached_data["articles"].append(cached_article)

            # Save to file
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cached_data, f, ensure_ascii=False, indent=2)

            logger.info(f"Cached {len(articles)} articles to {cache_path}")

        except Exception as e:
            logger.error(f"Failed to cache articles: {e}")
            # Don't fail the entire process if caching fails

    def _cleanup_old_caches(self):
        """
        Delete cache files older than cache_retention_days

        Automatically cleans up old cached articles to manage disk space
        """
        try:
            cutoff_date = datetime.now() - timedelta(days=self.cache_retention_days)
            deleted_count = 0

            # Iterate through all JSON files in cache directory
            for cache_file in self.cache_dir.glob("*.json"):
                try:
                    # Parse date from filename (YYYYMMDD.json)
                    file_date = datetime.strptime(cache_file.stem, "%Y%m%d")

                    # Delete if older than retention period
                    if file_date < cutoff_date:
                        cache_file.unlink()
                        deleted_count += 1
                        logger.debug(f"Deleted old cache: {cache_file.name}")

                except ValueError:
                    # Skip files that don't match YYYYMMDD.json format
                    logger.warning(f"Skipping invalid cache filename: {cache_file.name}")
                    continue

            if deleted_count > 0:
                logger.info(f"Cleaned up {deleted_count} old cache file(s)")
            else:
                logger.debug("No old cache files to clean up")

        except Exception as e:
            logger.error(f"Failed to cleanup old caches: {e}")
            # Don't fail the entire process if cleanup fails


if __name__ == "__main__":
    # Test article paraphraser
    paraphraser = ArticleParaphraser()

    # Sample article
    sample_article = {
        'title': 'OpenAIå‘å¸ƒGPT-5ï¼Œæ€§èƒ½æå‡æ˜¾è‘—',
        'source': 'æœºå™¨ä¹‹å¿ƒ',
        'content': '''OpenAIä»Šæ—¥æ­£å¼å‘å¸ƒäº†å¤‡å—æœŸå¾…çš„GPT-5å¤§è¯­è¨€æ¨¡å‹ã€‚æ ¹æ®å®˜æ–¹å…¬å¸ƒçš„æµ‹è¯•ç»“æœï¼Œ
        GPT-5åœ¨æ¨ç†èƒ½åŠ›ã€æ•°å­¦é—®é¢˜æ±‚è§£ã€ä»£ç ç”Ÿæˆç­‰å¤šä¸ªç»´åº¦ä¸Šéƒ½å®ç°äº†æ˜¾è‘—æå‡ã€‚æ–°æ¨¡å‹æ”¯æŒæœ€é•¿500K tokensçš„ä¸Šä¸‹æ–‡çª—å£ï¼Œ
        ç›¸æ¯”GPT-4æå‡äº†5å€ã€‚OpenAI CEOè¡¨ç¤ºï¼ŒGPT-5æ ‡å¿—ç€é€šç”¨äººå·¥æ™ºèƒ½çš„é‡è¦é‡Œç¨‹ç¢‘ï¼Œ
        å°†é€šè¿‡APIå‘ä¼ä¸šå®¢æˆ·å¼€æ”¾ä½¿ç”¨ã€‚å®šä»·æ–¹é¢ï¼Œè¾“å…¥ä»·æ ¼ä¸ºæ¯ç™¾ä¸‡tokens $10ï¼Œè¾“å‡ºä»·æ ¼ä¸º$30ã€‚''',
        'published_date': '2024-10-20',
        'evaluation': {
            'key_takeaway': 'GPT-5å‘å¸ƒï¼Œæ€§èƒ½å¤§å¹…æå‡ï¼Œæ”¯æŒæ›´é•¿ä¸Šä¸‹æ–‡'
        }
    }

    result = paraphraser.paraphrase_articles([sample_article])
    print(f"\nOriginal length: {len(sample_article['content'])} chars")
    print(f"Summary length: {len(result[0]['paraphrased_content'])} chars")
    print(f"\nSummary:\n{result[0]['paraphrased_content']}")
