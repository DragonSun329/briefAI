#!/usr/bin/env python3
"""
Deduplication Analysis Tool

Analyzes the impact of deduplication thresholds and strategies.
Helps understand why 96.9% deduplication was happening and validates new settings.

Usage:
    python utils/dedup_analyzer.py
"""

from deduplication_utils import DeduplicationUtils
import json
from pathlib import Path


def analyze_thresholds():
    """Show what the current thresholds mean"""
    print("\n" + "="*80)
    print("DEDUPLICATION THRESHOLD ANALYSIS")
    print("="*80)

    print("\nüìä Current Thresholds (IMPROVED):")
    print(f"  ‚Ä¢ Title similarity:    {DeduplicationUtils.TITLE_SIMILARITY_THRESHOLD:.0%}")
    print(f"  ‚Ä¢ Content similarity:  {DeduplicationUtils.CONTENT_SIMILARITY_THRESHOLD:.0%}")
    print(f"  ‚Ä¢ Entity overlap:      {DeduplicationUtils.ENTITY_OVERLAP_THRESHOLD:.0%}")

    print("\nüîç What These Mean:")
    print(f"""
  Title Similarity = 0.88:
    ‚úÖ MATCHES: "Claude 3.5 Sonnet released" + "Claude 3.5 Sonnet announced"
    ‚ùå NO MATCH: "Claude 3.5 released" + "Claude 3 released"
    ‚ùå NO MATCH: "Claude updates" + "Sonnet improvements"

  Content Similarity = 0.80:
    ‚úÖ MATCHES: Same news from two sources with identical paragraphs
    ‚ùå NO MATCH: Same event covered with different angles/focus
    ‚ùå NO MATCH: Articles about different aspects of same company

  Entity Overlap = 0.75:
    ‚úÖ MATCHES: Articles about "Claude + Anthropic + Reasoning" with 75%+ entity overlap
    ‚ùå NO MATCH: Articles sharing only a few company names but different topics

  Combined Strategy (Default):
    - Any of the three matches = duplicate (OR logic)
    - Conservative: Catches obvious duplicates, preserves different angles

  Combined_Strict Strategy (Ultra-conservative):
    - ALL THREE must match = duplicate (AND logic)
    - Ultra-conservative: Only merge truly identical articles
    """)

    print("\nüìà Expected Impact on Deduplication Rate:")
    print("""
  OLD SETTINGS (0.75/0.65/0.60):
    - Aggressive: 96.9% deduplication (154/159 ‚Üí 5 articles kept)
    - Issue: Different angles on same event treated as duplicates
    - Problem: Articles like "Claude released", "Claude features", "Claude benchmarks" all merged

  NEW SETTINGS (0.88/0.80/0.75):
    - Balanced: ~30-40% deduplication expected (159 ‚Üí 95-110 articles)
    - Better: Different angles on same event preserved
    - Benefit: Readers see multiple perspectives on major stories
    - Smart: Still merges true duplicates from wire services

  COMBINED_STRICT SETTINGS:
    - Ultra-conservative: ~10-20% deduplication expected (159 ‚Üí 130-145 articles)
    - Most diversity: Only exact duplicates merged
    - Use when: Maximum article variety is critical
    """)


def estimate_reduction(total_articles=159, old_dedup_rate=0.969, expected_new_rate=0.35):
    """Estimate how many articles you'll keep with new settings"""
    print("\n" + "="*80)
    print("DEDUPLICATION RATE COMPARISON")
    print("="*80)

    old_kept = int(total_articles * (1 - old_dedup_rate))
    new_kept = int(total_articles * (1 - expected_new_rate))

    print(f"\nStarting with: {total_articles} raw articles")
    print(f"\nOLD SETTINGS (aggressive thresholds):")
    print(f"  Dedup rate: {old_dedup_rate:.1%}")
    print(f"  Articles kept: {old_kept}/{total_articles} ({(old_kept/total_articles)*100:.1f}%)")
    print(f"  Articles merged: {total_articles - old_kept} ({(1-old_kept/total_articles)*100:.1f}%)")

    print(f"\nNEW SETTINGS (conservative thresholds):")
    print(f"  Expected dedup rate: {expected_new_rate:.1%}")
    print(f"  Articles kept: {new_kept}/{total_articles} (~{(new_kept/total_articles)*100:.1f}%)")
    print(f"  Articles merged: {total_articles - new_kept} (~{(1-new_kept/total_articles)*100:.1f}%)")

    improvement = new_kept - old_kept
    print(f"\n‚úÖ IMPROVEMENT: +{improvement} more articles (~{(improvement/total_articles)*100:.1f}% more diversity)")

    print(f"\nNEW COMBINED_STRICT SETTINGS (ultra-conservative):")
    ultra_rate = 0.15
    ultra_kept = int(total_articles * (1 - ultra_rate))
    print(f"  Expected dedup rate: {ultra_rate:.1%}")
    print(f"  Articles kept: {ultra_kept}/{total_articles} (~{(ultra_kept/total_articles)*100:.1f}%)")
    print(f"  Improvement vs old: +{ultra_kept - old_kept} articles")


def show_threshold_sensitivity():
    """Show how sensitive results are to small threshold changes"""
    print("\n" + "="*80)
    print("THRESHOLD SENSITIVITY ANALYSIS")
    print("="*80)

    print("""
How much do small threshold changes matter?

Old Title Threshold: 0.75 (75%)
  "Claude 3.5 Sonnet released" vs "Claude 3.5 Sonnet announced"
  Similarity: ~0.82-0.85
  ‚ùå MARKED DUPLICATE (‚â•0.75)

New Title Threshold: 0.88 (88%)
  Same comparison: 0.82-0.85
  ‚úÖ NOT A DUPLICATE (<0.88)

Result: +1-5 articles kept per week instead of discarding related news!

---

This demonstrates why 0.75 was too aggressive:
  - Small wording differences = 2-10% similarity drop
  - 0.75 threshold catches too many false positives
  - 0.88 threshold catches only truly identical titles
    """)


if __name__ == "__main__":
    analyze_thresholds()
    estimate_reduction(total_articles=159, old_dedup_rate=0.969, expected_new_rate=0.35)
    show_threshold_sensitivity()

    print("\n" + "="*80)
    print("RECOMMENDATION FOR YOUR USE CASE")
    print("="*80)
    print("""
For executive briefings, you want:
  1. No duplicate wire service stories ‚úÖ
  2. Different angles on major stories ‚úÖ
  3. Diversity of sources and perspectives ‚úÖ

‚Üí Use "combined" strategy (default) with new thresholds (0.88/0.80/0.75)
  Expected result: 95-110 articles from 159 raw (35-40% deduplication)

If you want MAXIMUM diversity:
‚Üí Use "combined_strict" strategy
  Expected result: 130-145 articles from 159 raw (10-15% deduplication)

If you still see >50% dedup rate in practice:
‚Üí These may need further tuning based on actual data patterns
‚Üí Run a test batch and check the logs to see which duplicates are being caught
    """)

    print("\n")
