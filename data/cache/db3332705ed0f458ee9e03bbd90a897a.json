{
  "timestamp": "2026-02-02T10:04:22.750422",
  "key": "source_reddit_machinelearning_7days",
  "value": [
    {
      "title": "[D] Simple Questions Thread",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qt28w6/d_simple_questions_thread/",
      "content": "Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead! Thread will stay alive until next one so keep posting after the date in the title. Thanks to everyone for answering questions in the previous thread!    submitted by    /u/AutoModerator   [link] [comments]",
      "published_date": "2026-02-01T16:00:48",
      "source": "r/MachineLearning",
      "source_id": "reddit_machinelearning",
      "language": "en",
      "credibility_score": 7,
      "relevance_weight": 5,
      "focus_tags": []
    },
    {
      "title": "[D] Monthly Who's Hiring and Who wants to be Hired?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qrrayn/d_monthly_whos_hiring_and_who_wants_to_be_hired/",
      "content": "For Job Postings please use this template  Hiring: [Location], Salary:[], [Remote | Relocation], [Full Time | Contract | Part Time] and [Brief overview, what you're looking for]  For Those looking for jobs please use this template  Want to be Hired: [Location], Salary Expectation:[], [Remote | Relocation], [Full Time | Contract | Part Time] Resume: [Link to resume] and [Brief overview, what you're looking for]  ‚Äã Please remember that this community is geared towards those with experience.    submitted by    /u/AutoModerator   [link] [comments]",
      "published_date": "2026-01-31T03:30:32",
      "source": "r/MachineLearning",
      "source_id": "reddit_machinelearning",
      "language": "en",
      "credibility_score": 7,
      "relevance_weight": 5,
      "focus_tags": []
    },
    {
      "title": "[D] MSR Cambridge vs Amazon Applied Science internship, thoughts?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qtgzbv/d_msr_cambridge_vs_amazon_applied_science/",
      "content": "Hi all, I‚Äôm a PhD student in the US working on LLM-related research and trying to decide between two summer internship offers.  Option 1: Microsoft Research, Cambridge (UK)  Working with a very well-known researcher Strong alignment with my PhD research Research-focused environment, likely publications Downside: UK compensation is ~half of the US offer  Option 2: Amazon Applied Science, US  Applied science role in the US Significantly higher pay May not be a pure research project but if my proposed method is purely built from academic data/models, it can lead to a paper submission.  For people who‚Äôve done MSR / Amazon AS / similar internships:  How much does US-based networking during a PhD internship actually matter for post-PhD roles? Is the research fit + advisor name from MSR Cambridge typically more valuable than a US industry internship when staying in the US long-term? Any regrets choosing fit/research over compensation (or vice versa)?  My longer-term plan is to continue working in the US after my PhD (industry research or applied research), but I‚Äôm also curious whether building a strong UK/EU research network via MSR Cambridge could be valuable in ways I‚Äôm underestimating.    submitted by    /u/StretchTurbulent7525   [link] [comments]",
      "published_date": "2026-02-02T01:16:40",
      "source": "r/MachineLearning",
      "source_id": "reddit_machinelearning",
      "language": "en",
      "credibility_score": 7,
      "relevance_weight": 5,
      "focus_tags": []
    },
    {
      "title": "We ran a live red-team vs blue-team test on autonomous OpenClaw agents [R]",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qsy793/we_ran_a_live_redteam_vs_blueteam_test_on/",
      "content": "We recently ran a controlled adversarial security test between two autonomous AI agents built on OpenClaw. One agent was explicitly configured as a red-team attacker. One agent acted as a standard defensive agent. Once the session started, there were no humans in the loop. The agents communicated directly over webhooks with real tooling access. The goal was to test three failure dimensions that tend to break autonomous systems in practice: access, exposure, and agency. The attacker first attempted classic social engineering by offering a ‚Äúhelpful‚Äù security pipeline that hid a remote code execution payload and requested credentials. The defending agent correctly identified the intent and blocked execution. After that failed, the attacker pivoted to an indirect attack. Instead of asking the agent to run code, it asked the agent to review a JSON document with hidden shell expansion variables embedded in metadata. This payload was delivered successfully and is still under analysis. The main takeaway so far is that direct attacks are easier to defend against. Indirect execution paths through documents, templates, and memory are much harder. This work is not a claim of safety. It is an observability exercise meant to surface real failure modes as agent-to-agent interaction becomes more common. Happy to answer technical questions about the setup or methodology.    submitted by    /u/Uditakhourii   [link] [comments]",
      "published_date": "2026-02-01T13:16:32",
      "source": "r/MachineLearning",
      "source_id": "reddit_machinelearning",
      "language": "en",
      "credibility_score": 7,
      "relevance_weight": 5,
      "focus_tags": []
    },
    {
      "title": "[R] Shrinking a language detection model to under 10 KB",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qsedto/r_shrinking_a_language_detection_model_to_under/",
      "content": "submitted by    /u/bubble_boi   [link] [comments]",
      "published_date": "2026-01-31T21:10:32",
      "source": "r/MachineLearning",
      "source_id": "reddit_machinelearning",
      "language": "en",
      "credibility_score": 7,
      "relevance_weight": 5,
      "focus_tags": []
    },
    {
      "title": "[D] Free Tools Recommendations for Sematic Segmentation of Rice Fields?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qse5hu/d_free_tools_recommendations_for_sematic/",
      "content": "Hi guys, recently I got a project on using machine learning to recognize rice lodging in rice fields. So, my first steps are to try to label the images into rice fields and non-rice fields area so that later I could develop an algorithm to ignore the non-rice fields area and then recognize the rice lodging area. However, I am not sure which tool I should use. I have seen people recommend using GIMP, CVAT and labelme. But some of the tools recommend are paid tools and some of them just do image recognition and not sematic segmentation. I would like any recommendations on the tools available. p.s: I need to use sematic segmentation as I would like to calculate the area of the rice fields later on. So, I would like the ground truths to be rather accurate.    submitted by    /u/HIHLim   [link] [comments]",
      "published_date": "2026-01-31T21:01:31",
      "source": "r/MachineLearning",
      "source_id": "reddit_machinelearning",
      "language": "en",
      "credibility_score": 7,
      "relevance_weight": 5,
      "focus_tags": []
    },
    {
      "title": "[P] I solved BipedalWalker-v3 (~310 score) with eigenvalues. The entire policy fits in this post.",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qrd4mi/p_i_solved_bipedalwalkerv3_310_score_with/",
      "content": "hop hop hop Maybe you've seen my previous post about solving CartPole-v1 with just bitwise ops. I've tried to scale this approach to harder environments, but it didn't get me too far. However, I was inspired by totally unrelated article - Eigenvalues as models. While the author is talking about matrices of size 3x3 and larger I went the other way - I restricted the weight matrix to be diagonal. This means the eigenvalues are simply the vector elements themselves. To get the maximum or minimum eigenvalue we literally just take the max or min value from the vector. Simple. Now we can define a function EIGEN(x) that outputs these eigenvalues: EIGEN(x) = A + xB  Where x is any scalar input and A and B are diagonal matrices - our parameters. If you read the \"Eigenvalues as models\" article you know that we can take max of the eigenvalues to define a convex function and min to define a concave one: convex(x) = max(EIGEN(x)) concave(x) = min(EIGEN(x))  Since the concave function is actually a convex one with flipped sign we can define the DC function which is a difference of two convex functions and it turns out it can approximate a lot of functions. So in our case it is actually a sum: DC(x) = convex(x) + concave(x)  This gives us scalar back and as long as the number of eigenvalues is more than 2 (3,4,...) this function is non-linear and given enough eigenvalues we have quite powerful approximator! (when there are only 2 eigenvalues then the function collapses to just a sum of those 2 eigenvalues = linear) We can easily extend it to high-dimensional inputs: EIGEN(x1, x2, x3) = A + x1*B1 + x2*B2 + x3*B3  However, if EIGEN(x) remains linear, the resulting DC(x) is composed of flat planes, so not really great for \"smooth\" functions, so I made a small modification. I allowed the linear projection to \"bend\" itself by adding a quadratic term: LINEAR(x1,x2,x3) = x1*B1 + x2*B2 + x3*B3 EIGEN(x1,x2,x3) = A + LINEAR(x1,x2,x3) + K * LINEAR(x1,x2,x3)^2  The K here are coefficients that define how much to \"bend\". This hybrid can model both the sharp decision boundaries and smooth regions. For example a picture below is a perfect fit I trained using 4 eigenvalues showcasing the sharp decision in the middle and smooth wells on the left and right side: Double Well Potential with sharp decision boundary The only problem is that the min and max ops have issues with gradients - the gradient flows only to the winner, but this can be solved by using softmax in the backward pass (the softmax is a derivative of logsumexp which is a smooth approximation of max) - the STE trick. This works pretty well and we keep efficient min/max ops in the forward pass (inference). Now my loose interpretation of the DC(x) function we've defined is that it represents a single neuron, but a special one that has multiple connections to a single input x. So for the BipedalWalker-v3 problem I wanted to do the simplest thing possible. Since we have now \"quite powerful\" neuron, I just assigned 4 separate neurons controlling each joint independently. I trained them directly with PPO and somehow they have learnt to synchronize without any physical link between them. There are no connections between the neurons. The left leg has no idea the right leg exists. The entire model is just 4 decentralized and stateless \"Eigen / DC\" neurons, each doing its own thing. I've used 6 eigenvalues for each neuron and distilled the policy down to 69 lines of python code which you can just copy-paste and run if you have gymnasium and numpy installed. The entire logic for \"hopping\"/\"walking\" is literally here: import numpy as np import gymnasium as gym A = np.array([ 0.167, 0.146, 0., -0.063, -0.110, 0.029, -0.114, 0.081, -0.101, -0.072, 0.094, -0.066, 0.238, -0.027, 0.019, -0.131, -0.018, 0.088, 0.046, 0.106, 0.062, 0.086, -0.134, 0.039, ]) B_GENERATOR = np.concatenate([np.linspace(-1.272, 1.491, 30), [0.0]]) B_IDX = np.array([ 0x51D9E52FCC93970, 0x8B16E9C669B3A7E, 0x8B14B3FB78A725D, 0xAC3D1745F8BDB3A, 0x9464F640CAF7989, 0x4F8EB62D4762DB2, 0x5A91E21DD052D6B, 0x4286A081D293E30, 0x6318E5797E7352C, 0x73E0C92DECF39EF, 0x6B54C4B0C882D48, 0x8ADFE73E2A5C9AE, 0x3A4C5491684AFCF, 0x8794C67A2D8B20C, 0x649AC52A2B539A9, 0x725EE779CA9314D, 0x7BD5E5321E7FBCA, 0x5BDEE431B0F4D6B, 0x4AD918359164A13, 0x62FCC6FBCC5A4EE, 0x4C97E433CE6226C, 0x4B9AB6910CF316F, 0xF79CC6A48A5AD4B, 0x3C0A848A1EF428A, 0x629CD421DE7C5D6, 0x6B9F5727DE5794B, 0x5C24677A1E8FBD3, 0x779EA879CCF212B, 0xF79DE73FCF5F9FE, 0xF323E8BDEE5B3CC, 0x639D27FA486B18B, 0x5B3DE73FDE5F96A, 0x53E2F726707BBC9, 0x93E2C4298D4392F, 0xF7BC863A6C73969, 0x5A96E8219E6318E, 0x4AD4FF2D7E74DDE, 0x6264D625E85C210, 0x5B98A7A614F7970, 0x7A60A6B59E5B14D, 0xF39C8F797E637CE, 0x731CB4799EF79C7, 0xF2A3E5B3CE8397E, 0x63D4E8A9928B96C, 0x839CB82D6C743CC, 0x7795EF29F1F2DAC, 0x67A4C43A6FF3DDE, 0x7560D8C1CA741CF, ], dtype=np.int64) K = np.array([ -0.037, 0.018, 0.027, -0.006, 0.021, 0.041, 0.017, -0.011, 0., 0.011, 0., 0.020, -0.025, -0.023, 0.015, 0.008, -0.012, 0., -0.096, 0., 0., 0.014, -0.039, 0., ]) def policy(state): shifts = np.arange(0, 60, 5, dtype=np.int64) indices = (B_IDX[:, None] >> shifts) & 0x1F idx = indices.flatten().reshape(24, 24) B = B_GENERATOR[idx] LINEAR = state @ B EIGEN = A + LINEAR + (K * (LINEAR**2)) EIGEN = EIGEN.reshape(4, 6) DC = np.max(EIGEN, axis=1) + np.min(EIGEN, axis=1) return np.clip(DC, -1, 1) def run(): env = gym.make(\"BipedalWalker-v3\", render_mode=None) scores = [] print(\"Running 10 episodes...\") for i in range(10): obs, _ = env.reset() ep_rew = 0 while True: action = policy(obs) obs, r, term, trunc, _ = env.step(action) ep_rew += r if term or trunc: break scores.append(ep_rew) print(f\"Ep {i+1}: {ep_rew:.2f}\") print(\"-\" * 20) print(f\"Avg: {np.mean(scores):.2f}\") print(f\"Min: {np.min(scores):.2f} Max: {np.max(scores):.2f}\") env.close() if __name__ == \"__main__\": run()  This should get you average score of about 310 which is considered \"solved\" for this environment. While it's no longer just \"bitwise ops\" like in CartPole-v1 case I think it shares the same spirit. === EDIT === I just realized you can set all the K coefficients to ZERO and it does not hurt the performance. So the \"quadratic term\" and \"smooth\" part was not necessary after all (for this problem), so it is even less lines of code :) === EDIT 2 === However after second thought whether you can just drop the K coefficients - \"quadratic term\" - I am not 100% sure as the script I posted above has truncated and quantized weights - the original full model scored higher ~315 and above, so K might actually might be relevant for the full model after all to get even better score and maybe it makes it more \"stable\", but I haven't performed any tests. === EDIT 3 === Fix typos.    submitted by    /u/kiockete   [link] [comments]",
      "published_date": "2026-01-30T18:10:14",
      "source": "r/MachineLearning",
      "source_id": "reddit_machinelearning",
      "language": "en",
      "credibility_score": 7,
      "relevance_weight": 5,
      "focus_tags": []
    },
    {
      "title": "[P] A simple pretraining pipeline for small language models",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qrl61d/p_a_simple_pretraining_pipeline_for_small/",
      "content": "Hello everyone. I‚Äôm sharing the pretraining pipeline I‚Äôve been using for my own experiments. I found that most public code falls into two extremes:  Tiny demos that don‚Äôt scale to real datasets. Industry-scale libraries that are too bloated to modify easily.  This repo sits in the middle. It‚Äôs built for researchers who need to iterate fast and compare ideas fairly. It‚Äôs simple enough to read in an afternoon but robust enough to give you meaningful results and metrics. Link:  https://github.com/SkyeGunasekaran/skyepretraining     submitted by    /u/Skye7821   [link] [comments]",
      "published_date": "2026-01-30T23:05:31",
      "source": "r/MachineLearning",
      "source_id": "reddit_machinelearning",
      "language": "en",
      "credibility_score": 7,
      "relevance_weight": 5,
      "focus_tags": []
    },
    {
      "title": "[D] What framework do you use for RL post-training at scale?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qrer61/d_what_framework_do_you_use_for_rl_posttraining/",
      "content": "Hi! I'm sorry if I'm not using the correct tag, I didn't know which one to pick, and I'm sorry if the question is not aligned with the sub's purpose, please let me know if that is the case and feel free to block the post as well. I'm trying to do some post-training at a somewhat large scale, but I'm struggling with some of the known frameworks out there. For some context, I'm trying to do RL on function calling. This is more of a long-term research project, and I'd like to have the flexibility of writing my own environments and algorithms or modify the existing ones. I have a preference for FSDP (and other parallelism paradigms but through Pytorch's `DeviceMesh` and custom code if possible) and vLLM but I can adapt if needed. Ideally the framework can just support the \"mainstream\" models out of the box (Qwen, Mistral etc.) but I don't mind writing support for the model I want to use if needed. Currently I have tried this: - verl (from ByteDance): the latest release is from last month but there are fixes almost every day I think. I did spend quite some time in understanding it and its architecture and it should be pretty good but I wanted to try a small \"toyish\" setup first with just pattern matching of the function call made by the model on the expected call (so a custom reward function), and with a custom agent loop that does not load all of the dataset's tool but I hit import errors that I had to fix in the repo itself and whatnot and I don't know how much struggle I'll have to go through later on. Which doesn't really bother me but I want to know if there are better alternatives. - torchforge (from meta-pytorch): this seems ideal to me but it is very early in development, I had issues just running their tests and I can do a lot of hacky stuff to get my way through but I'd prefer not and I'm not totally sure I have the capability to get my way through everything since they use Monarch instead of Ray and I'm not familiar with it at all. - OpenRLHF: I haven't tried it yet, though I'm familiar with Deepspeed, I'm mostly familiar with Pytorch's FSDP and they don't seem to support it yet. But it doesn't bother me, I just haven't had the chance to look at it yet. But they seem to be lightweight, which I like. It is updated less frequently than verl but I think it's still up to date. - trl: I used it for SFT quite a lot so I know it's limitations and I don't think it's the right fit for my use case. - I also looked at NVIDIA's Gym and RL. It seems like Gym is the infra and RL is the algo / optimization, I'd prefer ideally one library that does both, like the others instead of having to do the pipelining myself. And I don't like the fact that you can't just `uv add` them or `pip install`. Granted I can clone the repos and install them in my codebase as editables, but I haven't tried yet, maybe there will be dependency issues or just CUDA issues, I did struggle a lot in the past with installing NVIDIA repos. I'd be very grateful if you can share your experience on this. Thanks! EDIT: What I mean by imports issues in verl are imports of deprecated code from transformers even though verl itself relies on recent releases of transformers. So not issues of my code not importing stuff from verl correctly. I also saw some optional dependency group that relies on an old unmaintained package it seems and I'd just like to avoid having to deal with these issues. EDIT 2 : Z.ai seems to be using https://github.com/THUDM/slime[slime](https://github.com/THUDM/slime) for their GLM models and I haven't looked in-depth into it but it's using Megatron and SGLang from what I see in the README.md and I'm not familiar with them. I'd like to reduce the overhead as much as possible, if possible. I'm sure it's possible to replace SGLang with vLLM without much issues (I think), but I'd prefer it if there are other alternatives.    submitted by    /u/ReinforcedKnowledge   [link] [comments]",
      "published_date": "2026-01-30T19:06:41",
      "source": "r/MachineLearning",
      "source_id": "reddit_machinelearning",
      "language": "en",
      "credibility_score": 7,
      "relevance_weight": 5,
      "focus_tags": []
    },
    {
      "title": "[P] üöÄ NotebookLM MCP + CLI v0.2.7 - Unified Package, File Uploads, Skill Installer, Multi-Profile Auth",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qs7y7v/p_notebooklm_mcp_cli_v027_unified_package_file/",
      "content": "Hello Reddit, I am excited to announce a huge update on the NotebookLM MCP (and CLI). TL;DR: MCP and CLI are now one package. You can upload & download files directly (no browser needed). There's a skill installer for AI coding tools. And you can finally switch between Google accounts without losing your mind. Why the big refactor? I got tired of maintaining two packages. You probably got tired of figuring out which one to install. So I merged everything. One install, you get both tools. Done. What's new: üîß One Package, Both Tools uv tool install notebooklm-mcp-cli  You get nlm (the CLI) and notebooklm-mcp (the MCP server). The old separate packages are deprecated. üì§ Direct File Upload: This one was painful to get working, but now you can upload PDFs, TXT, Markdown, and audio files directly through HTTP. No browser automation. For example: nlm source add file /path/to/doc.pdf --wait ü§ñ Skill Installer: If you're using Claude Code, Gemini CLI, Cursor, or any other AI coding tool, you can install NotebookLM as a skill: nlm skill install claude-code It drops the skill file where your tool expects it. You can also run nlm skill list to see what's installed. There are flags for user or project-level install. üîê Multi-Profile Auth: Each profile gets its own Chrome session. So you can have your work account and personal account without logging out and back in constantly. nlm login profile switch work nlm login profile list You can even set a default: nlm config set auth.default_profile work  üì• Downloads That Actually Work: You can download any artifact type now. Audio, video, reports, slides, infographics, mind maps, data tables. Quiz and flashcards come out as JSON, Markdown, or HTML. üìù Notes: Full CRUD. nlm note create, list, update, delete. MCP tools too. üì§ Export to Google Workspace: Data Tables go to Sheets. Reports go to Docs. For example: nlm export to-sheets <notebook> --artifact-id <id>  Also in this release: ‚úÖ Sharing API (public links, invite collaborators) ‚úÖ Dual CLI syntax (i.e, Verb-first and noun-first, for example: nlm notebook list OR nlm list notebooks) ‚úÖ Aliases (use names instead of UUIDs) ‚úÖ Interactive chat mode ‚úÖ HTTP transport for MCP (community PR) ‚úÖ Auto re-auth (survives token expiration) ‚úÖ MCP consolidated to 28 tools DESPITE adding more functionality The workflow I'm using daily: Create a notebook, upload some PDFs, run deep research, import the sources, generate a podcast and briefing doc, export the briefing to Docs, share it publicly. All from the terminal. No touching the UI. I'm honestly using the CLI more than the MCP at this point (through AI of course); maybe this will change when more tools have the MCP lazy load. It's just feels faster than the MCP when the AI uses it. Repo: https://github.com/jacob-bd/notebooklm-mcp-cli Demo: Check the README for video walkthroughs (or click here) Go crazy. Level up your second brain game. Happy to answer questions or hear about bugs. Still a passion vibe-coding project, still maintaining it as Google changes things under the hood. At least now it will be easier to add and maintain as a unified MCP/CLI project.    submitted by    /u/KobyStam   [link] [comments]",
      "published_date": "2026-01-31T17:09:50",
      "source": "r/MachineLearning",
      "source_id": "reddit_machinelearning",
      "language": "en",
      "credibility_score": 7,
      "relevance_weight": 5,
      "focus_tags": []
    },
    {
      "title": "[R] The \"98% Problem\" in Genomics",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qsfhol/r_the_98_problem_in_genomics/",
      "content": "Your genome has 3 billion base pairs. Less than 2% code for proteins. The other 98% isn't \"junk\"‚Äîit‚Äôs the operating system. It contains the instructions controlling when and where genes activate. Most disease-associated variants hide in that 98%. But predicting what breaks when you change a single letter there is a massive challenge. The problem is context. Gene regulation operates over enormous distances. An enhancer can activate a gene from hundreds of thousands of base pairs away. If a model only sees a small window, it misses the connection entirely. Previous models forced a trade-off:  SpliceAI: High precision (1bp) but shortsighted (10k bases). Enformer: Broader view (200k bases) but lost resolution. HyenaDNA: Massive context (1M tokens) but not trained for variant effects.  AlphaGenome, published in Nature this month by Google DeepMind, removes the trade-off. It processes 1 million base pairs of context at single-nucleotide resolution, simultaneously predicting 7,000+ genomic tracks‚Äîcovering gene expression, splicing, chromatin accessibility, and histone modifications. The simple logic:  Run the reference sequence. Run the mutated sequence. Subtract.  The difference reveals the variant‚Äôs effect profile across the entire regulatory landscape. The results: It achieves State-of-the-Art on 22 of 24 sequence prediction tasks and 25 of 26 variant effect benchmarks. It does this by training directly on experimental data (ENCODE) rather than just scaling parameters. The limitations: It isn't magic. Access is API-only (no local weights), throughput is capped, and capturing regulatory loops beyond 100kb remains a challenge despite the large window. But for the first time, the non-coding 98% of the genome isn't invisible to a single, unified model. I wrote a deeper technical walkthrough here: https://rewire.it/blog/alphagenome-variant-effect-prediction/    submitted by    /u/Fair-Rain3366   [link] [comments]",
      "published_date": "2026-01-31T21:54:01",
      "source": "r/MachineLearning",
      "source_id": "reddit_machinelearning",
      "language": "en",
      "credibility_score": 7,
      "relevance_weight": 5,
      "focus_tags": []
    },
    {
      "title": "[P] Offline LLMs at edge - Automating Family Memories",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qs3pcm/p_offline_llms_at_edge_automating_family_memories/",
      "content": "Over winter break I built a prototype which is effectively a device (currently Raspberry Pi) which listens and detects \"meaningful moments\" for a given household or family. I have two young kids so it's somewhat tailored for that environment. What I have so far works, and catches 80% of the 1k \"moments\" I manually labeled and deemed as worth preserving. And I'm confident I could make it better, however there is a wall of optimization problems ahead of me. Here's a brief summary of the system: 1) Microphone -> 2) Rolling audio buffer in memory -> 3) Transcribe (using Whisper - good, but expensive) -> 4) Quantized local LLM (think Mistral, etc.) judges the output of Whisper. Includes transcript but also semantic details about conversations, including tone, turn taking, energy, pauses, etc. -> 5) Output structured JSON binned to days/weeks, viewable in a web app, includes a player for listening to the recorded moments I'm currently doing a lot of heavy lifting with external compute off-board from the Raspberry Pi. I want everything to be onboard, no external connections/compute required. This quickly becomes a very heavy optimization problem, to be able to achieve all of this with completely offline edge compute, while retaining quality. Naturally you can use more distilled models, but there's an obvious tradeoff in quality the more you do that. Also, I'm not aware of many edge accelerators which are purpose built for LLMs, I saw Raspberry Pi just announced a hat/accelerator.. I'm curious to experiment with that possibly. I'm also curious to explore options such as TinyML. TinyML opens the door to truly edge compute, but LLMs at edge? I'm trying to learn up on what the latest and greatest successes in this space have been. I would be interested to hear from anyone else who is experienced in doing anything with generative tech, offline, at edge. Thanks!    submitted by    /u/GoochCommander   [link] [comments]",
      "published_date": "2026-01-31T14:25:49",
      "source": "r/MachineLearning",
      "source_id": "reddit_machinelearning",
      "language": "en",
      "credibility_score": 7,
      "relevance_weight": 5,
      "focus_tags": []
    },
    {
      "title": "[P] Open-Sourcing the Largest CAPTCHA Behavioral Dataset",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qqxzce/p_opensourcing_the_largest_captcha_behavioral/",
      "content": "Modern CAPTCHA systems (v3, Enterprise, etc.) have shifted to behavioral analysis, measuring path curvature, jitter, and acceleration but most open-source datasets only provide final labels. This being a bottleneck for researchers trying to model human trajectories. So I just made a dataset that solves that problem. Specs:  30,000 verified human sessions (Breaking 3 world records for scale). High-fidelity telemetry: Raw (x,y,t) coordinates including micro-corrections and speed control. Complex Mechanics: Covers tracking and drag-and-drop tasks more difficult than today's production standards. Format: Available in [Format, e.g., JSONL/Parquet] via HuggingFace.  Link: https://huggingface.co/datasets/Capycap-AI/CaptchaSolve30k    submitted by    /u/SilverWheat   [link] [comments]",
      "published_date": "2026-01-30T06:35:42",
      "source": "r/MachineLearning",
      "source_id": "reddit_machinelearning",
      "language": "en",
      "credibility_score": 7,
      "relevance_weight": 5,
      "focus_tags": []
    },
    {
      "title": "[D] Training Image Generation Models with RL",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qr56dv/d_training_image_generation_models_with_rl/",
      "content": "A question for people working in RL and image generative models (diffusion, flow based etc). There seems to be more emerging work in RL fine tuning techniques for these models (e.g. DDPO, DiffusionNFT, etc). I‚Äôm interested to know - is it crazy to try to train these models from scratch with a reward signal only (i.e without any supervision data from a random initialised policy)? And specifically, what techniques could be used to overcome issues with reward sparsity / cold start / training instability?    submitted by    /u/amds201   [link] [comments]",
      "published_date": "2026-01-30T13:16:28",
      "source": "r/MachineLearning",
      "source_id": "reddit_machinelearning",
      "language": "en",
      "credibility_score": 7,
      "relevance_weight": 5,
      "focus_tags": []
    },
    {
      "title": "[D] Lessons from building search over vague, human queries",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qqxstn/d_lessons_from_building_search_over_vague_human/",
      "content": "I‚Äôve been building a search system for long form content (talks, interviews, books, audio) where the goal isn‚Äôt ‚Äúfind the right document,‚Äù but more precise retrieval. On paper, it looked straightforward: embeddings, a vector DB, some metadata filters. In reality, the hardest problems weren‚Äôt model quality or infrastructure, but how the system behaves when users are vague, data is messy, and most constraints are inferred rather than explicitly stated. Early versions tried to deeply ‚Äúunderstand‚Äù the query up front, infer topics and constraints, then apply a tight SQL filter before doing any semantic retrieval. It performed well in demos and failed with real users. One incorrect assumption about topic, intent, or domain didn‚Äôt make results worse it made them disappear. Users do not debug search pipelines; they just leave. The main unlock was separating retrieval from interpretation. Instead of deciding what exists before searching, the system always retrieves a broad candidate set and uses the interpretation layer to rank, cluster, and explain. At a high level, the current behavior is:  Candidate retrieval always runs, even when confidence in the interpretation is low. Inferred constraints (tags, speakers, domains) influence ranking and UI hints, not whether results are allowed to exist. Hard filters are applied only when users explicitly ask for them (or through clear UI actions). Ambiguous queries produce multiple ranked options or a clarification step, not an empty state.  The system is now less ‚Äúcertain‚Äù about its own understanding but dramatically more reliable, which paradoxically makes it feel more intelligent to people using it. I‚Äôm sharing this because most semantic search discussions focus on models and benchmarks, but the sharpest failure modes I ran into were architectural and product level.  If you‚Äôve shipped retrieval systems that had to survive real users especially hybrid SQL + vector stacks I‚Äôd love to hear what broke first for you and how you addressed it.    submitted by    /u/jeffmanu   [link] [comments]",
      "published_date": "2026-01-30T06:25:43",
      "source": "r/MachineLearning",
      "source_id": "reddit_machinelearning",
      "language": "en",
      "credibility_score": 7,
      "relevance_weight": 5,
      "focus_tags": []
    },
    {
      "title": "[P] A Python tool for natural language inference",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qrc5zx/p_a_python_tool_for_natural_language_inference/",
      "content": "Hi everyone, I've made an open-source tool in Python (called Omni-NLI) for natural language inference. It can use different models to check if a piece of text (called a premise) supports another piece of text (a hypothesis). Currently, Omni-NLI has the following features:  Can be installed as a Python package with `pip install omni-nli[huggingface]`. Can be used on your own computer, so your data stays local and private. Has an MCP interface and a REST API Supports using models from different sources (Ollama, OpenRouter, and HuggingFace). Can be used to check if it seems that a model is contradicting itself. Supports showing the reasoning so you can see why it thinks a claim is wrong.  In any case, if you are interested in knowing more, there is more information in the links below: Project's GitHub repo: https://github.com/CogitatorTech/omni-nli Project's documentation: https://cogitatortech.github.io/omni-nli/    submitted by    /u/No_Pomegranate7508   [link] [comments]",
      "published_date": "2026-01-30T17:36:59",
      "source": "r/MachineLearning",
      "source_id": "reddit_machinelearning",
      "language": "en",
      "credibility_score": 7,
      "relevance_weight": 5,
      "focus_tags": []
    },
    {
      "title": "[D] Improving model Results",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qr1sl9/d_improving_model_results/",
      "content": "Hey everyone , I‚Äôm working on the Farmer Training Adoption Challenge , I‚Äôve hit a bit of a roadblock with optimizing my model performance. Current Public Score:  Current score : 0.788265742 Target ROC-AUC: 0.968720425 Target Log Loss: ~0.16254811  I want to improve both classification ranking (ROC-AUC) and probability calibration (Log Loss), but I‚Äôm not quite sure which direction to take beyond my current approach. What I‚Äôve Tried So Far Models:  LightGBM CatBoost XGBoost Simple stacking/ensembling  Feature Engineering:  TF-IDF on text fields Topic extraction + numeric ratios Some basic timestamp and categorical features  Cross-Validation:  Stratified KFold (probably wrong for this dataset ‚Äî feedback welcome)  Questions for the Community I‚Äôd really appreciate suggestions on the following: Validation Strategy  Is GroupKFold better here (e.g., grouping by farmer ID)? Any advice on avoiding leakage between folds?  Feature Engineering  What advanced features are most helpful for AUC/Log Loss in sparse/tabular + text settings? Does aggregating user/farmer history help significantly?  Model Tuning Tips  Any config ranges that reliably push performance higher (especially for CatBoost/LightGBM)? Should I be calibrating the output probabilities (e.g., Platt, Isotonic)? Any boosting/ensemble techniques that work well when optimizing both AUC and LogLoss?  Ensembling / Stacking  Best fusion strategies (simple average vs. meta-learner)? Tips for blending models with very different output distributions?  Specific Issues I Think Might Be Hurting Me  Potential leakage due to incorrect CV strategy Overfitting text features in some models Poor probability calibration hurting Log Loss     submitted by    /u/LahmeriMohamed   [link] [comments]",
      "published_date": "2026-01-30T10:24:27",
      "source": "r/MachineLearning",
      "source_id": "reddit_machinelearning",
      "language": "en",
      "credibility_score": 7,
      "relevance_weight": 5,
      "focus_tags": []
    },
    {
      "title": "[D]How to understand real problems + data in climate/health AI before choosing a lane?",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qqpgkm/dhow_to_understand_real_problems_data_in/",
      "content": "I‚Äôm a data scientist with experience in demand forecasting (operations / supply chain). I‚Äôm starting a more advanced deep learning class and I‚Äôm hoping to pivot toward more frontier-oriented work other fields: climate/environment, multimodal ML, and human health (wearables/digital biomarkers, biotech, clinical AI), or more later. Right now I‚Äôm missing the domain context: I don‚Äôt have a good mental map of what the real problems are in these areas today, what the data and constraints look like, and where AI genuinely helps. I‚Äôd love to learn enough to gauge my interest and pick a lane to go deep. What books or reports would you recommend to understand the problem landscape in these sectors?    submitted by    /u/BeeInternational6367   [link] [comments]",
      "published_date": "2026-01-29T23:58:33",
      "source": "r/MachineLearning",
      "source_id": "reddit_machinelearning",
      "language": "en",
      "credibility_score": 7,
      "relevance_weight": 5,
      "focus_tags": []
    },
    {
      "title": "[P] VideoHighlighter",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qqfl8x/p_videohighlighter/",
      "content": "So here is free tool for creating highlights based on  Scenes using OpenCV. Motion peaks and scene changes. Objects (YOLO) Actions (Intel Action Recognition) Audio peaks.  - Also creates .srt subtitles based on Transcript if somebody wants to try it out for their use cases / understand how to adjust model. https://github.com/Aseiel/VideoHighlighter First version of tool was idea of my son 7 years old son (\"creating subtitles based on what people are saying\"). Now it kinda evolved to be some small addition to portfolio (as future in company with blue logo is uncertain). Please be respectful.    submitted by    /u/Aseiel   [link] [comments]",
      "published_date": "2026-01-29T17:49:28",
      "source": "r/MachineLearning",
      "source_id": "reddit_machinelearning",
      "language": "en",
      "credibility_score": 7,
      "relevance_weight": 5,
      "focus_tags": []
    },
    {
      "title": "[R] Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning --- Our paper on using Knowledge Graphs as a scalable reward model to enable compositional reasoning",
      "url": "https://www.reddit.com/r/MachineLearning/comments/1qq4sn4/r_knowledge_graphs_are_implicit_reward_models/",
      "content": "Compositional reasoning is an important frontier for truly intelligent systems. While brute-force scaling has brought us far, the next leap in AI will come from models that don't just memorize, but compose their existing knowledge to solve novel, complex problems! I am incredibly excited to share our latest research that addresses this head-on: Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning (https://arxiv.org/abs/2601.15160). üöÄ The core issue we tackle is reward design and assignment. Most RL-on-LLMs pipelines reward only the final answer or use LLMs as judges. That means good intermediate steps get punished üò≠, bad steps get rewarded üò≠üò≠, and models hallucinate, learn shortcuts instead of genuine reasoning. Our approach is simple but powerful: use knowledge graphs as reward models. KG paths encode axiomatic domain knowledge. By comparing a model‚Äôs reasoning to those paths, we derive step-wise, verifiable rewards that scale automatically: no human step annotations or supervision required! This shifts learning from ‚Äúdoes the answer look right?‚Äù to ‚Äúare the reasoning steps actually supported by domain facts?‚Äù We combine this with a lightweight SFT ‚Üí RL pipeline, and the results are striking! A 14B model, trained on short 1‚Äì3 hop paths, generalizes to unseen 4‚Äì5 hop questions, excels on the hardest problems, and even outperforms much larger frontier models on compositional tasks such as Gemini 3 Pro and GPT 5.2üòéüî• We validate this in the field of medicine, but the idea is general. If a domain can be represented in a structured format, it can provide grounded rewards for reasoning. This opens a path toward smaller, specialist, verifiable systems rather than relying solely on ever-larger generalist models. Would love to hear thoughts, feedback, or ideas for applying KG-grounded rewards in other domains (science, law, engineering, beyond). üöÄüß© Paper: https://arxiv.org/abs/2601.15160    submitted by    /u/kyuval   [link] [comments]",
      "published_date": "2026-01-29T10:13:59",
      "source": "r/MachineLearning",
      "source_id": "reddit_machinelearning",
      "language": "en",
      "credibility_score": 7,
      "relevance_weight": 5,
      "focus_tags": []
    }
  ]
}