{
  "timestamp": "2026-01-29T09:41:06.406442",
  "key": "source_venturebeat_7days",
  "value": [
    {
      "title": "Factify wants to move past PDFs and .docx by giving digital documents their own brain",
      "url": "https://venturebeat.com/infrastructure/factify-wants-to-move-past-pdfs-and-docx-by-giving-digital-documents-their",
      "content": "Tel Aviv-based startup Factify emerged from stealth today with a $73 million seed round for an ambitious, yet quixotic mission: to bring digital documents beyond the standard formats most businesses use — .PDF, .docx, collaborative cloud files like Google Docs — and into the intelligence era.For Matan Gavish, Factify’s Founder and CEO, this isn't just a software upgrade—it is an inevitability he has been obsessed with for years.\"The PDF was developed when I was in elementary school,\" Gavish told VentureBeat. \"The bedrock of the software ecosystem hasn't really evolved... someone has to redesign the digital document itself.\"Gavish, a tenured professor of computer science and Stanford PhD, admits that his fixation on administrative file formats is an anomaly for someone with his credentials. \"It's a very uncool problem to be obsessed with,\" he says. \"Given the fact that my academic background is AI and machine learning, my mom wanted me to start an AI company because it's cool. I'm not sure why I'm obsessed and then possessed by documents.\"But that obsession has now attracted a sizeable seed round led by Valley Capital Partners and backed by AI heavyweights like former Google AI chief John Giannandrea.The bet is simple the static rigidity of most digital files has limited their utility, and a better, more intelligent document that actually shares its edit history and ownership with users as intended, is not only possible — it's a multi-billion-dollar opportunity.  The history of digital documentsTo understand why a seed round would balloon to $73 million, you have to understand the scale of the trap businesses are in. There are currently an estimated three trillion PDFs in circulation. \"Some people see the PDF more than they see their kids,\" Gavish jokes.The history of the digital document is not a linear progression where one format replaces another. Instead, it is a story of \"speciation,\" where different formats evolved to fill distinct ecological niches: creation, distribution, and collaboration.The era of files: Microsoft Word (1980s–1990s)Digital documents began as isolated artifacts. In the 1980s, the \"document\" was inextricably linked to the hardware that created it. A file created in WordPerfect on a DOS machine was effectively gibberish to a Macintosh user.Microsoft Word, which traces its lineage to the pioneering WYSIWYG editors at Xerox PARC, changed this by leveraging the dominance of the Windows operating system. By the 1990s, the binary .doc format became the default container for editable professional documents. However, these files were structurally complex \"memory dumps\" designed for the limited hardware of the time, often leading to corruption or privacy leaks where deleted text remained hidden in the file's binary data.The era of digital 'stone': the PDF (1990s-2006)The PDF did not originate as a tool for writing; it was a tool for viewing. In 1991, Adobe co-founder John Warnock penned the \"Camelot Project\" white paper, envisioning a \"digital envelope\" that would look identical on any display or printer.Unlike Word files, which were malleable, PDFs were designed to be immutable. They used the PostScript imaging model to place characters at precise coordinates, ensuring visual fidelity. While adoption was initially slow, Adobe’s 1994 decision to release the Acrobat Reader for free established PDF as the global standard for \"digital concrete\"—the format of finality used for contracts, government forms, and archives.The collaborative cloud docs era (2006-present)In 2006, Google disrupted the model again by moving the document from the hard drive to the browser. Using \"Operational Transformation\" algorithms, Google Docs allowed multiple users to edit the same stream of text simultaneously.This shifted the paradigm from \"sending a file\" to \"sharing a link.\" While Google Workspace now claims over 3 billion users (mostly consumers and education), it fundamentally changed how we work—turning documents into living, collaborative processes rather than static artifacts.The status quo: fragmentationDespite these advances, the business world remains fragmented. We draft in Google Docs (the \"Digital Stream\"), format in Word (the \"Digital Clay\"), and sign in PDF (the \"Digital Stone\").But this fragmentation has a cost. \"The problem is not the document. It is everything around it,\" the company notes. \"Once a PDF leaves your system, control is gone. Versions drift. Access is unclear. Nothing is visible.\"Turning digital documents into intelligent infrastructureFactify’s wager is that in the age of AI, this fragmentation is no longer just annoying—it is a critical failure. AI models need structured, verifiable data to function. When an AI \"reads\" a PDF, it is essentially guessing, using optical character recognition to scrape text from what is effectively a digital photo.\"What we're dealing with here is a megalomaniac vision, but it's at the same time probably something that is inevitable,\" Gavish says.Factify’s solution is to treat documents not as static files, but as intelligent infrastructure. In the \"Factified\" standard, a document carries its own brain. It possesses a unique identity, a live permission system, and an immutable audit log that travels with it.\"We wrote a new document format that supplants the PostScript,\" Gavish explains. \"We created a new data layer that supports the document as a first class citizen... and it's always available inside the organization and potentially outside.\"This distinction—between a File and an API—is the core of the company's pitch\"Files are liabilities: They accumulate, get lost, and can be stolen. \"It goes back to a brick status,\" Gavish says. \"Files are liabilities, if anything, because they just accumulate there, you have to guard them.\"APIs are assets: A Factify document is an active object. You can ask it questions: \"Who has seen you? When do you expire? Are you the most up-to-date version?\"'People don't change', but formats doHistory is littered with formats that tried to replace the PDF (like Microsoft’s XPS). They failed because they demanded too much behavioral change from users. Gavish is keenly aware of this trap.\"When I talk to enterprise software entrepreneurs, I tell them the two laws to know about starting a company in enterprise software is that people don't care, and no one changes,\" he says.To skirt this, Factify has built deep backwards compatibility. A Factified document can look exactly like a PDF, complete with page breaks and margins. Users don't need to learn a new interface to get value; they just need to solve a specific pain point—like an executive who wants to ensure an investment memo can’t be forwarded.\"All they have to tell their team is, 'Dear Chief of Staff, employment agreements and investment memoranda... are going to be Factified. The rest carry on,'\" Gavish says. \"They see immediate benefit... but then they discover that they've crossed the Rubicon.\"What's next for Factify?The capital from this round will be used to deepen the platform's core engineering—which Gavish describes as a \"heavy engineering lift\" requiring them to rebuild the document format, data layer, and application layer from scratch. The company is also establishing a major operational hub in Pittsburgh to support its U.S. expansion.Ultimately, Factify isn't trying to build another collaboration tool like Google Docs. They are trying to build the immutable record of the future—the standard for \"truth\" in a digital world.\"The PDF... became a standard meaning I cannot file my taxes using any other format. This is how victory looks like,\" Gavish says. \"We are creating a document standard that is not specific for health care or for insurance, but is just document as such.\"For the three trillion static files currently sitting in cloud storage, the writing may finally be on the wall.",
      "published_date": "2026-01-28T17:34:00",
      "source": "VentureBeat",
      "source_id": "venturebeat",
      "language": "en",
      "credibility_score": 9,
      "relevance_weight": 8,
      "focus_tags": [
        "创业融资",
        "金融科技",
        "AI应用",
        "市场洞察"
      ]
    },
    {
      "title": "Adaptive6 emerges from stealth to reduce enterprise cloud waste (and it's already optimizing Ticketmaster)",
      "url": "https://venturebeat.com/infrastructure/adaptive6-emerges-from-stealth-to-reduce-enterprise-cloud-waste-and-its",
      "content": "The generative AI era has sped everything up for most enterprises we talk to, especially development cycles (thanks to \"vibe coding\" and \"agentic swarming\").But even as they seek to leverage the power of new AI-assisted programming tools and coding agents like Claude Code to generate code, enterprises must contend with a looming concern — no, not safety (although that's another one!): cloud spend. According to Gartner, public cloud spend will rise 21.3% in 2026 and yet, according to Flexera's last State of the Cloud report, up to 32% of enterprise cloud spend is actually just wasted resources — duplicated code, non-functional code, outdated code, needless scaffolding, inefficient processes, etc.Today, a new firm, Adaptive6 emerged from stealth to reduce this cloud waste in realtime — automatically. The company, which also announced $44 million in total funding including a $28 million Series A led by U.S. Venture Partners (USVP), aims to treat cloud waste not as a financial discrepancy, but as a code vulnerability that must be detected and patched.Co-founded by CEO Aviv Revach, an experienced founder, former Head of Strategy at Taboola, and a former security research team leader for the Israeli Military Intelligence Unit 8200, the idea behind the venture came directly from his experience working in cybersecurity.“We realized this is not a financial problem; it’s an engineering problem,\" Revach told VentureBeat in an exclusive video call interview conducted recently. \"We drew on our background in cybersecurity, where to find vulnerabilities, you scan the cloud, identify the issues, map them back to the relevant code, find the responsible developer or engineer, and remediate—or, in some cases, shift left and prevent them altogether... it was obvious that this is exactly what we need to do.”Adaptive6’s platform introduces a radical shift in how enterprises govern infrastructure: instead of asking finance teams to spot inefficiencies they can’t fix, it empowers engineers to resolve waste directly in their workflow. By applying the rigor of cybersecurity—scanning, tracing, and remediation—Adaptive6 automates the cleanup of \"Shadow Waste\" across complex multi-cloud environments.The shift: from billing to engineeringFor years, the industry standard for managing cloud costs has been \"visibility\"—dashboards that tell you yesterday’s news. Revach argues that visibility without action is just noise.\"The first generation of tools are sort of trying to help on the financial side of the cloud,\" Revach told VentureBeat. \"They typically deal with the financial aspects of cloud cost... showing you costs going up, costs going down, forecasting, budgeting. But what they don't really focus on is one of the biggest problems, which is the waste problem.\" According to Revach, the disconnect lies in ownership.\"Just like you have the CISO in cybersecurity trying to get everybody to be thinking about security, you now have the FinOps person trying to get everybody to be thinking about cloud cost.\" Technology: hunting \"shadow waste\"The core of Adaptive6’s offering is its \"Cloud Cost Governance and Optimization\" (CCGO) platform. It doesn't just look for idle servers; it hunts for what the company calls Shadow Waste—hidden inefficiencies in architecture and application workloads that traditional cost tools often miss.The system operates without agents, using standard cloud APIs to gain read-only access to environments. Revach explained to VentureBeat that the platform scans across AWS, GCP, and Azure, as well as PaaS layers like Databricks and Snowflake, and even deep into Kubernetes clusters. \"We have unique technology that basically allows us to match each resource in the cloud [where] we found a problem to the relevant line of code that actually created that problem,\" Revach explained. This \"Cloud to Code\" technology allows the system to identify the specific engineer who made the change and serve them a fix directly in their workflow (Jira, Slack, or ServiceNow). Beyond basic resource sizing, the platform analyzes complex configurations, including those for emerging AI workloads. Revach highlighted a specific technical nuance regarding \"provisioned throughput\" for Large Language Models (LLMs) on AWS. He noted that engineers often struggle to balance commitment levels—committing too little risks performance, while committing too much wastes capital. Adaptive6’s engine analyzes these specific usage patterns to recommend the precise throughput commitment needed, a level of granularity that general finance tools lack. Revach also provided a specific example of \"Shadow Waste\" involving application-level inefficiencies:\"If you're using Python... and you're not using the latest version—right now, version 3.12 made a major change that made it far more efficient,\" he said. \"Most folks, when they think about cloud cost, they don't necessarily think of the Python version, so they only think about the size of the machine. By moving to that version, you gain the efficiency so your code just runs faster, and you reduce the cost.\" The AI paradox: both problem and solutionWhile Adaptive6 uses AI to generate remediation scripts and \"1-Click Fixes,\" Revach was careful to distinguish their deep-tech approach from generic AI coding agents. In fact, he noted that AI-generated code is often a source of waste itself.\"The code that is produced by AI is many times not that efficient because it was trained on a lot of code that other people wrote that didn't necessarily take cloud cost optimization and governance into account,\" Revach warned.This is why Adaptive6 relies on a research team of experts rather than just generative models to identify inefficiencies. \"Just like with vulnerability research, you see cyber companies getting the best of the best security researchers to find things... we are doing the exact same thing for cost inefficiencies,\" Revach said. Impact and adoptionThe platform is already in use by major enterprises, including Ticketmaster, Bayer, and Norstella, with customers reporting 15–35% reductions in total cloud spend.For global organizations, the ability to decentralized cost management is critical. \"As complex as it gets with a big organization, that's exactly our sweet spot,\" Revach noted. He cited one dramatic instance of the tool's efficacy: \"We've had a case where one misconfiguration that basically an organization solved actually resulted in more than a million dollars of savings.\"Looking aheadThe system also includes \"shift left\" prevention capabilities, integrating directly into CI/CD pipelines. This allows the platform to scan code for cost inefficiencies before it ever goes live, effectively blocking expensive architectural mistakes before they are deployed—much like a security scanner blocks vulnerable code. \"We detect what's already wasting money, prevent new inefficiencies before they deploy, and remediate at scale,\" Revach said. By shifting the responsibility left to developers, Adaptive6 suggests the future of cloud cost management won't be found in a spreadsheet, but in a pull request.",
      "published_date": "2026-01-28T15:04:00",
      "source": "VentureBeat",
      "source_id": "venturebeat",
      "language": "en",
      "credibility_score": 9,
      "relevance_weight": 8,
      "focus_tags": [
        "创业融资",
        "金融科技",
        "AI应用",
        "市场洞察"
      ]
    },
    {
      "title": "Airtable's Superagent maintains full execution visibility to solve multi-agent context problem",
      "url": "https://venturebeat.com/data/airtables-superagent-maintains-full-execution-visibility-to-solve-multi",
      "content": "Airtable is applying its data-first design philosophy to AI agents with the debut of Superagent on Tuesday. It's a standalone research agent that deploys teams of specialized AI agents working in parallel to complete research tasks.The technical innovation lies in how Superagent's orchestrator maintains context. Earlier agent systems used simple model routing where an intermediary filtered information between models. Airtable's orchestrator maintains full visibility over the entire execution journey: the initial plan, execution steps and sub-agent results. This creates what co-founder Howie Liu calls \"a coherent journey\" where the orchestrator made all decisions along the way.\n\n\"It ultimately comes down to how you leverage the model's self-reflective capability,\" Liu told VentureBeat. Liu co-founded Airtable more than a dozen years ago with a cloud-based relational database at its core.Airtable built its business on a singular bet: Software should adapt to how people work, not the other way around. That philosophy powered growth to over 500,000 organizations, including 80% of the Fortune 100, using its platform to build custom applications fitted to their workflows.The Superagent technology is an evolution of capabilities originally developed by DeepSky (formerly known as Gradient), which Airtable acquired in October 2025.From structured data to free-form agentsLiu frames Airtable and Superagent as complementary form factors that together address different enterprise needs. Airtable provides the structured foundation, and Superagent handles unstructured research tasks.\"We obviously started with a data layer. It's in the name Airtable: It's a table of data,\" Liu said.The platform evolved as scaffolding around that core database with workflow capabilities, automations, and interfaces that scale to thousands of users. \"I think Superagent is a very complementary form factor, which is very unstructured,\" Liu said. \"These agents are, by nature, very free form.\"The decision to build free-form capabilities reflects industry learnings about using increasingly capable models. Liu said that as the models have gotten smarter, the best way to use them is to have fewer restrictions on how they run.How Superagent's multi-agent system worksWhen a user submits a query, the orchestrator creates a visible plan that breaks complex research into parallel workstreams. So, for example if you're researching a company for investment, it'll break that up into different parts of that task, like research the team, research the funding history, research the competitive landscape. Each workstream gets delegated to a specialized agent that executes independently. These agents work in parallel, their work coordinated by the system, each contributing its piece to the whole.While Airtable describes Superagent as a multi-agent system, it relies on a central orchestrator that plans, dispatches, and monitors subtasks — a more controlled model than fully autonomous agents.Airtable's orchestrator maintains full visibility over the entire execution journey: the initial plan, execution steps and sub-agent results. This creates what Liu calls \"a coherent journey\" where the orchestrator made all decisions along the way. The sub-agent approach aggregates cleaned results without polluting the main orchestrator's context. Superagent uses multiple frontier models for different sub-tasks, including OpenAI, Anthropic, and Google.This solves two problems: It manages context windows by aggregating cleaned results without pollution, and it enables adaptation during execution.\"Maybe it tried doing a research task in a certain way that didn't work out, couldn't find the right information, and then it decided to try something else,\" Liu said. \"It knows that it tried the first thing and it didn't work. So it won't make the same mistake again.\"Why data semantics determine agent performanceFrom a builder perspective, Liu argues that agent performance depends more on data structure quality than model selection or prompt engineering. He based this on Airtable's experience building an internal data analysis tool to figure out what works.The internal tool experiment revealed that data preparation consumed more effort than agent configuration.\"We found that the hardest part to get right was not actually the agent harness, but most of the special sauce had more to do with massaging the data semantics,\" Liu said. \"Agents really benefit from good data semantics.\"The data preparation work focused on three areas: restructuring data so agents could find the right tables and fields, clarifying what those fields represent, and ensuring agents could use them reliably in queries and analysis. What enterprises need to knowFor organizations evaluating multi-agent systems or building custom implementations, Liu's experience points to several technical priorities.Data architecture precedes agent deployment. The internal experiment demonstrated that enterprises should expect data preparation to consume more resources than agent configuration. Organizations with unstructured data or poor schema documentation will struggle with agent reliability and accuracy regardless of model sophistication.Context management is critical. Simply stitching different LLMs together to create an agentic workflow isn't enough. There needs to be a proper context orchestrator that can maintain state and information with a view of the whole workflow.Relational databases matter. Relational database architecture provides cleaner semantics for agent navigation than document stores or unstructured repositories. Organizations standardizing on NoSQL for performance reasons should consider maintaining relational views or schemas for agent consumption.Orchestration requires planning capabilities. Just like a relational database has a query planner to optimize results, agentic workflows need an orchestration layer that plans and manages outcomes.\"So the punchline and the short version is that a lot of it comes down to having a really good planning and execution orchestration layer for the agent, and being able to fully leverage the models for what they're good at,\" Liu said.",
      "published_date": "2026-01-28T14:30:00",
      "source": "VentureBeat",
      "source_id": "venturebeat",
      "language": "en",
      "credibility_score": 9,
      "relevance_weight": 8,
      "focus_tags": [
        "创业融资",
        "金融科技",
        "AI应用",
        "市场洞察"
      ]
    },
    {
      "title": "How SAP Cloud ERP enabled Western Sugar’s move to AI-driven automation",
      "url": "https://venturebeat.com/infrastructure/how-sap-cloud-erp-enabled-western-sugars-move-to-ai-driven-automation",
      "content": "Presented by SAPTen years ago, Western Sugar made a decision that would prove prescient: move from on-premise SAP ECC to SAP S/4HANA Cloud Public Edition. At the time, artificial intelligence wasn't a priority on most roadmaps. The company was simply trying to escape what Director of Corporate Controlling, Richard Caluori, calls \"a trainwreck:” a heavily customized ERP system so laden with custom ABAP code that it had become unupgradable.Today, that early cloud adoption is proving to be the foundation for Western Sugar's AI transformation. As SAP accelerates its rollout of business AI capabilities across finance, supply chain, HR, and more, Western Sugar finds itself uniquely positioned to take advantage of the technology.\"We didn't move to the cloud thinking about AI,\" Caluori says. \"But that decision to embrace clean core principles and standardized processes turned out to be exactly what we needed when AI capabilities became available. The clean data, the standardized workflows, the disciplined processes, all of that groundwork we laid for basic operational reasons is now the foundation that makes AI work. We were ready without even knowing it.\"Building AI readiness with a clean core ERP foundationWestern Sugar's journey began with a familiar enterprise problem: technical debt. Years of on-premise customization had created a system that was nearly impossible to maintain or upgrade. \"Because we were on premise, we could do our own coding in ABAP, and over the years we created such a mess with our internal coding that the software was no longer upgradable,\" Caluori explains. \"The immediate benefits of moving to public cloud were clear: reduced infrastructure burden and access to standard processes refined by SAP. They've been in this business for 40 to 50 years, and they put all their experience into this one solution. Now upgrades just work.\"But the most significant advantage proved to be the clean core philosophy inherent in public cloud deployments, which means the software is maintained and upgraded by SAP. This approach, combined with robust API connectivity, created an environment where Western Sugar's IT department could easily integrate systems. In SAP S/4HANA Cloud Public Edition, this model keeps core ERP logic standardized and upgradeable, while extensions and integrations are handled through SAP-supported APIs and services.\"In the end, we have lower total cost of ownership, a better product, and the data quality and process discipline that's essential for AI adoption,\" Caluori says.This clean core foundation — standardized, continuously updated, and API-connected — is what makes embedded AI viable inside SAP Cloud ERP.How clean core processes in SAP enabled AI automationWhen SAP began rolling out SAP Business AI capabilities inside SAP S/4HANA Cloud Public Edition, Caluori was amped to improve processes through automation and standardization in ways they'd never previously imagined. The company's first major AI implementation focused on central invoice management. Today, invoices arrive from external sources and pass through the firewall. If they meet predefined AI confidence thresholds, SAP Business AI automatically posts them with zero human keyboard input. Each transaction is continuously evaluated using a traffic-light model: green items are processed automatically, yellow items are routed for review, and red items are flagged for immediate attention.\"Because the invoice just flows through the system automatically, we're held to a high standard,\" he says. \"The AI-driven functionality only works if the whole process chain, from purchasing requisition to purchase order to receiving to issuing is clean, so we're constantly improving those upstream processes to meet the demands of our AI innovations.\"Quantifying the operational impact of AI automationCaluori estimates Western Sugar has achieved six-figure direct cost savings through AI automation, not accounting for improved visibility and control. \"When I log into my computer now, I can see immediately in real time what's going on across the procurement side,\" he says. \"I have a comprehensive cockpit view that I didn't have before. Because I have much more visibility, I have much more control over operations.\"The company is now expanding AI adoption into new areas. With Western Sugar's recent transition to SAP's three-speed landscape, Caluori is targeting month-end closing processes for AI automation. \"My goal is that AI handles the vast majority of the month-end close,\" he says. \"Over time, as AI learns what we're doing and how we close the books, the goal is to automate over 50 percent of the month's end closing activities. We're also looking forward to AI-managed procurement networks, and proactive reporting and intelligence, all of which will soon be possible.\"Western Sugar is also developing predictive maintenance AI for its manufacturing equipment — a critical capability for its large-scale facilities, where equipment failures can halt production and lead to losses in the hundreds of thousands of dollars. These efforts build on SAP’s AI and analytics capabilities across asset management and manufacturing systems.\"We've started an internal team working on predictive analytics with AI, where the system can tell us in advance if we need to be on alert for specific equipment — that a particular machine could break down in the next two or three days or weeks,\" Caluori explains. \"If we can proactively address these issues before they cause production stoppages, this will save us millions of dollars.\"Managing organizational change alongside AI adoptionWhile the technology itself has delivered clear benefits, the organizational impact has been more complex. For Western Sugar, modernizing its core systems early — by moving to SAP’s cloud and embracing standardized, upgrade-driven processes — required not just new workflows, but a fundamental shift in how employees thought about change itself.For Caluori, that readiness is non-negotiable. “Change management is the number-one key to success,” he says. “We had to do a lot of change management, not only around business processes, but around employee behavior as well.”That work paid off over time, in part because cloud adoption normalized continuous change. As upgrades became routine rather than disruptive, employees grew more comfortable with evolution as an operating condition.“Now, when SAP comes with a new upgrade, they know change is coming,” Caluori explains. “The mindset has shifted to being eager to see what improvements the next update will bring.” And that cultural shift has proven critical as Western Sugar moves beyond system upgrades and into more advanced initiatives.“Now people are even eager to move into AI — the bigger projects,” he adds. However, that cultural readiness must be driven from the top. At Western Sugar, executive leadership — many of whom came from large international organizations — understands the competitive necessity of staying current with technology. That top-down commitment has helped normalize continuous change and created the foundation required to pursue AI strategically.Lessons in AI readiness from Western SugarFor companies considering their own AI journeys, Western Sugar's experience offers a clear lesson: AI readiness begins long before AI adoption. The clean core, standardized processes, and strong data quality that Western Sugar established a decade ago, driven purely by the need to escape technical debt, proved to be exactly what AI required. And while Caluori acknowledges the advantage an early start gave them, he says the second-best time to start is now.\"You have to embrace these changes, otherwise you're left behind,\" Caluori says. \"That continuous improvement is what SAP provides us, and now with AI capabilities integrated throughout, we're seeing benefits we couldn't have imagined when we started this journey.\"Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
      "published_date": "2026-01-28T05:00:00",
      "source": "VentureBeat",
      "source_id": "venturebeat",
      "language": "en",
      "credibility_score": 9,
      "relevance_weight": 8,
      "focus_tags": [
        "创业融资",
        "金融科技",
        "AI应用",
        "市场洞察"
      ]
    },
    {
      "title": "SOC teams are automating triage — but 40% will fail without governance boundaries",
      "url": "https://venturebeat.com/security/tier-1-soc-work-becoming-code-architectural-decisions-hours-minutes-response",
      "content": "The average enterprise SOC receives 10,000 alerts per day. Each requires 20 to 40 minutes to investigate properly, but even fully staffed teams can only handle 22% of them. More than 60% of security teams have admitted to ignoring alerts that later proved critical.Running an efficient SOC has never been harder, and now the work itself is changing. Tier-1 analyst tasks — like triage, enrichment, and escalation — are becoming software functions, and more SOC teams are turning to supervised AI agents to handle the volume. Human analysts are shifting their priorities to investigate, review, and make edge-case decisions. Response times are being reduced.Not integrating human insight and intuition comes with a high cost, however. Gartner predicts over 40% of agentic AI projects will be canceled by the end of 2027, with the main drivers being unclear business value and inadequate governance. Getting change management right and making sure generative AI doesn’t become a chaos agent in the SOC are even more important. Why the legacy SOC model needs to change Burnout is so severe in many SOCs today that senior analysts are considering career changes. Legacy SOCs that have multiple systems that deliver conflicting alerts, and the many systems that can’t talk to each other at all, are making the job a recipe for burnout, and the talent pipeline cannot refill faster than burnout empties it. CrowdStrike's 2025 Global Threat Report documents breakout times as fast as 51 seconds and found 79% of intrusions are now malware-free. Attackers rely on identity abuse, credential theft, and living-off-the-land techniques instead. Manual triage built for hourly response cycles cannot compete.As Matthew Sharp, CISO at Xactly, told CSO Online: \"Adversaries are already using AI to attack at machine speed. Organizations can't defend against AI-driven attacks with human-speed responses.\"How bounded autonomy compresses response timesSOC deployments that compress response times share a common pattern: bounded autonomy. AI agents handle triage and enrichment automatically, but humans approve containment actions when severity is high. This division of labor processes alert volume at machine speed while keeping human judgment on decisions that carry operational risk.Graph-based detection changes how defenders see the network. Traditional SIEMs show isolated events. Graph databases show relationships between those events, letting AI agents trace attack paths instead of triaging alerts one at a time. A suspicious login looks different when the system understands that the account is two hops from the domain controller.Speed gains are measurable. AI compresses threat investigation timeframes while increasing accuracy against senior analyst decisions. Separate deployments show AI-driven triage achieving over 98% agreement with human expert decisions while cutting manual workloads by more than 40 hours per week. Speed means nothing if accuracy drops.ServiceNow and Ivanti signal broader shift to agentic IT operationsGartner predicts that multi-agent AI in threat detection will rise from 5% to 70% of implementations by 2028. ServiceNow spent approximately $12 billion on security acquisitions in 2025 alone. Ivanti, which compressed a three-year kernel-hardening roadmap into 18 months when nation-state attackers validated the urgency, announced agentic AI capabilities for IT service management, bringing the bounded-autonomy model reshaping SOCs to the service desk. Customer preview launches in Q1, with general availability later in 2026.The workloads breaking SOCs are breaking service desks, too. Robert Hanson, CIO at Grand Bank, faced the same constraint security leaders know well. \"We can deliver 24/7 support while freeing our service desk to focus on complex challenges,\" Hanson said. Continuous coverage without proportional headcount. That outcome is driving adoption across financial services, healthcare, and government.Three governance boundaries for bounded autonomyBounded autonomy requires explicit governance boundaries. Teams should specify three things: which alert categories agents can act on autonomously, which require human review regardless of confidence score, and which escalation paths apply when certainty falls below threshold. High-severity incidents require human approval before containment.Having governance in place before deploying AI across SOCs is critical if any organization is going to get the time and containment benefits this latest generation of tools has to offer. When adversaries weaponize AI and actively mine CVE vulnerabilities faster than defenders respond, autonomous detection becomes the new table stakes for staying resilient in a zero-trust world.The path forward for security leadersTeams should start with workflows where failure is recoverable. Three workflows consume 60% of analyst time while contributing minimal investigative value: phishing triage (missed escalations can be caught in secondary review), password reset automation (low blast radius), and known-bad indicator matching (deterministic logic). Automate these first, then validate accuracy against human decisions for 30 days.",
      "published_date": "2026-01-27T22:30:00",
      "source": "VentureBeat",
      "source_id": "venturebeat",
      "language": "en",
      "credibility_score": 9,
      "relevance_weight": 8,
      "focus_tags": [
        "创业融资",
        "金融科技",
        "AI应用",
        "市场洞察"
      ]
    },
    {
      "title": "Contextual AI launches Agent Composer to turn enterprise RAG into production-ready AI agents",
      "url": "https://venturebeat.com/technology/contextual-ai-launches-agent-composer-to-turn-enterprise-rag-into-production",
      "content": "In the race to bring artificial intelligence into the enterprise, a small but well-funded startup is making a bold claim: The problem holding back AI adoption in complex industries has never been the models themselves.Contextual AI, a two-and-a-half-year-old company backed by investors including Bezos Expeditions and Bain Capital Ventures, on Monday unveiled Agent Composer, a platform designed to help engineers in aerospace, semiconductor manufacturing, and other technically demanding fields build AI agents that can automate the kind of knowledge-intensive work that has long resisted automation.The announcement arrives at a pivotal moment for enterprise AI. Four years after ChatGPT ignited a frenzy of corporate AI initiatives, many organizations remain stuck in pilot programs, struggling to move experimental projects into full-scale production. Chief financial officers and business unit leaders are growing impatient with internal efforts that have consumed millions of dollars but delivered limited returns.Douwe Kiela, Contextual AI's chief executive, believes the industry has been focused on the wrong bottleneck. \"The model is almost commoditized at this point,\" Kiela said in an interview with VentureBeat. \"The bottleneck is context — can the AI actually access your proprietary docs, specs, and institutional knowledge? That's the problem we solve.\"Why enterprise AI keeps failing, and what retrieval-augmented generation was supposed to fixTo understand what Contextual AI is attempting, it helps to understand a concept that has become central to modern AI development: retrieval-augmented generation, or RAG.When large language models like those from OpenAI, Google, or Anthropic generate responses, they draw on knowledge embedded during training. But that knowledge has a cutoff date, and it cannot include the proprietary documents, engineering specifications, and institutional knowledge that make up the lifeblood of most enterprises.RAG systems attempt to solve this by retrieving relevant documents from a company's own databases and feeding them to the model alongside the user's question. The model can then ground its response in actual company data rather than relying solely on its training.Kiela helped pioneer this approach during his time as a research scientist at Facebook AI Research and later as head of research at Hugging Face, the influential open-source AI company. He holds a Ph.D. from Cambridge and serves as an adjunct professor in symbolic systems at Stanford University.But early RAG systems, Kiela acknowledges, were crude.\"Early RAG was pretty crude — grab an off-the-shelf retriever, connect it to a generator, hope for the best,\" he said. \"Errors compounded through the pipeline. Hallucinations were common because the generator wasn't trained to stay grounded.\"When Kiela founded Contextual AI in June 2023, he set out to solve these problems systematically. The company developed what it calls a \"unified context layer\" — a set of tools that sit between a company's data and its AI models, ensuring that the right information reaches the model in the right format at the right time.The approach has earned recognition. According to a Google Cloud case study, Contextual AI achieved the highest performance on Google's FACTS benchmark for grounded, hallucination-resistant results. The company fine-tuned Meta's open-source Llama models on Google Cloud's Vertex AI platform, focusing specifically on reducing the tendency of AI systems to invent information.Inside Agent Composer, the platform that promises to turn complex engineering workflows into minutes of workAgent Composer extends Contextual AI's existing platform with orchestration capabilities — the ability to coordinate multiple AI tools across multiple steps to complete complex workflows.The platform offers three ways to create AI agents. Users can start with pre-built agents designed for common technical workflows like root cause analysis or compliance checking. They can describe a workflow in natural language and let the system automatically generate a working agent architecture. Or they can build from scratch using a visual drag-and-drop interface that requires no coding.What distinguishes Agent Composer from competing approaches, the company says, is its hybrid architecture. Teams can combine strict, deterministic rules for high-stakes steps — compliance checks, data validation, approval gates — with dynamic reasoning for exploratory analysis.\"For highly critical workflows, users can choose completely deterministic steps to control agent behavior and avoid uncertainty,\" Kiela said.The platform also includes what the company calls \"one-click agent optimization,\" which takes user feedback and automatically adjusts agent performance. Every step of an agent's reasoning process can be audited, and responses come with sentence-level citations showing exactly where information originated in source documents.From eight hours to 20 minutes: what early customers say about the platform's real-world performanceContextual AI says early customers have reported significant efficiency gains, though the company acknowledges these figures come from customer self-reporting rather than independent verification.\"These come directly from customer evals, which are approximations of real-world workflows,\" Kiela said. \"The numbers are self-reported by our customers as they describe the before-and-after scenario of adopting Contextual AI.\"The claimed results are nonetheless striking. An advanced manufacturer reduced root-cause analysis from eight hours to 20 minutes by automating sensor data parsing and log correlation. A specialty chemicals company reduced product research from hours to minutes using agents that search patents and regulatory databases. A test equipment maker now generates test code in minutes instead of days.Keith Schaub, vice president of technology and strategy at Advantest, a semiconductor test equipment company, offered an endorsement. \"Contextual AI has been an important part of our AI transformation efforts,\" Schaub said. \"The technology has been rolled out to multiple teams across Advantest and select end customers, saving meaningful time across tasks ranging from test code generation to customer engineering workflows.\"The company's other customers include Qualcomm, the semiconductor giant; ShipBob, a tech-enabled logistics provider that claims to have achieved 60 times faster issue resolution; and Nvidia, the chip maker whose graphics processors power most AI systems.The eternal enterprise dilemma: should companies build their own AI systems or buy off the shelf?Perhaps the biggest challenge Contextual AI faces is not competing products but the instinct among engineering organizations to build their own solutions.\"The biggest objection is 'we'll build it ourselves,'\" Kiela acknowledged. \"Some teams try. It sounds exciting to do, but is exceptionally hard to do this well at scale. Many of our customers started with DIY, and found themselves still debugging retrieval pipelines instead of solving actual problems 12-18 months later.\"The alternative — off-the-shelf point solutions — presents its own problems, the company argues. Such tools deploy quickly but often prove inflexible and difficult to customize for specific use cases.Agent Composer attempts to occupy a middle ground, offering a platform approach that combines pre-built components with extensive customization options. The system supports models from OpenAI, Anthropic, and Google, as well as Contextual AI's own Grounded Language Model, which was specifically trained to stay faithful to retrieved content.Pricing starts at $50 per month for self-serve usage, with custom enterprise pricing for larger deployments.\"The justification to CFOs is really about increasing productivity and getting them to production faster with their AI initiatives,\" Kiela said. \"Every technical team is struggling to hire top engineering talent, so making their existing teams more productive is a huge priority in these industries.\"The road ahead: multi-agent coordination, write actions, and the race to build compound AI systemsLooking ahead, Kiela outlined three priorities for the coming year: workflow automation with actual write actions across enterprise systems rather than just reading and analyzing; better coordination among multiple specialized agents working together; and faster specialization through automatic learning from production feedback.\"The compound effect matters here,\" he said. \"Every document you ingest, every feedback loop you close, those improvements stack up. Companies building this infrastructure now are going to be hard to catch.\"The enterprise AI market remains fiercely competitive, with offerings from major cloud providers, established software vendors, and scores of startups all chasing the same customers. Whether Contextual AI's bet on context over models will pay off depends on whether enterprises come to share Kiela's view that the foundation model wars matter less than the infrastructure that surrounds them.But there is a certain irony in the company's positioning. For years, the AI industry has fixated on building ever-larger, ever-more-powerful models — pouring billions into the race for artificial general intelligence. Contextual AI is making a quieter argument: that for most real-world work, the magic isn't in the model. It's in knowing where to look.",
      "published_date": "2026-01-27T17:00:00",
      "source": "VentureBeat",
      "source_id": "venturebeat",
      "language": "en",
      "credibility_score": 9,
      "relevance_weight": 8,
      "focus_tags": [
        "创业融资",
        "金融科技",
        "AI应用",
        "市场洞察"
      ]
    },
    {
      "title": "How Moonshot's Kimi K2.5 helps AI builders spin up agent swarms easier than ever",
      "url": "https://venturebeat.com/orchestration/moonshot-ai-debuts-kimi-k2-5-most-powerful-open-source-llm-beating-opus-4-5",
      "content": "Chinese company Moonshot AI upgraded its open-sourced Kimi K2 model, transforming it into a coding and vision model with an architecture that supports an agent swarm orchestration. The new model, Moonshot Kimi K2.5, is a good option for enterprises that want agents that can automatically pass off actions instead of having a framework be a central decision-maker.The company characterized Kimi K2.5 as an “all-in-one model” that supports both visual and text inputs, letting users leverage the model for more visual coding projects.Moonshot did not publicly disclose K2.5’s parameter count, but the Kimi K2 model that it's based on, had 1 trillion total parameters and 32 billion activated parameters thanks to its mixture-of-experts architecture.This is the latest open-source model to offer an alternative to the more closed options from Google, OpenAI, and Anthropic, and it outperforms them on key metrics including agentic workflows, coding, and vision. On the Humanity’s Last Exam (HLE) benchmark, Kimi K2.5 scored 50.2% (with tools), surpassing OpenAI’s GPT-5.2 (xhigh) and Claude Opus 4.5. It also achieved 76.8% on SWE-bench Verified, cementing its status as a top-tier coding model, though GPT-5.2 and Opus 4.5 overtake it here at 80 and 80.9, respectively. Moonshot said in a press release that it's seen a 170% increase in users between September and November for Kimi K2 and Kimi K2 Thinking, which was released in early November. Agent swarm and built-in orchestrationMoonshot aims to leverage self-directed agents and the agent swarm paradigm built into Kimi K2.5. Agent swarm has been touted as the next frontier in enterprise AI development and agent-based systems. It has attracted significant attention in the past few months. For enterprises, this means that if they build agent ecosystems with Kimi K2.5, they can expect to scale more efficiently. But instead of scaling “up” or growing model sizes to create larger agents, it’s betting on making more agents that can essentially orchestrate themselves. Kimi K2.5 “creates and coordinates a swarm of specialized agents working in parallel.” The company compared it to a beehive where each agent performs a task while contributing to a common goal. The model learns to self-direct up to 100 sub-agents and can execute parallel workflows of up to 1,500 tool calls.“Benchmarks only tell half the story. Moonshot AI believes AGI should ultimately be evaluated by its ability to complete real-world tasks efficiently under real-world time constraints. The real metric they care about is: how much of your day did AI actually give back to you? Running in parallel substantially reduces the time needed for a complex task — tasks that required days of work now can be accomplished in minutes,” the company said.Enterprises considering their orchestration strategies have begun looking at agentic platforms where agents communicate and pass off tasks, rather than following a rigid orchestration framework that dictates when an action is completed. While Kimi K2.5 may offer a compelling option for organizations that want to use this form of orchestration, some may feel more comfortable avoiding agent-based orchestration baked into the model and instead using a different platform to differentiate the model training from the agentic task. This is because enterprises often want more flexibility in which models make up their agents, so they can build an ecosystem of agents that tap LLMs that work best for specific actions. Some agent platforms, such as Salesforce, AWS Bedrock, and IBM, offer separate observability, management, and monitoring tools that help users orchestrate AI agents built with different models and enable them to work together. Multimodal coding and visual debuggingThe model lets users code visual layouts, including user interfaces and interactions. It reasons over images and videos to understand tasks encoded in visual inputs. For example, K2.5 can reconstruct a website’s code simply by analyzing a video recording of the site in action, translating visual cues into interactive layouts and animations.“Interfaces, layouts, and interactions that are difficult to describe precisely in language can be communicated through screenshots or screen recordings, which the model can interpret and turn into fully functional websites. This enables a new class of vibe coding experiences,” Moonshot said.This capability is integrated into Kimi Code, a new terminal-based tool that works with IDEs like VSCode and Cursor. It supports \"autonomous visual debugging,\" where the model visually inspects its own output — such as a rendered web page — references documentation, and iterates on the code to fix layout shifts or aesthetic errors without human intervention.Unlike other multimodal models that can create and understand images, Kimi K2.5 can build frontend interactions for websites with visuals, not just the code behind them.API pricingMoonshot AI has aggressively priced the K2.5 API to compete with major U.S. labs, offering significant reductions compared to its previous K2 Turbo model.Input: 60 cents per million tokens (a 47.8% decrease).Cached Input: 10 cents per million tokens (a 33.3% decrease).Output: $3 per million tokens (a 62.5% decrease).The low cost of cached inputs ($0.10/M tokens) is particularly relevant for the \"Agent Swarm\" features, which often require maintaining large context windows across multiple sub-agents and extensive tool usage.Modified MIT licenseWhile Kimi K2.5 is open-sourced, it is released under a Modified MIT License that includes a specific clause targeting \"hyperscale\" commercial users.The license grants standard permissions to use, copy, modify, and sell the software. However, it stipulates that if the software or any derivative work is used for a commercial product or service that has more than 100 million monthly active users (MAU) or more than $20 million USD in monthly revenue, the entity must prominently display \"Kimi K2.5\" on the user interface. This clause ensures that while the model remains free and open for the vast majority of the developer community and startups, major tech giants cannot white-label Moonshot’s technology without providing visible attribution.It's not full \"open source\" but it is better than Meta's similar Llama Licensing terms for its \"open source\" family of models, which required those companies with 700 million or more monthly users to obtain a special enterprise license from the company. What it means for modern enterprise AI buildersFor the practitioners defining the modern AI stack — from LLM decision-makers optimizing deployment cycles to AI orchestration leaders setting up agents and AI-powered automated business processes — Kimi K2.5 represents a fundamental shift in leverage. By embedding swarm orchestration directly into the model, Moonshot AI effectively hands these resource-constrained builders a synthetic workforce, allowing a single engineer to direct a hundred autonomous sub-agents as easily as a single prompt. This \"scale-out\" architecture directly addresses data decision-makers' dilemma of balancing complex pipelines with limited headcount, while the slashed pricing structure transforms high-context data processing from a budget-breaking luxury into a routine commodity. Ultimately, K2.5 suggests a future where the primary constraint on an engineering team is no longer the number of hands on keyboards, but the ability of its leaders to choreograph a swarm.",
      "published_date": "2026-01-27T15:55:00",
      "source": "VentureBeat",
      "source_id": "venturebeat",
      "language": "en",
      "credibility_score": 9,
      "relevance_weight": 8,
      "focus_tags": [
        "创业融资",
        "金融科技",
        "AI应用",
        "市场洞察"
      ]
    }
  ]
}