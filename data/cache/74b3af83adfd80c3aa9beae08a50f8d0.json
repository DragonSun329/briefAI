{
  "timestamp": "2026-01-06T11:28:58.122764",
  "key": "source_venturebeat_7days",
  "value": [
    {
      "title": "TII’s Falcon H1R 7B can out-reason models up to 7x its size — and it’s (mostly) open",
      "url": "https://venturebeat.com/technology/tiis-falcon-h1r-7b-can-out-reason-models-up-to-7x-its-size-and-its-mostly",
      "content": "For the last two years, the prevailing logic in generative AI has been one of brute force: if you want better reasoning, you need a bigger model. While \"small\" models (under 10 billion parameters) have become capable conversationalists, they have historically crumbled when asked to perform multi-step logical deduction or complex mathematical proofs.Today, the Technology Innovation Institute (TII) in Abu Dhabi is challenging that scaling law with the release of Falcon H1R 7B. By abandoning the pure Transformer orthodoxy in favor of a hybrid architecture, TII claims to have built a 7-billion parameter model that not only rivals but outperforms competitors nearly 7X its size — including the 32B and 47B variants of Alibaba's Qwen and Nvidia's Nemotron.The release marks a significant shift in the open-weight ecosystem, moving the battleground from raw parameter count to architectural efficiency and inference-time scaling.The full model code is available now at Hugging Face and can be tested by individuals in a live demo inference on Falcon Chat (a chatbot experience). TII further released a seemingly quite comprehensive technical report on the approach and training methodology for Falcon H1 7B, as well. Moving Beyond the Foundational LLM Tech, the TransformerThe defining feature of Falcon H1R 7B is its \"hybrid\" backbone. Most modern LLMs rely exclusively on the Transformer architecture, which scales predictably but suffers from high memory costs when processing long sequences. Falcon H1R 7B integrates Mamba, a state-space model (SSM) architecture, alongside standard Transformer attention layers.Originally developed by researchers Albert Gu and Tri Dao at Carnegie Mellon University and Princeton University, Mamba was first introduced in the paper \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\" published on December 1, 2023.The architecture processes data sequences differently than Transformers: while Transformers compare every piece of data to every other piece (quadratic scaling), Mamba processes tokens sequentially, allowing it to handle vast amounts of information with linear scaling and significantly reduced compute costs.This combination addresses one of the most persistent bottlenecks in deploying reasoning models: the cost of \"thinking.\" Reasoning models require generating long \"chains of thought\"—step-by-step internal monologues—before arriving at an answer. For standard Transformers, these long contexts explode computational costs.According to TII’s technical report, the hybrid approach allows Falcon H1R 7B to maintain high throughput even as response lengths grow. At a batch size of 64, the model processes approximately 1,500 tokens per second per GPU—nearly double the speed of the competing Qwen3 8B model.Benchmark Performance: Punching UpIn the benchmarks released by TII, the disparity between Falcon H1R 7B’s size and its performance is stark. On the AIME 2025 leaderboard—a rigorous test of mathematical reasoning—Falcon H1R 7B scored 83.1%, a result that disrupts the traditional hierarchy of model sizing.While the 7B model naturally trails massive proprietary frontiers like GPT-5.2 (99.0%) and Gemini 3 Flash (97.0%) on the separate Artificial Analysis index (run by the independent organization of the same name, which has not yet benchmarked Falcon H1R 7B yet), it has effectively collapsed the gap between \"efficient\" open weights and mid-tier proprietary systems.Beating Larger \"Thinkers\": Falcon H1R 7B (83.1%) outperforms the 15-billion parameter Apriel-v1.6-Thinker (82.7%) and the 32-billion parameter OLMo 3 Think (73.7%), validating TII's claim that hybrid architectures can out-reason larger Transformers.Chasing Proprietary Leaders: It sits within striking distance of Claude 4.5 Sonnet (88.0%) and Amazon Nova 2.0 Lite (88.7%), suggesting that for specific math-heavy workflows, this 7B model is a viable, low-latency alternative to expensive commercial APIs.Outperforming Legacy Giants: On this specific reasoning metric, it decisively beats broadly capable but older architectures like Mistral Large 3 (38.0%) and Llama 4 Maverick (19.3%), highlighting how specialized reasoning training (\"Deep Think\") has become more critical than raw scale for logic tasks.Other key domain wins include:Coding: The model achieved 68.6% on the LCB v6 benchmark, a score TII claims is the highest among all tested models, including those four times its size.General Reasoning: While it dominates in math and code, its general reasoning score (49.48%) remains competitive, sitting just below the 14B and 15B parameter models but comfortably ahead of comparable 8B models.Training TechniquesFalcon H1R 7B’s performance is not just architectural; it stems from a rigorous, two-stage training pipeline designed to maximize reasoning density without inflating parameter count, according to TII's technical report on the model.Stage 1: Cold-Start Supervised Fine-Tuning (SFT). The model underwent \"cold-start\" SFT on a curated dataset dominated by mathematics (56.8% of tokens) and code (29.8%), with response lengths stretching up to 48,000 tokens.Difficulty-Aware Weighting: TII rejected the standard practice of treating all data equally. Instead, they applied a weighting scheme where \"hard\" problems were up-weighted by 1.25x to 1.75x, while easy problems were down-weighted or removed entirely to prevent overfitting to trivial tasks.Single-Teacher Consistency: Ablation studies revealed that mixing reasoning traces from multiple \"teacher\" models actually degraded performance due to conflicting reasoning styles. Consequently, TII opted for a single-teacher approach to maintain coherent internal logic.Balanced Token Normalization: To handle the massive variance in sequence lengths (short instructions vs. massive reasoning chains), the team introduced a Balanced Data-Parallel Token Normalization strategy. This technique equalizes the gradient contribution of each token across GPUs, preventing ranks with shorter sequences from destabilizing the loss—a change that yielded a consistent 4-10% accuracy boost during training.Stage 2: Reinforcement Learning via Group Relative Policy Optimization (GRPO). Following SFT, the model was refined using GRPO a reinforcement learning algorithm that rewards correct outcomes without needing a separate value model.The \"No-KL\" Shift: In a deviation from standard RLHF, TII removed the KL-divergence penalty (beta=0) entirely. This allowed the model to drift significantly from its base SFT policy, encouraging aggressive exploration of novel reasoning paths.Math-Only Curriculum: Surprisingly, TII found that training exclusively on math problems during the RL stage yielded better generalization across all domains—including code and science—than mixed strategies. Ablations showed that \"code-only\" training improved coding scores but harmed general reasoning, whereas math-focused RL lifted performance globally.TII optimized the model specifically for Test-Time Scaling (TTS), a technique where a model generates multiple reasoning paths in parallel to find the best solution.The model utilizes Deep Think with Confidence (DeepConf), which leverages the model's internal confidence scores to dynamically prune low-quality reasoning traces.Adaptive Pruning: During generation, the system initiates a \"warm-up\" phase with 16 traces to establish a confidence baseline. It then aggressively filters subsequent traces, terminating any chain that falls below the 10th percentile of the baseline confidence.Efficiency Gains: This method creates a new Pareto frontier for deployment. In benchmark tests, Falcon H1R 7B achieved 96.7% accuracy on AIME 25 while reducing token usage by 38% compared to the DeepSeek-R1-0528-Qwen3-8B baseline.Licensing: Open For Commercial Usage, But With Strings AttachedTII has released Falcon H1R 7B under the custom Falcon LLM License 1.0 based on Apache 2.0 — but with notable modifications — chiefly among them: not to litigate against TII, and also to always credit it.For developers and startups, the license is largely permissive:Royalty-Free: Users can run, modify, and distribute the model commercially without paying TII.Attribution: Any derivative work (including fine-tunes) must prominently state: \"[Name of work] is built using Falcon LLM technology from the Technology Innovation Institute\".However, unlike a pure Open Source Initiative (OSI) license, the Falcon license includes a strict Acceptable Use Policy (AUP). The license terminates automatically if the model is used to create work that conflicts with the AUP or if the user initiates patent litigation against TII. Specifically, the AUP prohibits using Falcon H1R 7B or its derivatives for:Violating Laws: Any use that violates applicable national, federal, state, local, or international laws or regulations.Harm to Minors or Living Beings: Exploiting, harming, or attempting to exploit or harm minors or any living beings.Disinformation: Generating or disseminating verifiably false information with the purpose of harming others.Harassment: Defaming, disparaging, or otherwise harassing others.The Hybrid Wave: Nvidia, IBM, AI21, and MistralTII is not alone in betting on this hybrid future; the industry is increasingly moving toward architectures that blend the strengths of SSMs and Transformers.Nvidia recently debuted the Nemotron 3 family on December 15, 2025, which utilizes a hybrid mixture-of-experts (MoE) and Mamba-Transformer design to drive efficient agentic AI.IBM launched its Granite 4.0 family on October 2, 2025, using a hybrid Mamba-Transformer architecture to cut memory requirements by over 70% while maintaining high performance on enterprise benchmarks.AI21 has pursued this path with its Jamba (Joint Attention and Mamba) models, releasing the Jamba 1.5 family on August 22, 2024, to boost agentic AI capabilities through a hybrid SSM-Transformer approach.Mistral entered the space early with Codestral Mamba on July 16, 2024, a model specifically optimized for faster, longer code generation.Falcon H1R 7B represents the latest evolution in this trend, specifically targeting dense reasoning tasks in a compact form factor.",
      "published_date": "2026-01-05T20:27:00",
      "source": "VentureBeat",
      "source_id": "venturebeat",
      "language": "en",
      "credibility_score": 9,
      "relevance_weight": 8,
      "focus_tags": [
        "创业融资",
        "金融科技",
        "AI应用",
        "市场洞察"
      ]
    },
    {
      "title": "Nvidia’s Cosmos Reason 2 aims to bring reasoning VLMs into the physical world",
      "url": "https://venturebeat.com/orchestration/nvidias-cosmos-reason-2-aims-to-bring-reasoning-vlms-into-the-physical-world",
      "content": "Nvidia CEO Jensen Huang said last year that we are now entering the age of physical AI. While the company continues to offer LLMs for software use cases, Nvidia is increasingly positioning itself as a provider of AI models for fully AI-powered systems — including agentic AI in the physical world.At CES 2026, Nvidia announced a slate of new models designed to push AI agents beyond chat interfaces and into physical environments.Nvidia launched Cosmos Reason 2, the latest version of its vision-language model designed for embodied reasoning. Cosmos Reason 1, released last year, introduced a two-dimensional ontology for embodied reasoning and currently leads Hugging Face’s physical reasoning for video leaderboard.Cosmos Reason 2 builds on the same ontology while giving enterprises more flexibility to customize applications and enabling physical agents to plan their next actions, similar to how software-based agents reason through digital workflows.Nvidia also released a new version of Cosmos Transfer, a model that lets developers generate training simulations for robots.Other vision-language models, such as Google’s PaliGemma and Pixtral Large from Mistral, can process visual inputs, but not all commercially available VLMs support reasoning.“Robotics is at an inflection point. We are moving from specialist robots limited to single tasks to generalist specialist systems,” said Kari Briski, Nvidia vice president for generative AI software, in a briefing with reporters. She was referring to robots that combine broad foundational knowledge with deep task-specific skills. “These new robots combine broad fundamental knowledge with deep proficiency and complex tasks.”She added that Cosmos Reason 2 “enhances the reasoning capabilities that robots need to navigate the unpredictable physical world.”Moving to physical agentsBriski noted that Nvidia’s roadmap follows “the same pattern of assets across all of our open models.”“In building specialized AI agents, a digital workforce, or the physical embodiment of AI in robots and autonomous vehicles, more than just the model is needed,” Briski said. “First, the AI needs the compute resources to train, simulate the world around it. Data is the fuel for AI to learn and improve and we contribute to the world's largest collection of open and diverse datasets, going beyond just opening the weights of the models. The open libraries and training scripts give developers the tools to purpose-build AI for their applications, and we publish blueprints and examples to help deploy AI as systems of models.”The company now has open models specifically for physical AI in Cosmos, robotics, with the open-reasoning vision-language-action (VLA) model Gr00t and its Nemotron models for agentic AI. Nvidia is making the case that open models across different branches of AI form a shared enterprise ecosystem that feeds data, training, and reasoning to agents in both the digital and physical worlds. Additions to the Nemotron familyBriski said Nvidia plans to continue expanding its open models, including its Nemotron family, beyond reasoning to include a new RAG and embeddings model to make information more readily available to agents. The company released Nemotron 3, the latest version of its agentic reasoning models, in December. Nvidia announced three new additions to the Nemotron family: Nemotron Speech, Nemotron RAG and Nemotron Safety. In a blog post, Nvidia said Nemotron Speech delivers “real-time low-latency speech recognition for live captions and speech AI applications” and is 10 times faster than other speech models. Nemotron RAG is technically comprised of two models: an embedding model and a rerank model, both of which can understand images to provide more multimodal insights that data agents will tap. “Nemotron RAG is on top of what we call the MMTab, or the Massive Multilingual Text Embedding Benchmark, with strong multilingual performance while using less computing power memory, so they are a good fit for systems that must handle a lot of requests very quickly and with low delay,” Briski said. Nemotron Safety detects sensitive data so AI agents do not accidentally unleash personally identifiable data.",
      "published_date": "2026-01-05T20:00:00",
      "source": "VentureBeat",
      "source_id": "venturebeat",
      "language": "en",
      "credibility_score": 9,
      "relevance_weight": 8,
      "focus_tags": [
        "创业融资",
        "金融科技",
        "AI应用",
        "市场洞察"
      ]
    },
    {
      "title": "Brex bets on ‘less orchestration’ as it builds an Agent Mesh for autonomous finance",
      "url": "https://venturebeat.com/orchestration/brex-bets-on-less-orchestration-as-it-builds-an-agent-mesh-for-autonomous",
      "content": "Fintech Brex is betting that the future of enterprise AI isn’t better orchestration — it’s less of it.As generative AI agents move from copilots to autonomous systems, Brex CTO James Reggio says traditional agent orchestration frameworks are becoming a constraint rather than an enabler. Instead of relying on a central coordinator or rigid workflows, Brex has built what it calls an “Agent Mesh”: a network of narrow, role-specific agents that communicate in plain language and operate independently — but with full visibility.“Our goal is to use AI to make Brex effectively disappear,” Reggio told VentureBeat. “We’re aiming for total automation.”Brex learned that for its purposes, agents need to work in narrow, specific roles to be more modular, flexible, and auditable. Reggio said the architectural goal is to enable every manager in an enterprise “to have a single point of contact within Brex that’s handling the totality of their responsibilities, be it spend management, requesting travel, or approving spend limit requests.”The journey from Brex AssistantThe financial services industry has long embraced AI and machine learning to handle the massive amounts of data it processes. But when it comes to bringing AI models and agents, the industry took a more cautious road at the beginning. Now, more financial services companies, including Brex, have launched AI-powered platforms and several agentic workflows. Brex’s first foray into generative AI was with its Brex Assistant, released in 2023, which helped customers automate certain finance and expense tasks. It provides suggestions to complete expenses, automatically fills in information, and follows up on expenses that violate policies. Reggio acknowledges that Brex Assistant works, but it’s not enough.  “I think to some degree, it remains a bit of a technology where we don't entirely know the limits of it,\" he said. \"There's quite a large number of patterns that need to exist around it that are kind of being developed by the industry as the technology matures and as more companies build with it.\"  Brex Assistant uses multiple models, including Anthropic’s Claude and custom Brex-models, as well as OpenAI’s API. The assistant automates some tasks but is still limited in how low-touch it can be. Reggio said Brex Assistant still plays a big role in the company’s autonomy journey, mainly because its Agent Mesh product flows into the application. Agent Mesh to replace orchestrationThe consensus in the industry is that multi-agent ecosystems, in which agents communicate to accomplish tasks, require an orchestration framework to guide them. Reggio, on the other hand, has a different take. \"Deterministic orchestration infrastructure … was a solution for the problems that we saw two years ago, which was that agents, just like the models, hallucinate a lot,” Reggio said. “They're not very good with multiple tools, so you need to give them these degrees of freedom, but in a more structured, rigid system. But as the models get better, I think it's starting to hold back the range of possibilities that are expanding.”More traditional agent orchestration architectures either focus on a single agent that does everything or, more commonly, coordinator/orchestrator plus tool agents  that explicitly define workflows. Reggio said both frameworks are too rigid and solve issues more commonly seen in traditional software than in AI. The difference, Reggio argues, is structural:Traditional orchestration: predefined workflows, central coordinator, deterministic pathsAgent Mesh: event-driven, role-specialized agents, message-based coordinationAgent Mesh relies on stitching together networks of many small agents, each specializing in a single task. The agents, once again using the hybrid mix of models as with the Brex Assistant, communicate with other agents “in plain English” over a shared message stream. A routing model quickly determines which tools to invoke, he said.  A single reimbursement request triggers several tasks: a compliance check to align with expense policies, budget validation, receipt matching, and then payment initiation. While an agent can certainly be coded to do all of that, this method is “brittle and error-prone,” and it responds to new information shared through a message stream anyway. Reggio said the idea is to disambiguate all of those separate tasks and assign them to smaller agents instead. He likened the architecture to a Wi-Fi mesh, where no single node controls the system — reliability emerges from many small, overlapping contributors. “We basically found a really good fit with the idea of embodying specific roles as agents on top of the best platform to manage specific responsibilities, much like how you might delegate accounts payable to one team versus expense management to another team,” Reggio said. Brex defines three core ideas in the Agent Mesh architecture:Config, where definitions of the agent, model, tools and subscription liveMessageStream, a log of every message, tool call and state transition Clock, which ensures deterministic ordering Brex also built evaluations into the system, in which the LLM acts as a judge, and an audit agent reviews each agent’s decisions to ensure they adhere to accuracy and behavioral policies. Success so farBrex says it has seen substantial efficiency gains among its customers in its AI ecosystem. Brex did not provide third-party benchmarks or customer-specific data to validate those gains.But Reggio said enterprise customers using Brex Assistant and the company’s machine learning systems “are able to achieve 99% automation, especially for customers that really leaned into AI.”This is a marked improvement from the 60 to 70% Brex customers who were able to automate their expense processes before the launch of Brex Assistant. The company is still early in its autonomy journey, Reggio said. But if the Agent Mesh approach works, the most successful outcome may be invisible: employees no longer thinking about expenses at all.",
      "published_date": "2026-01-05T08:00:00",
      "source": "VentureBeat",
      "source_id": "venturebeat",
      "language": "en",
      "credibility_score": 9,
      "relevance_weight": 8,
      "focus_tags": [
        "创业融资",
        "金融科技",
        "AI应用",
        "市场洞察"
      ]
    },
    {
      "title": "The creator of Claude Code just revealed his workflow, and developers are losing their minds",
      "url": "https://venturebeat.com/technology/the-creator-of-claude-code-just-revealed-his-workflow-and-developers-are",
      "content": "When the creator of the world's most advanced coding agent speaks, Silicon Valley doesn't just listen — it takes notes.For the past week, the engineering community has been dissecting a thread on X from Boris Cherny, the creator and head of Claude Code at Anthropic. What began as a casual sharing of his personal terminal setup has spiraled into a viral manifesto on the future of software development, with industry insiders calling it a watershed moment for the startup.\"If you're not reading the Claude Code best practices straight from its creator, you're behind as a programmer,\" wrote Jeff Tang, a prominent voice in the developer community. Kyle McNease, another industry observer, went further, declaring that with Cherny's \"game-changing updates,\" Anthropic is \"on fire,\" potentially facing \"their ChatGPT moment.\"The excitement stems from a paradox: Cherny's workflow is surprisingly simple, yet it allows a single human to operate with the output capacity of a small engineering department. As one user noted on X after implementing Cherny's setup, the experience \"feels more like Starcraft\" than traditional coding — a shift from typing syntax to commanding autonomous units.Here is an analysis of the workflow that is reshaping how software gets built, straight from the architect himself. How running five AI agents at once turns coding into a real-time strategy gameThe most striking revelation from Cherny's disclosure is that he does not code in a linear fashion. In the traditional \"inner loop\" of development, a programmer writes a function, tests it, and moves to the next. Cherny, however, acts as a fleet commander.\"I run 5 Claudes in parallel in my terminal,\" Cherny wrote. \"I number my tabs 1-5, and use system notifications to know when a Claude needs input.\"By utilizing iTerm2 system notifications, Cherny effectively manages five simultaneous work streams. While one agent runs a test suite, another refactors a legacy module, and a third drafts documentation. He also runs \"5-10 Claudes on claude.ai\" in his browser, using a \"teleport\" command to hand off sessions between the web and his local machine.This validates the \"do more with less\" strategy articulated by Anthropic President Daniela Amodei earlier this week. While competitors like OpenAI pursue trillion-dollar infrastructure build-outs, Anthropic is proving that superior orchestration of existing models can yield exponential productivity gains.The counterintuitive case for choosing the slowest, smartest modelIn a surprising move for an industry obsessed with latency, Cherny revealed that he exclusively uses Anthropic's heaviest, slowest model: Opus 4.5.\"I use Opus 4.5 with thinking for everything,\" Cherny explained. \"It's the best coding model I've ever used, and even though it's bigger & slower than Sonnet, since you have to steer it less and it's better at tool use, it is almost always faster than using a smaller model in the end.\"For enterprise technology leaders, this is a critical insight. The bottleneck in modern AI development isn't the generation speed of the token; it is the human time spent correcting the AI's mistakes. Cherny's workflow suggests that paying the \"compute tax\" for a smarter model upfront eliminates the \"correction tax\" later.One shared file turns every AI mistake into a permanent lessonCherny also detailed how his team solves the problem of AI amnesia. Standard large language models do not \"remember\" a company's specific coding style or architectural decisions from one session to the next.To address this, Cherny's team maintains a single file named CLAUDE.md in their git repository. \"Anytime we see Claude do something incorrectly we add it to the CLAUDE.md, so Claude knows not to do it next time,\" he wrote.This practice transforms the codebase into a self-correcting organism. When a human developer reviews a pull request and spots an error, they don't just fix the code; they tag the AI to update its own instructions. \"Every mistake becomes a rule,\" noted Aakash Gupta, a product leader analyzing the thread. The longer the team works together, the smarter the agent becomes.Slash commands and subagents automate the most tedious parts of developmentThe \"vanilla\" workflow one observer praised is powered by rigorous automation of repetitive tasks. Cherny uses slash commands — custom shortcuts checked into the project's repository — to handle complex operations with a single keystroke.He highlighted a command called /commit-push-pr, which he invokes dozens of times daily. Instead of manually typing git commands, writing a commit message, and opening a pull request, the agent handles the bureaucracy of version control autonomously.Cherny also deploys subagents — specialized AI personas — to handle specific phases of the development lifecycle. He uses a code-simplifier to clean up architecture after the main work is done and a verify-app agent to run end-to-end tests before anything ships.Why verification loops are the real unlock for AI-generated codeIf there is a single reason Claude Code has reportedly hit $1 billion in annual recurring revenue so quickly, it is likely the verification loop. The AI is not just a text generator; it is a tester.\"Claude tests every single change I land to claude.ai/code using the Claude Chrome extension,\" Cherny wrote. \"It opens a browser, tests the UI, and iterates until the code works and the UX feels good.\"He argues that giving the AI a way to verify its own work — whether through browser automation, running bash commands, or executing test suites — improves the quality of the final result by \"2-3x.\" The agent doesn't just write code; it proves the code works.What Cherny's workflow signals about the future of software engineeringThe reaction to Cherny's thread suggests a pivotal shift in how developers think about their craft. For years, \"AI coding\" meant an autocomplete function in a text editor — a faster way to type. Cherny has demonstrated that it can now function as an operating system for labor itself.\"Read this if you're already an engineer... and want more power,\" Jeff Tang summarized on X.The tools to multiply human output by a factor of five are already here. They require only a willingness to stop thinking of AI as an assistant and start treating it as a workforce. The programmers who make that mental leap first won't just be more productive. They'll be playing an entirely different game — and everyone else will still be typing.",
      "published_date": "2026-01-05T07:45:00",
      "source": "VentureBeat",
      "source_id": "venturebeat",
      "language": "en",
      "credibility_score": 9,
      "relevance_weight": 8,
      "focus_tags": [
        "创业融资",
        "金融科技",
        "AI应用",
        "市场洞察"
      ]
    },
    {
      "title": "'Intelition' changes everything: AI is no longer a tool you invoke",
      "url": "https://venturebeat.com/technology/intelition-changes-everything-ai-is-no-longer-a-tool-you-invoke",
      "content": "AI is evolving faster than our vocabulary for describing it. We may need a few new words. We have “cognition” for how a single mind thinks, but we don't have a word for what happens when human and machine intelligence work together to perceive, decide, create and act. Let’s call that process intelition. Intelition isn’t a feature; it’s the organizing principle for the next wave of software where humans and AI operate inside the same shared model of the enterprise. Today’s systems treat AI models as things you invoke from the outside. You act as a “user,” prompting for responses or wiring a “human in the loop” step into agentic workflows. But that's evolving into continuous co-production: People and agents are shaping decisions, logic and actions together, in real time. Read on for a breakdown of the three forces driving this new paradigm. A unified ontology is just the beginningIn a recent shareholder letter, Palantir CEO Alex Karp wrote that “all the value in the market is going to go to chips and what we call ontology,” and argued that this shift is “only the beginning of something much larger and more significant.” By ontology, Karp means a shared model of objects (customers, policies, assets, events) and their relationships. This also includes what Palantir calls an ontology’s “kinetic layer” that defines the actions and security permissions connecting objects.In the SaaS era, every enterprise application creates its own object and process models. Combined with a host of legacy systems and often chaotic models, enterprises face the challenge of stitching all this together. It’s a big and difficult job, with redundancies, incomplete structures and missing data. The reality: No matter how many data warehouse or data lake projects commissioned, few enterprises come close to creating a consolidated enterprise ontology. A unified ontology is essential for today’s agentic AI tools. As organizations link and federate ontologies, a new software paradigm emerges: Agentic AI can reason and act across suppliers, regulators, customers and operations, not just within a single app.  As Karp describes it, the aim is “to tether the power of artificial intelligence to objects and relationships in the real world.” World models and continuous learningToday’s models can hold extensive context, but holding information isn’t the same as learning from it. Continual learning requires the accumulation of understanding, rather than resets with each retraining. To his aim, Google recently announced “Nested Learning” as a potential solution, grounded direclty into existing LLM architecture and training data. The authors don’t claim to have solved the challenges of building world models. But, Nested Learning could supply the raw ingredients for them: Durable memory with continual learning layered into the system. The endpoint would make retraining obsolete. In June 2022, Meta's chief AI scientist Yann LeCun created a blueprint for “autonomous machine intelligence” that featured a hierarchical approach to using joint embeddings to make predictions using world models. He called the technique H-JEPA, and later put bluntly: “LLMs are good at manipulating language, but not at thinking.”Over the past three years, LeCun and his colleagues at Meta have moved H-JEPA theory into practice with open source models V-JEPA and I-JEPA, which learn image and video representations of the world. The personal intelition interface The third force in this agentic, ontology-driven world is the personal interface. This puts people at the center rather than as “users” on the periphery. This is not another app; it is the primary way a person participates in the next era of work and life. Rather than treating AI as something we visit through a chat window or API cal, the personal intelition interface will be always-on, aware of our context, preferences and goals and capable of acting on our behalf across the entire federated economy. Let’s analyze how this is already coming together.In May, Jony Ive sold his AI device company io to OpenAI to accelerate a new AI device category. He noted at the time: “If you make something new, if you innovate, there will be consequences unforeseen, and some will be wonderful, and some will be harmful. While some of the less positive consequences were unintentional, I still feel responsibility. And the manifestation of that is a determination to try and be useful.” That is, getting the personal intelligence device right means more than an attractive venture opportunity. Apple is looking beyond LLMs for on-device solutions that require less processing power and result in less latency when creating AI apps to understand “user intent.” Last year, they created UI-JEPA, an innovation that moves to “on-device analysis” of what the user wants. This strikes directly at the business model of today’s digital economy, where centralized profiling of “users” transforms intent and behavior data into vast revenue streams.Tim Berners-Lee, the inventor of the World Wide Web, recently noted: “The user has been reduced to a consumable product for the advertiser ... there's still time to build machines that work for humans, and not the other way around.\" Moving user intent to the device will drive interest in a secure personal data management standard, Solid, that Berners-Lee and his colleagues have been developing since 2022. The standard is ideally suited to pair with new personal AI devices. For instance, Inrupt, Inc., a company founded by Berners-Lee, recently combined Solid with Anthropic’s MCP standard for Agentic Wallets. Personal control is more than a feature of this paradigm; it is the architectural safeguard as systems gain the ability to learn and act continuously.Ultimately, these three forces are moving and converging faster than most realize. Enterprise ontologies provide the nouns and verbs, world-model research supplies durable memory and learning and the personal interface becomes the permissioned point of control. The next software era isn't coming. It's already here.Brian Mulconrey is SVP at Sureify Labs.",
      "published_date": "2026-01-04T19:00:00",
      "source": "VentureBeat",
      "source_id": "venturebeat",
      "language": "en",
      "credibility_score": 9,
      "relevance_weight": 8,
      "focus_tags": [
        "创业融资",
        "金融科技",
        "AI应用",
        "市场洞察"
      ]
    },
    {
      "title": "Why “which API do I call?” is the wrong question in the LLM era",
      "url": "https://venturebeat.com/orchestration/why-which-api-do-i-call-is-the-wrong-question-in-the-llm-era",
      "content": "For decades, we have adapted to software. We learned shell commands, memorized HTTP method names and wired together SDKs. Each interface assumed we would speak its language. In the 1980s, we typed 'grep', 'ssh' and 'ls' into a shell; by the mid-2000s, we were invoking REST endpoints like GET /users; by the 2010s, we imported SDKs (client.orders.list()) so we didn’t have to think about HTTP. But underlying each of those steps was the same premise: Expose capabilities in a structured form so others can invoke them.But now we are entering the next interface paradigm. Modern LLMs are challenging the notion that a user must choose a function or remember a method signature. Instead of “Which API do I call?” the question becomes: “What outcome am I trying to achieve?” In other words, the interface is shifting from code → to language. In this shift, Model Context Protocol (MCP) emerges as the abstraction that allows models to interpret human intent, discover capabilities and execute workflows, effectively exposing software functions not as programmers know them, but as natural-language requests.MCP is not a hype-term; multiple independent studies identify the architectural shift required for “LLM-consumable” tool invocation. One blog by Akamai engineers describes the transition from traditional APIs to “language-driven integrations” for LLMs. Another academic paper on “AI agentic workflows and enterprise APIs” talks about how enterprise API architecture must evolve to support goal-oriented agents rather than human-driven calls. In short: We are no longer merely designing APIs for code; we are designing capabilities for intent.Why does this matter for enterprises? Because enterprises are drowning in internal systems, integration sprawl and user training costs. Workers struggle not because they don’t have tools, but because they have too many tools, each with its own interface. When natural language becomes the primary interface, the barrier of “which function do I call?” disappears. One recent business blog observed that natural‐language interfaces (NLIs) are enabling self-serve data access for marketers who previously had to wait for analysts to write SQL. When the user just states intent (like “fetch last quarter revenue for region X and flag anomalies”), the system underneath can translate that into calls, orchestration, context memory and deliver results. Natural language becomes not a convenience, but the interfaceTo understand how this evolution works, consider the interface ladder:EraInterfaceWho it was built forCLIShell commandsExpert users typing textAPIWeb or RPC endpointsDevelopers integrating systemsSDKLibrary functionsProgrammers using abstractionsNatural language (MCP)Intent-based requestsHuman + AI agents stating what they wantThrough each step, humans had to “learn the machine’s language.” With MCP, the machine absorbs the human’s language and works out the rest. That’s not just UX improvement, it’s an architectural shift.Under MCP, functions of code are still there: data access, business logic and orchestration. But they’re discovered rather than invoked manually. For example, rather than calling \"billingApi.fetchInvoices(customerId=…),\" you say “Show all invoices for Acme Corp since January and highlight any late payments.” The model resolves the entities, calls the right systems, filters and returns structured insight. The developer’s work shifts from wiring endpoints to defining capability surfaces and guardrails.This shift transforms developer experience and enterprise integration. Teams often struggle to onboard new tools because they require mapping schemas, writing glue code and training users. With a natural-language front, onboarding involves defining business entity names, declaring capabilities and exposing them via the protocol. The human (or AI agent) no longer needs to know parameter names or call order. Studies show that using LLMs as interfaces to APIs can reduce the time and resources required to develop chatbots or tool-invoked workflows.The change also brings productivity benefits. Enterprises that adopt LLM-driven interfaces can turn data access latency (hours/days) into conversation latency (seconds). For instance, if an analyst previously had to export CSVs, run transforms and deploy slides, a language interface allows “Summarize the top five risk factors for churn over the last quarter” and generate narrative + visuals in one go. The human then reviews, adjusts and acts — shifting from data plumber to decision maker. That matters: According to a survey by McKinsey & Company, 63% of organizations using gen AI are already creating text outputs, and more than one-third are generating images or code. (While many are still in the early days of capturing enterprise-wide ROI, the signal is clear: Language as interface unlocks new value.In architectural terms, this means software design must evolve. MCP demands systems that publish capability metadata, support semantic routing, maintain context memory and enforce guardrails. An API design no longer needs to ask “What function will the user call?”, but rather “What intent might the user express?” A recently published framework for improving enterprise APIs for LLMs shows how APIs can be enriched with natural-language-friendly metadata so that agents can select tools dynamically. The implication: Software becomes modular around intent surfaces rather than function surfaces.Language-first systems also bring risks and requirements. Natural language is ambiguous by nature, so enterprises must implement authentication, logging, provenance and access control, just as they did for APIs. Without these guardrails, an agent might call the wrong system, expose data or misinterpret intent. One post on “prompt collapse” calls out the danger: As natural-language UI becomes dominant, software may turn into “a capability accessed through conversation” and the company into “an API with a natural-language frontend”. That transformation is powerful, but only safe if systems are designed for introspection, audit and governance.The shift also has cultural and organizational ramifications. For decades, enterprises hired integration engineers to design APIs and middleware. With MCP-driven models, companies will increasingly hire ontology engineers, capability architects and agent enablement specialists. These roles focus on defining the semantics of business operations, mapping business entities to system capabilities and curating context memory. Because the interface is now human-centric, skills such as domain knowledge, prompt framing, oversight and evaluation become central.What should enterprise leaders do today? First, think of natural language as the interface layer, not as a fancy add-on. Map your business workflows that can safely be invoked via language. Then catalogue the underlying capabilities you already have: data services, analytics and APIs. Then ask: “Are these discoverable? Can they be called via intent?” Finally, pilot an MCP-style layer: Build a small domain (customer support triage) where a user or agent can express outcomes in language, and let systems do the orchestration. Then iterate and scale.Natural language is not just the new front-end. It is becoming the default interface layer for software, replacing CLI, then APIs, then SDKs. MCP is the abstraction that makes this possible. Benefits include faster integration, modular systems, higher productivity and new roles. For those organizations still tethered to calling endpoints manually, the shift will feel like learning a new platform all over again. The question is no longer “which function do I call?” but “what do I want to do?”Dhyey Mavani is accelerating gen AI and computational mathematics.",
      "published_date": "2026-01-03T22:00:00",
      "source": "VentureBeat",
      "source_id": "venturebeat",
      "language": "en",
      "credibility_score": 9,
      "relevance_weight": 8,
      "focus_tags": [
        "创业融资",
        "金融科技",
        "AI应用",
        "市场洞察"
      ]
    },
    {
      "title": "Nvidia just admitted the general-purpose GPU era is ending",
      "url": "https://venturebeat.com/infrastructure/inference-is-splitting-in-two-nvidias-usd20b-groq-bet-explains-its-next-act",
      "content": "Nvidia’s $20 billion strategic licensing deal with Groq represents one of the first clear moves in a four-front fight over the future AI stack. 2026 is when that fight becomes obvious to enterprise builders.For the technical decision-makers we talk to every day — the people building the AI applications and the data pipelines that drive them — this deal is a signal that the era of the one-size-fits-all GPU as the default AI inference answer is ending.We are entering the age of the disaggregated inference architecture, where the silicon itself is being split into two different types to accommodate a world that demands both massive context and instantaneous reasoning.Why inference is breaking the GPU architecture in twoTo understand why Nvidia CEO Jensen Huang dropped one-third of his reported $60 billion cash pile on a licensing deal, you have to look at the existential threats converging on his company’s reported 92% market share. The industry reached a tipping point in late 2025: For the first time, inference — the phase where trained models actually run — surpassed training in terms of total data center revenue, according to Deloitte. In this new \"Inference Flip,\" the metrics have changed. While accuracy remains the baseline, the battle is now being fought over latency and the ability to maintain \"state\" in autonomous agents.There are four fronts of that battle, and each front points to the same conclusion: Inference workloads are fragmenting faster than GPUs can generalize. 1. Breaking the GPU in two: Prefill vs. decodeGavin Baker, an investor in Groq (and therefore biased, but also unusually fluent on the architecture), summarized the core driver of the Groq deal cleanly: “Inference is disaggregating into prefill and decode.”Prefill and decode are two distinct phases:The prefill phase: Think of this as the user’s \"prompt\" stage. The model must ingest massive amounts of data — whether it's a 100,000-line codebase or an hour of video — and compute a contextual understanding. This is \"compute-bound,\" requiring massive matrix multiplication that Nvidia’s GPUs are historically excellent at.The generation (decode) phase: This is the actual token-by-token \"generation.” Once the prompt is ingested, the model generates one word (or token) at a time, feeding each one back into the system to predict the next. This is \"memory-bandwidth bound.\" If the data can't move from the memory to the processor fast enough, the model stutters, no matter how powerful the GPU is. (This is where Nvidia was weak, and where Groq’s special language processing unit (LPU) and its related SRAM memory, shines. More on that in a bit.)Nvidia has announced an upcoming Vera Rubin family of chips that it’s architecting specifically to handle this split. The Rubin CPX component of this family is the designated \"prefill\" workhorse, optimized for massive context windows of 1 million tokens or more. To handle this scale affordably, it moves away from the eye-watering expense of high bandwidth memory (HBM) — Nvidia’s current gold-standard memory that sits right next to the GPU die — and instead utilizes 128GB of a new kind of memory, GDDR7. While HBM provides extreme speed (though not as quick as Groq’s static random-access memory (SRAM)), its supply on GPUs is limited and its cost is a barrier to scale; GDDR7 provides a more cost-effective way to ingest massive datasets.Meanwhile, the \"Groq-flavored\" silicon, which Nvidia is integrating into its inference roadmap, will serve as the high-speed \"decode\" engine. This is about neutralizing a threat from alternative architectures like Google's TPUs and maintaining the dominance of CUDA, Nvidia’s software ecosystem that has served as its primary moat for over a decade.All of this was enough for Baker, the Groq investor, to predict that Nvidia’s move to license Groq will cause all other specialized AI chips to be canceled — that is, outside of Google’s TPU, Tesla’s AI5, and AWS’s Trainium.2. The differentiated power of SRAMAt the heart of Groq’s technology is SRAM. Unlike the DRAM found in your PC or the HBM on an Nvidia H100 GPU, SRAM is etched directly into the logic of the processor.Michael Stewart, managing partner of Microsoft’s venture fund, M12, describes SRAM as the best for moving data over short distances with minimal energy. \"The energy to move a bit in SRAM is like 0.1 picojoules or less,\" Stewart said. \"To move it between DRAM and the processor is more like 20 to 100 times worse.\"In the world of 2026, where agents must reason in real-time, SRAM acts as the ultimate \"scratchpad\": a high-speed workspace where the model can manipulate symbolic operations and complex reasoning processes without the \"wasted cycles\" of external memory shuttling.However, SRAM has a major drawback: it is physically bulky and expensive to manufacture, meaning its capacity is limited compared to DRAM. This is where Val Bercovici, chief AI officer at Weka, another company offering memory for GPUs, sees the market segmenting.Groq-friendly AI workloads — where SRAM has the advantage — are those that use small models of 8 billion parameters and below, Bercovici said. This isn’t a small market, though. “It’s just a giant market segment that was not served by Nvidia, which was edge inference, low latency, robotics, voice, IoT devices — things we want running on our phones without the cloud for convenience, performance, or privacy,\" he said.This 8B \"sweet spot\" is significant because 2025 saw an explosion in model distillation, where many enterprise companies are shrinking massive models into highly efficient smaller versions. While SRAM isn't practical for the trillion-parameter \"frontier\" models, it is perfect for these smaller, high-velocity models.3. The Anthropic threat: The rise of the ‘portable stack’Perhaps the most under-appreciated driver of this deal is Anthropic’s success in making its stack portable across accelerators.The company has pioneered a portable engineering approach for training and inference — basically a software layer that allows its Claude models to run across multiple AI accelerator families — including Nvidia’s GPUs and Google’s Ironwood TPUs. Until recently, Nvidia's dominance was protected because running high-performance models outside of the Nvidia stack was a technical nightmare. “It’s Anthropic,” Weka’s Bercovici told me. “The fact that Anthropic was able to … build up a software stack that could work on TPUs as well as on GPUs, I don’t think that’s being appreciated enough in the marketplace.”(Disclosure: Weka has been a sponsor of VentureBeat events.)Anthropic recently committed to accessing up to 1 million TPUs from Google, representing over a gigawatt of compute capacity. This multi-platform approach ensures the company isn't held hostage by Nvidia's pricing or supply constraints. So for Nvidia, the Groq deal is equally a defensive move. By integrating Groq’s ultra-fast inference IP, Nvidia is making sure that the most performance-sensitive workloads — like those running small models or as part of real-time agents — can be accommodated within Nvidia’s CUDA ecosystem, even as competitors try to jump ship to Google's Ironwood TPUs. CUDA is the special software Nvidia provides to developers to integrate GPUs. 4. The agentic ‘statehood’ war: Manus and the KV CacheThe timing of this Groq deal coincides with Meta’s acquisition of the agent pioneer Manus just two days ago. The significance of Manus was partly its obsession with statefulness.If an agent can’t remember what it did 10 steps ago, it is useless for real-world tasks like market research or software development. KV Cache (Key-Value Cache) is the \"short-term memory\" that an LLM builds during the prefill phase.Manus reported that for production-grade agents, the ratio of input tokens to output tokens can reach 100:1. This means for every word an agent says, it is \"thinking\" and \"remembering\" 100 others. In this environment, the KV Cache hit rate is the single most important metric for a production agent, Manus said. If that cache is \"evicted\" from memory, the agent loses its train of thought, and the model must burn massive energy to recompute the prompt.Groq’s SRAM can be a \"scratchpad\" for these agents — although, again, mostly for smaller models — because it allows for the near-instant retrieval of that state. Combined with Nvidia's Dynamo framework and the KVBM, Nvidia is building an \"inference operating system\" that enables inference servers to tier this state across SRAM, DRAM, HBM, and other flash-based offerings like that from Bercovici’s Weka.Thomas Jorgensen, senior director of Technology Enablement at Supermicro, which specializes in building clusters of GPUs for large enterprise companies, told me in September that compute is no longer the primary bottleneck for advanced clusters. Feeding data to GPUs was the bottleneck, and breaking that bottleneck requires memory.\"The whole cluster is now the computer,\" Jorgensen said. \"Networking becomes an internal part of the beast … feeding the beast with data is becoming harder because the bandwidth between GPUs is growing faster than anything else.\"This is why Nvidia is pushing into disaggregated inference. By separating the workloads, enterprise applications can use specialized storage tiers to feed data at memory-class performance, while the specialized \"Groq-inside\" silicon handles the high-speed token generation.The verdict for 2026We are entering an era of extreme specialization. For decades, incumbents could win by shipping one dominant general-purpose architecture — and their blind spot was often what they ignored on the edges. Intel’s long neglect of low-power is the classic example, Michael Stewart, managing partner of Microsoft’s venture fund M12, told me. Nvidia is signaling it won’t repeat that mistake. “If even the leader, even the lion of the jungle will acquire talent, will acquire technology — it’s a sign that the whole market is just wanting more options,” Stewart said.For technical leaders, the message is to stop architecting your stack like it’s one rack, one accelerator, one answer. In 2026, advantage will go to the teams that label workloads explicitly — and route them to the right tier:prefill-heavy vs. decode-heavylong-context vs. short-contextinteractive vs. batchsmall-model vs. large-modeledge constraints vs. data-center assumptionsYour architecture will follow those labels. In 2026, “GPU strategy” stops being a purchasing decision and becomes a routing decision. The winners won’t ask which chip they bought — they’ll ask where every token ran, and why.",
      "published_date": "2026-01-03T01:00:00",
      "source": "VentureBeat",
      "source_id": "venturebeat",
      "language": "en",
      "credibility_score": 9,
      "relevance_weight": 8,
      "focus_tags": [
        "创业融资",
        "金融科技",
        "AI应用",
        "市场洞察"
      ]
    }
  ]
}