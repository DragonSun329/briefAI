[
  {
    "title": "Inside America’s AI Strategy: Infrastructure, Regulation, and Global Competition",
    "url": "https://www.youtube.com/watch?v=NyP-euljCHM",
    "source": "Podcast: All-In Podcast",
    "date": "",
    "content": "Great to see everyone and I'm thrilled to be able to talk about the issue of the day and that is artificial intelligence and AI in our world. Um David, Michael, I'd love you to talk about what where we are right now in terms of the pursuit to be the number one uh lead AI country. How are we doing, David? >> I think we're doing great. Um Maria, last year uh President Trump gave a major AI policy speech. is in July and he declared that the United States had to win the AI race. Uh he he had first of all declared that we were in one. Uh and I think his speech was reminiscent of when President Kenny declared that we were in a space race and had to win that race. I think since then what you've seen is that American companies have only innovated more. You're seeing all sorts of really incredible products being released all the time. I think that um American uh uh AI models, chips, um data centers only just keep um getting better and better. And so I feel very good about the American position in this AI race. Certainly we have some very uh you know competent uh and formidable competitors. Um China obviously has a lot of very smart people working in this area. But I do think that uh just what you see from uh American companies in Silicon Valley right now is really incredible. And yet there are still so many questions about all of the spending underway uh to build this out with regard to data centers. And of course the question keeps coming up are we spending too much. Will we get the return on investment? How do you see that? >> I I think that we will. Um I think that the reason why you're seeing this huge infrastructure buildout is because the demand is ultimately there. I I know a lot of people worry and about whether this could be like a dot situation. And remember where we had the whole fiber build out in the late '9s and then we had a doc crash. The difference here is that uh in the late '9s and early 2000s we had a a problem known as dark fiber where you had this fiber buildout and then it didn't get used. There's no such thing as a dark GPU right now. Every GPU that's being put in a data center is getting used. Uh and it's being used to generate tokens and that's to power the this new generation of AI chat bots or coding assistants. uh and there's just been some releases in the last couple months on the coding front that you know it's if you're following what develop software developers are saying they're saying it's mind-blowing it's completely revolutionizing their industry so demand for tokens just increases and that increases the demand for this data center buildout that we're seeing so I don't think it's going to stop anytime soon and just last year this infrastructure buildout added about uh 2% to the GDP growth rate and I and that's what helped propel us to this you know four to 5% % growth rate and I think you're going to see something similar this year. >> Well, it is certainly leading growth, Michael. Um, and I'm so happy to be able to get this conversation going with both of you who are really leading this. David, thank you. And Michael, thank you. Same questions for you, Michael. Assess where we are right now on AI. >> I I think just a reminder for the group for those who haven't been tracking as closely as we do every day. The the plan really had essentially three pillars and it talked about how one, how can the US continue to out innovate our competitors? two, how can we drive the infrastructure build that we need to support um this this AI revolution? And three, how do we actually share with the world or export our great American technology? And for each of those three pillars, there was quite a lot of actions that the federal government has taken to drive that forward. Um, and I think I think we're pretty proud to say that we've made, I think, pretty good progress on on all three. Um just focusing a little bit on the innovation one you were talking about earlier. I think the the the the the core um insight that we've always had about how you drive this innovation is you have to have a regulatory environment that allows this technology to be developed and ultimately commercialized in the United States. And the US has done a great job compared to the rest of the world on sort of setting that up and creating a framework that works. But we can always do better and improve it. And the president in his his speech in July talked a lot about um this issue of a patchwork of state regulations and how can we ensure that there aren't 50 different rules around AI and and what's most what's most important about this debate which I I think a lot of people sometimes don't sometimes miss is the patchwork is actually most detrimental to early stage young companies and entrepreneurs. If you want to develop a new AI technology, if you want to build something on top of one of our great frontier models, having to figure out how to navigate 50 different rules across 50 different states creates a lot of friction and ultimately the big guys are the ones that can succeed in in that environment the best. Um, so we're spending a lot of time trying to think about how can you create a legislative proposal that can actually um deliver on um a sensible national framework to solve to solve this regulatory issue. So, so what would you say then, Michael, are the basic frameworks that are uh sort of must-have in in that kind of federal oversight? Because some states did push back in the US and say, \"No, no, we want to be able to control our destiny when it comes to AI.\" What's most important when you look at that framework in terms of um a federal oversight? Yeah, I think in the executive order the president signed in in December directing us to kind of work through this proposal, he listed a few things that um the state should continue to be able to pursue individually on their own. Um legislation or rules around child safety was on that list. Um the rules around permitting of data centers and buildouts are continuing to be something that states should should look at. So there are a few things that were enumerated, but that's the kind of stuff that I guess Dave and I are going to be working through. I don't know if you have any thoughts on on that. Yeah, I mean I I think the the basic problem that we have is that I mean frankly the states are going hog wild right now with regulation. There's over,200 bills going through state legislatores right now. I think it's very much a knee-jerk reaction. I know there's a lot of fears and concerns about AI, but it seems like for every hypothetical concern, there's um multiple state bills now to try and regulate that thing before we really know how it's going to play out. And um I think it would be better to I think since this technology is so new and the environment is so dynamic, I think it'd be better to spend a little bit more time studying how AI is actually being used and what risks are actually materializing before you overregulate the thing. But in any event, that that's the what we're seeing right now at the state level. And um and and I think that the president's been very consistent that it would be better to have a single have one rule book, a single rule book at the federal level, lightweight federal standard. Uh I think this problem is only going to get more acute over time because again you you as you have 50 different states running in 50 different directions, the patchwork problem only gets um more significant. So, in any event, this is something that we're going to work, I think, closely together on this year, which is to see if we can get enough consensus on a federal framework to enact a law. Only Congress can ultimately cramp the states. We understand that. Um, and you know, as you know, it's very difficult to get a bill through Congress. You need 60 votes in the Senate. So, has to be bipartisan to to a certain degree. So, but we're going to try and see if we can um work to get that consensus. >> Yeah. And do you have any clarity on the timing on that in terms of um support in Congress for a federal oversight or do you see push back there as well depending on the state you're talking about? >> Well, there's push back in Congress to the idea of preeemption without a federal standard. So, in other words, you can't replace something with nothing. This is sort of the the thing that we heard uh repeatedly. But I think there is uh quite a bit of interest in both the House and the Senate towards having again some sort of lightweight federal standard. But we're still in the early stages of those conversations and we're going to see what we can try and get done this year. >> Meanwhile, you've got some people pushing back after wanting to see the innovation and growth of data centers. Now they're saying not in my backyard. What about that? Is that an issue? >> Yeah, I mean we got a letter recently from Bernie Sanders saying stop all data centers, all data center development. And you know if we do that we will lose the AI race. I mean you do need this infrastructure. Uh other countries are building out this infrastructure. China's building out I think they're um spinning up a a new uh nuclear power plant or um or coal plant new energy every single week and a lot of that is going to power their data centers. So it would fundamentally I think uh the United States in the AI race if we just stopped building data centers altogether. At the same time, there are concerns about affordability, about um whether consumers would have to pay a higher electrical rate because of data centers. Uh President Trump's been really clear that consumers should not have to pay higher rates for electricity because of data centers. You saw just last week Microsoft stepped up and made a pledge that it will that its data centers will not cause residential rates to increase. I think you'll likely see other tech companies stepping up and making similar commitments. And in fact, when I've talked to the hyperscale",
    "summary": "The podcast discusses the United States' position in the global AI race, emphasizing the country's competitive edge due to innovation from American companies and the infrastructure buildout supporting AI. Concerns are raised about state-level overregulation, with a call for a unified federal framework to foster innovation and prevent a patchwork of regulations.",
    "entities": [
      "United States",
      "President Trump",
      "China",
      "Silicon Valley",
      "Michael"
    ],
    "topics": [
      "AI race",
      "AI policy",
      "infrastructure buildout",
      "regulation",
      "global competition"
    ],
    "credibility_score": 8,
    "podcast_channel": "All-In Podcast",
    "duration_min": 47.0,
    "is_transcript": true
  },
  {
    "title": "Europe's Free Speech Crackdown and the \"Censorship Industrial Complex\" with Sarah B. Rogers",
    "url": "https://www.youtube.com/watch?v=e59hzyoKQCs",
    "source": "Podcast: All-In Podcast",
    "date": "",
    "content": "David and I are staying in a 300-year-old house and uh we've both smashed our head on the beams twice already. Uh but this is both our first Davos, David. It's our first Davos. We've been here for 24 hours and um uh any first impressions here. >> Um it's interesting. You know, we're we're staying very far away. >> Yeah, >> they didn't apparently they didn't want you to be part of this. >> They didn't want me too close. Yeah. >> But we finally we got you finally got you an invitation. your invitation to not get lost in the mail. >> My invitation didn't get lost in the mail this time. >> For those of you who watch the pod, you know what I'm talking about. >> Inside joke. Um, but it's great to be here and uh great to be here at USA House. Thanks to all the sponsors and um really delighted for our first guest for the pod. Sarah Rogers works for is the under secretary uh for public diplomacy at the State Department. for members here who don't know this position or what you've been charged with or what you've decided to work on. I'm curious about that. Do they tell you what to do or do you come up with your own mandate? But yeah, tell us everything about what you're doing. >> Longtime listener, first time guest and thank you to both of us. Uh thank you from all of us at America House for joining us here. >> Yeah. >> So I am the under secretary for public diplomacy and when I got this nomination, my friends and family all congratulated me and then conferably said what is that? >> Yeah. Uh so diplomacy traditionally concerns the relationship between the American government and foreign governments. Two ambassadors, shake hands, make a deal, um solve a war. But public diplomacy is different. Public diplomacy addresses the relationship between the American government and foreign publics. And this has become a very important under secretariat with the rise of the internet and then during the Biden administration especially uh these mushrooming concerns about so-called disinformation and what do we do when there are allegedly malign influences on the public view of America um the public's intersection with American interest how do we interact with the internet and the information ecosystem that is part of my portfolio I also oversee other soft power activities including our educational and cultural and sports diplomacy. So I am privileged to play a role in the World Cup this summer and the LA Olympics coming up and uh the Fulbright program and others like it. So >> you seem particularly focused on freedom of speech and a little bit of tension between our standards and the companies in America which have made the move to being strongly freedom of speech. something that kind of got lost in our industry for a couple years in technology but has now made I think um some significant progress on does seem like some folks in Europe don't share our love of freedom of speech. Maybe you could explain to us what the tension is today and what some of the regulations are that have been put in place in Europe. >> Sure. Absolutely. So the two main regulations that I've interfaced with since taking office and part of this is just a product of my first official trip was to Europe and while I was in Europe a large fine came down on an American platform X under the digital services act which I'll get into in a moment. So Europe um especially since since the second world war but I think really since the American founding and our codification of the first amendment you know America has taken a much stronger approach on free speech than even most of the west and with the rise of the internet and all communication or a lot of communication becoming transnational. We see these new technocratic regulatory frameworks in Europe bumping up against the commitments to free speech in the United States. And Jason makes an important observation that for a while some of these large American technology platforms were more inclined to moderate or to censor kind of in conformity with some prevailing um some prevailing norms and concerns in the United States. But I think in the United States we've shifted back toward a less centurious approach and so have these platforms. And at the same time, you have uh regulatory efforts in Europe and the UK. And I'll I'll name a couple that I think have been particularly relevant. So the UK has something called the online safety act. The online safety act imposes age gating obligations on a broad swath of content, almost any content that's upsetting, and then requires platforms to run risk assessments for and in some cases remove content that the UK would say is illegal. And in the UK, you know, major categories of content are banned, are rendered illegal. That would not be illegal in the United States, which is where these platforms are located, which is where their original user base is, which is where their executives live um and which is their primary regulator. So, under the online safety act, we now have um active litigation by the relevant regulator against several American websites. These are websites that don't reach into the UK. They're not these aren't websites dedicated to discussing the queen. They're not websites that sell goods in the United Kingdom. These are websites that exist on American soil, host large quantities of American users, and in often in oftentimes discuss American political topics. But because users are permitted to discuss them in a way that offends UK law, there's the imposition of a UK fine. The Digital Services Act in the EU is similar. So DSA contains but doesn't just contain uh contentbased regulations, hate speech regulations. So DSA requires all of the EU member states to adopt at minimum kind of a floor for hate speech prohibition. And those prohibitions in the statute, I think, are much vagger than American lawyers are accustomed to. And one of our one of our jurist credential principles under the American First Amendment is if you're going to enact any regulation that comes close to touching speech, it needs to be very clear what you are prohibiting because you have this chilling effect concept. A vague prohibition will chill speech, especially when that prohibition is imposed on a large riskaverse corporation. So you impose vague prohibitions on large riskaverse corporations and that's how it becomes illegal to make jokes around the water cooler for example. You see the same effect here. Digital services act also regulates other aspects of um digital commerce and social media. So it regulates um things like transparency and competition. And I think you know we have we have a lot of Europeans in the audience today and I hope none of them will find it contentious if I suggest that in Europe there is more of a focus on uh technocratic regulation as an arbiter of what's acceptable than there might be in America where we have this tradition that um really emphasizes like rugged individualism and individual conscience. And to be clear, no one is saying, certainly not the State Department or America, hey, you can't have your own platforms in Europe, right? >> Build your own. Build your own Facebook, build your own Instagram, build your own Twitter/X, uh, Tik Tok, whatever you'd like to build, and you can have whatever standards you like on your platforms. We're saying, \"Hey, these are our platforms. This is our standard, and we don't want our users uh or our platforms to be receiving fines.\" That's that's our position. >> I I think I think that's basically it. And look, when American companies operate abroad, they abide by the laws where they operate. But at a certain point, so so we recently issued some sanctions, which we'll get into. And one of the individuals we sanctioned uh was a former EU official who threatened Elon Musk with enforcement action because X within the United States had said that it was going to host on a live uh Twitter space an interview with Donald Trump, our president. So it wasn't that Donald Trump had said anything violative. It wasn't there was a specific piece of content that the EU wanted to ban. It was just that the act of an American business hosting an interview with an American president might offend EU uh EU preferences about speech generated a regulatory threat. And when you reach across borders and make a threat like that, that offends American interests and American values. And so you can expect America to respond. And I think so I my history is as an American lawyer in American courts and we have you know we're a nation of 50 states and each state has its own regulations and we've had to think about you know when there's a website in California that operates in Texas how do you decide to what extent Texas gets to regulate and we have all these jurisdictional concepts like does the website purposefully avail itself of the forum is it is are you posting defamatory statements about a person in Texas but the mere existence of a website in California that Texas doesn't like is hardly ever, basically never, a basis for regulation. And so when we talk about things like extr territoriality, you know, what we're really talking about is it's undisputed that Europeans get to have their own laws in Europe, but we also get to have our own laws in the United States. And we're celebrating 250 years of American independence. And so we want, you know, we want our markets to be able to interoperate online. Um, but we're not willing to give up American freedom of speech and the bargain. >> Hey, David, when we look at, and I'm asking you this one so I can give you a pass on it, but what what do you think people are so scared of in the UK when it comes to freedom of speech? Uh, and and maybe the most freedom of the the most um rockus platform X specifically. >> Well, I don't think the people are afraid. I think the government is afraid of the people criticizing it and therefore they're engaged in what sensors always do which is protect the people in power. Um there's s",
    "summary": "In this podcast, Sarah B. Rogers, Under Secretary for Public Diplomacy at the US State Department, discusses the tension between European regulations and American free speech standards, particularly in the context of the internet. She highlights the differences in approach to content moderation between the US and Europe, and the challenges this presents for American tech platforms operating in Europe.",
    "entities": [
      "Sarah B. Rogers",
      "State Department",
      "European Union",
      "UK",
      "Digital Services Act",
      "Online Safety Act",
      "Twitter",
      "Donald Trump",
      "Elon Musk"
    ],
    "topics": [
      "Free speech",
      "Censorship",
      "European regulations",
      "American tech platforms",
      "Public diplomacy",
      "Content moderation"
    ],
    "credibility_score": 8,
    "podcast_channel": "All-In Podcast",
    "duration_min": 45.0,
    "is_transcript": true
  },
  {
    "title": "Paul Rosolie: Uncontacted Tribes in the Amazon Jungle | Lex Fridman Podcast #489",
    "url": "https://www.youtube.com/watch?v=Z-FRe5AKmCU",
    "source": "Podcast: Lex Fridman",
    "date": "",
    "content": "- ... were standing there. Everyone is waiting, 'cause at any moment an arrow could just fly through your neck, and there's people holding shotguns. And the anthropologist, this little guy, is standing there in the front, and he's going, \"Enimole.\" He's going, \"Brothers.\" And then it happened. Then you start hearing people screaming, \"Mashco! Mashco!\" And people are screaming and women are lifting children and running into the huts and the dogs and chickens are going nuts and— - So fear, fear. - Fear. He's going, \"Look there. He has a bow. He has a bow.\" And we're looking up the beach and there's just this clan walking down the beach with these seven-foot bows and they're hunched over and they're pointing at us. They're going, \"Look at that one. Look, there's a gun there.\" And you can see them communicating to each other and the butterflies are swirling off the beach and they can hit a spider monkey out of the treetops at 40 meters. They can sneak up and you will never know they're there. And so when that arrow passes through your body, you'll only have a moment to realize it before you fall over. In order for any of this to make sense, I have to show you this footage. - And this has not been shown ever before. - This is a world first. - The following is a conversation with Paul Rosolie, his third time on the podcast. Paul is a naturalist, explorer, writer and is someone who has dedicated his life to protecting the Amazon rainforest and celebrating the beauty of the natural world. He has a new book coming out in a few days titled Jungle Keeper that you should definitely go pre-order now. It tells some intense stories about his time in the jungle over the past several years, building up to a few epic recent events, including a new extended encounter with a non-contacted tribe that we discuss in this podcast. Both the book and audiobook are great. I highly recommend it. If you would like to support Paul and his team in their mission to protect the jungle, go to junglekeepers.org. You can help with donations or by spreading the word or checking out the gala that Paul is hosting in New York on January 22nd in a few days. They are doing all they can to help raise funds for the mission of safeguarding as much of the rainforest as possible, and I think it's a mission worth fighting for. The Amazon jungle is one of the most special and beautiful places on Earth. As an aside, allow me to look back briefly and mention something that I've been struggling with a bit. For context, I traveled to the Amazon rainforest with Paul a while back. It was an adventure of a lifetime, with lots of crazy twists and turns. We did record a podcast out there, literally in the jungle. Episode 429, if you want to go check it out. It was awesome. And we also recorded a bunch of disparate footage of the journey just for fun. And I would still love to somehow put all that together into a cohesive video in case it's interesting to someone. But I've learned just how difficult it is to organize and edit a pile of chaotically recorded footage like that. So, let's see if I can pull it off. But in any case, this kind of raw vlog-style video is something that I would love to be able to do more of as a way to celebrate amazing human beings like Paul and others, including everyday people who I meet on my travels. So, I'll keep trying, tinkering, learning, and I ask for your patience and support along the way. Now, back to our regularly scheduled programming. This is the Lex Fridman Podcast. To support it, please check out our sponsors in the description where you can also find links to contact me, ask questions, give feedback, and so on. And now, dear friends, here's Paul Rosolie. We've survived a challenging time out in the jungle about a year and a half ago, and since then, your life has increasingly gotten more intense. So, you've achieved the incredible feat of saving now more than 130,000 acres of rainforest. And the goal that you're working towards is protecting 200,000 acres more. And doing so while facing extreme danger from narcos, narco-traffickers, so-called Cocaine Mafia in an escalating drug war. This is insane. These are new developments. Illegal loggers, as we've talked about before. Gold miners, and the incredible recent encounter with an uncontacted tribe. And we'll talk about all of this. So your new book, Jungle Keeper, opens with the killing of two loggers by the warriors of an uncontacted tribe, the Mashco Piro, in August 2024. And then you reveal that you had your own dramatic encounter with the tribe two months later in October 2024. So if I may, let me read the opening of the book. \"Far out on the western edge of the Amazon rainforest, deep in the Peruvian jungle, a pair of loggers plunged their chainsaws into the buttressed roots of an ancient ironwood. An ironwood, or shihuahuaco, of this size is a giant among giants, an emergent sentinel that reaches heights of 160 feet, towering over the rest of the canopy.\" I've read that many are over 1,000 years old, by the way, as an aside. And you've found ones that are 1,200 years old. - Incredibly old. - Anyway, you continue. \"This particular tree had started its life as a tiny sapling in the great jungle, a story that began before the Spanish reached Peru, long before the United States was even a dream. At a time when Leonardo da Vinci was still honing his talents in a far away part of the world, through the Renaissance, the First and Second World Wars, and the birth of our grandparents.\" This tree was out there slowly charging upward, anonymous, just one pillar among the billions of others. But on this day, in August 2024, when the two loggers worked, this witness of the centuries came crashing down to the canopy with such cataclysmic power that it shook the earth. And then you go on to talk about how the shaking of the earth was felt and heard by the uncontacted tribe. You go on to describe how these particular loggers were killed by the uncontacted tribe of Mashco Piro. What do we know about these warriors of the uncontacted tribe? - We know that across the Amazon basin there's still perhaps thousands of clans of uncontacted peoples, people that are living in nomadic isolation in what remains of the intact Amazon basin and want to remain that way. And so, what happened with these loggers was that local people told them, \"Don't go out there. Don't go into these territories.\" And what happens is that people that aren't from... There's this thing with the jungle, people don't believe that it's as wild as the legends say. And so when they say there's calatos out there, there's wild people out there, these loggers from another region go, \"Yeah, that's just some story. We're fine. We'll go. We have shotguns.\" They don't realize you're dealing with a civilization of people that is still nomadic, still uses bamboo-tipped arrows, still lives naked in the Amazon rainforest, has knowledge of medicines that we've yet to encounter or may never discover, and that they can hit a spider monkey out of the treetops at 40 meters. And so while you're using a chainsaw, they can sneak up and you will never know they're there. And so when that arrow passes through your body, you'll only have a moment to realize it before you fall over. - And we're looking at something you posted on your Instagram- ... which are the arrows that they use, which are bigger than you. So they're like six or seven feet. - Six, seven feet. More like seven feet. And that's- - Look how sharp that is. - ... incredibly sharp. They cure it over the fire and they have a way of sharpening it. That edge of bamboo becomes incredibly knife-sharp. You can cut meat with it easily; I've done it. These arrows... Look at that. I mean, I'm 5'9\". That's easily a seven-foot arrow. - Yeah, so for people who are just listening, this \"arrow\" is really a spear. Some people would think it was a spear, but they're shooting this thing with a gigantic bow. That's crazy. - Yeah, and so to be holding that... Look at that, they even twist the fletching so the arrow spins in the air. They have incredible craftsmanship, and then you see all the little string on there is plant fibers they've woven, and then this is them. - Yeah, the warriors of the tribe. - The warriors of the tribe. And so the fact that we're sitting here talking on microphones, and that we have airplanes and cell phones and all the things we have in the modern world, and there's still... We still live in this age where there's, right now at this moment, people living out in the jungle who have been there since before history, is an incredible thing. - Let me look this up on Perplexity: what are the technologies we modern humans have that the Mashco Piro do not? It's just interesting to think about the kind of technologies we take for granted. Energy and power, obviously all the electricity generation, grids, batteries, solar panels, and electric motors, metals and materials, mass-produced steel, aluminum, advanced alloys, plastics, composites, glass, concrete, all of those things. - All of those things. - Tools, of course, and machinery. The infrastructure of roads and bridges and buildings, and the weapons of war—everything but the spears and arrows that they have—and the medicine and biology. Of course they probably have complicated medicines that they've developed for their own— ...that are available within the jungle. - I mean, that entire list is no. - No. - I mean, metal, I think you have to be able to excavate into the earth and forge metal. These people don't even... As one of the local anthropologists said to me, a Peruvian anthropologist, he said, \"You know, people think of them as Stone Age tribes.\" And he was like, \"They don't have stones.\" He's like, \"They don't...\" So they don't know that water... They see water that they drink. They don't know that water freezes, because they've never seen it. They don't know that water boils, because they don't even make clay pots. They j",
    "summary": "Paul Rosolie, a naturalist and explorer, discusses his experiences in the Amazon rainforest, including an encounter with an uncontacted tribe, the Mashco Piro, and his efforts to protect the rainforest from threats such as narco-traffickers, illegal loggers, and gold miners. He has saved over 130,000 acres of rainforest and aims to protect 200,000 more.",
    "entities": [
      "Paul Rosolie",
      "Mashco Piro",
      "Amazon rainforest",
      "Jungle Keeper",
      "junglekeepers.org"
    ],
    "topics": [
      "Uncontacted tribes",
      "Amazon rainforest protection",
      "Threats to the rainforest",
      "Encounters with the Mashco Piro tribe"
    ],
    "credibility_score": 9,
    "podcast_channel": "Lex Fridman",
    "duration_min": 186.0,
    "is_transcript": true
  },
  {
    "title": "Infinity, Paradoxes, Gödel Incompleteness & the Mathematical Multiverse | Lex Fridman Podcast #488",
    "url": "https://www.youtube.com/watch?v=14OPT6CcsH4",
    "source": "Podcast: Lex Fridman",
    "date": "",
    "content": "- The following is a conversation with Joel David Hamkins, a mathematician and philosopher specializing in set theory, the foundation of mathematics and the nature of infinity. He is the number one highest rated user on MathOverflow, which I think is a legendary accomplishment. MathOverflow, by the way, is like StackOverflow but for research mathematicians. He is also the author of several books, including Proof in the Art of Mathematics and Lectures on the Philosophy of Mathematics. And he has a great blog, infinitelymore.xyz. This is a super technical and super fun conversation about the foundation of modern mathematics and some mind-bending ideas about infinity, nature of reality, truth, and the mathematical paradoxes that challenged some of the greatest minds of the 20th century. I have been hiding from the world a bit, reading, thinking, writing, soul-searching, as we all do every once in a while. But mostly, just deeply focused on work and preparing mentally for some challenging travel I plan to take on in the new year. Through all of it, a recurring thought comes to me, how damn lucky I am to be alive and to get to experience so much love from folks across the world. I want to take this moment to say thank you from the bottom of my heart for everything, for your support, for the many amazing conversations I've had with people across the world. I got a little bit of hate and a whole lot of love, and I wouldn't have it any other way. I'm grateful for all of it. This is the Lex Fridman Podcast. To support it, please check out our sponsors in the description, where you can also find ways to contact me, ask questions, give feedback, and so on. And now, dear friends, here's Joel David Hamkins. Some infinities are bigger than others. This idea from Cantor at the end of the 19th century, I think it's fair to say, broke mathematics before rebuilding it. And I also read that this was a devastating and transformative discovery for several reasons. So one, it created a theological crisis. Because infinity is associated with God, how could there be multiple infinities? And also, Cantor was deeply religious himself. Second, there's a kind of mathematical civil war. The leading German mathematician Kronecker called Cantor a corrupter of youth and tried to block his career. Third, many fascinating paradoxes emerged from this, like Russell's paradox, about the set of all sets that don't contain themselves, and those threatened to make all of mathematics inconsistent. And finally, on the psychological side and the personal side, Cantor's own breakdown. He literally went mad, spending his final years in and out of sanatoriums, obsessed with proving the continuum hypothesis. So, laying that all out on the table, can you explain the idea of infinity, that some infinities are larger than others, and why was this so transformative to mathematics? - Well, that's a really great question. I would want to start talking about infinity and telling the story much earlier than Cantor, actually, because, I mean, you can go all the way back to Ancient Greek times when Aristotle emphasized the potential aspect of infinity as opposed to the impossibility, according to him, of achieving an actual infinity. And Archimedes' method of exhaustion where he is trying to understand the area of a region by carving it into more and more triangles, say, and sort of exhausting the area and thereby understanding the total area in terms of the sum of the areas of the pieces that he put into it. And it proceeded on this kind of potential understanding of infinity for hundreds of years, thousands of years. Almost all mathematicians were almost all mathematicians were potentialists only and thought that it was incoherent to speak of an actual infinity at all. Galileo is an extremely prominent exception to this, though he argued against this sort of potentialist orthodoxy in The Dialogue of Two New Sciences. Really lovely account there that he gave. And that the... In many ways, Galileo was anticipating Cantor's developments, except he couldn't quite push it all the way through and ended up throwing up his hands in confusion in a sense. I mean, the Galileo paradox is the idea or the observation that if you think about the natural numbers, I would start with zero but I think maybe he would start with one. The numbers one, two, three, four, and so on, and you think about which of those numbers are perfect squares. So zero squared is zero and one squared is one and two squared is four, three squared is nine, 16, 25, and so on. And Galileo observed that, that the perfect squares can be put into a one-to-one correspondence with all of the numbers. I mean, we just did it. I associated every number with its square. And so it seems like on the basis of this one-to-one correspondence that there should be exactly the same number of squares, perfect squares as there are numbers, and yet there's all the gaps in between the perfect squares, right? And, and this suggests that there should be fewer perfect squares, more numbers than squares because the numbers include all the squares plus a lot more in between them, right? And Galileo was quite troubled by this observation because he took it to cause a kind of incoherence in the comparison of infinite quantities, right? And another example is, if you take two line segments of different lengths, and you can imagine drawing a kind of foliation, a fan of lines that connect them. So the endpoints are matched from the shorter to the longer segment, and the midpoints are matched and so on. So spreading out the lines as you go. And so every point on the shorter line would be associated with a, a unique distinct point on the longer line in a one-to-one way. And so it seems like the two line segments have the same number of points on them because of that, even though the longer one is longer. And so it makes, again, a kind of confusion over our ideas about infinity. And also with two circles, if you just place them concentrically and draw the rays from the center, then every point on the smaller circle is associated with a corresponding point on the larger circle, you know, in a one-to-one way. And, and again, that seems to show that the smaller circle has the same number of points on it as the larger one, precisely precisely because they can be put into this one-to-one correspondence. Of course, the contemporary attitude about this situation is that those two infinities are exactly the same, and that Galileo was right in those observations about the equinumerosity. We would talk about it now by appealing to what I call the Cantor-Hume principle, or some people just call it Hume's principle, which is the idea that if you have two collections, whether they're finite or infinite, then we want to say that those two collections have the same size. They're equinumerous if and only if there's a one-to-one correspondence between those collections. Galileo was observing that line segments of different lengths are equinumerous, and the perfect squares are equinumerous with all of the natural numbers, and any two circles are equinumerous, and so on. The tension between the Cantor-Hume principle and what could be called Euclid's principle, which is that the whole is always greater than the part, is a principle that Euclid appealed to in the Elements many times when he's calculating area and so on. It's a basic idea that if something is just a part of another thing, then the whole is greater than the part. So what Galileo was troubled by was this tension between what we call the Cantor-Hume principle and Euclid's principle. It wasn't fully resolved, I think, until Cantor. He's the one who really explained so clearly about these different sizes of infinity and so on in a way that was so compelling. He exhibited two different infinite sets and proved that they're not equinumerous; they can't be put into one-to-one correspondence. It's traditional to talk about the uncountability of the real numbers. Cantor's big result was that the set of all real numbers is an uncountable set. Maybe if we're going to talk about countable sets, then I would suggest that we talk about Hilbert's Hotel, which really makes that idea perfectly clear. - Yeah, let's talk about Hilbert's Hotel. - Hilbert's Hotel is a hotel with infinitely many rooms. Each room is a full floor suite. So there's floor zero... I always start with zero because for me, the natural numbers start with zero, although that's maybe a point of contention for some mathematicians. The other mathematicians are wrong. - Like I mentioned, I'm a programmer, so starting at zero is a wonderful place to start. - Exactly. So there's floor zero, floor one, floor two, or room zero, one, two, three, and so on, just like the natural numbers. So Hilbert's Hotel has a room for every natural number, and it's completely full. There's a person occupying room N for every N. But meanwhile, a new guest comes up to the desk and wants a room. \"Can I have a room, please?\" The manager says, \"Hang on a second, just give me a moment.\" You see, when the other guests had checked in, they had to sign an agreement with the hotel that maybe there would be some changing of the rooms during this stay. So the manager sent a message up to all the current occupants and told every person, \"Hey, can you move up one room, please?\" So the person in room five would move to room six, and the person in room six would move to room seven and so on. And everyone moved at the same time. And of course, we never want to be placing two different guests in the same room, and we want everyone to have their own private room and... But when you move everyone up one room, then the bottom room, room zero, becomes available, of course. And so he can put the new guest in that room. So even when you have infinitely many things, then the new guest can be accommodated. And that's a way of showing how the particular infinity of the occupants of Hilbert's H",
    "summary": "In this Lex Fridman podcast, mathematician and philosopher Joel David Hamkins discusses the concept of infinity, its transformative impact on mathematics, and the paradoxes that emerged from it. The conversation delves into the historical development of infinity's understanding, from Ancient Greek times to Cantor's groundbreaking work, and its implications for the nature of reality and truth.",
    "entities": [
      "Joel David Hamkins",
      "MathOverflow",
      "Cantor",
      "Kronecker",
      "Russell",
      "Aristotle",
      "Archimedes",
      "Galileo"
    ],
    "topics": [
      "infinity",
      "foundation of mathematics",
      "mathematical paradoxes",
      "nature of reality",
      "truth",
      "Ancient Greek mathematics",
      "Galileo's paradox",
      "Cantor-Hume principle",
      "Euclid's principle"
    ],
    "credibility_score": 9,
    "podcast_channel": "Lex Fridman",
    "duration_min": 232.0,
    "is_transcript": true
  },
  {
    "title": "No Priors Live: Building Durable Software in the AI Age with MongoDB President & CEO CJ Desai",
    "url": "https://www.youtube.com/watch?v=qQAhK2EWw4I",
    "source": "Podcast: No Priors: AI, Machine Learning, Tech, & Startups",
    "date": "",
    "content": "since 2022. The future of software is in question. This is from the investor community but also customers. It is a very pivotal moment on the software stack. And then you look at the software stack and you say okay what is the one thing that will always be there? How many companies today that are there that are more than 10 billion in just pure play software revenue? It's single digits. Why is that? The software industry has been around for a long time created by many many smart people like yourself. Why is it  only singledigit companies are more than 10 billion in revenue? Because >> platforms are rare. >> Platforms are rare. Speed matters. When technology transitions happen, are you building as fast as you can? And then are you learning on that technology shift? Whether it's the internet age or AI age or mobile, are you pivoting fast? It's just that you have to stay ahead of that game. If you fall behind that game, investors or customers will always ask you that  question, what is the future of your company? >> Welcome to the very first live recording of the No Priors podcast with host Sarah Guo and MongoDB president and CEO CJ Desai. Hey everyone, I am so happy to be here with you guys and my uh longtime friend CJ. I know you guys have had a great day of announcements and and learnings here at the conference, but I I'm really excited personally to have the opportunity to zoom out with CJ to talk about uh the future of software um what's happening in SAS and and where the value is going to be. These are important questions to me in my you know day job as a as a venture investor. Um so CJ, you have worked at these platform enterprise software and infrastructure companies. uh became CEO of MongoDB recently. I feel like the one question that we were just talking about that every investor asks you and then everybody in the technology ecosystem has the back of their mind is what is the value of software when you can generate a bunch of software and and so I I'd love to just get your thoughts on this. >> That's a very spicy question to start with. I like it. >> I'm making sure everybody's awake. >> Yeah. Yeah. >> U first thank you for doing no prior live for the first time. it's made in and we have a really good crowd here. So, uh it's always exciting. You know when you think about technology transitions software whether you look at internet age or mainframe all the way till AI you have to really think through what is the mode here right whichever applications you create you know SAS applications got created late '9s right late '90s I think Salesforce had their 25 years anniversary recently and so SAS has been around for at least 25 years from from a transition perspective and now with AI the question is just in general what is the future of software what's the stack and do you really have a moat as a company or not there are folks uh who will say hey my moat is I have a great customer relationship or my channel is amazing and that's my mode as I disrupt myself within um but from my standpoint speed matters matters, right? Speed matters. So when technology transitions happen, are you building as fast as you can and then are you learning on that technology shift? Whether it's the internet age or AI age or mobile back in u early 2010 when meta made the pivot towards uh mobile are you pivoting fast and if you pivot fast to leverage the technology whatever the platform shifts are happening, I think it is fine. It's just that you have to stay ahead of that game. If you fall behind that game, investors or customers will always ask you that question, what is the future of your company? And that is something you have to be on the leading edge. Not every bet will work. But from my standpoint, the just going on the extreme terminal value being zero for some of the software, they are overblown. uh and you know uh we'll figure this out together. part of your career you were leading product at service now it is a fran it is one of the most durable enterprise software companies or so everyone assumed until relatively recently and now now this question is up for debate um I I think for a lot of people within an engineering mindset who think about buying developer tools or using developer infrastructure uh the the word like customer stickiness or distribution as the moat it feels very abstract can you just talk a little bit about like how important you know service now as an example is to its customers and why what you think about that. >> One of the things is platforms are sticky products are not. >> Okay. >> So no matter which software company you create today in the world of uh in the age of AI or you created in the past products can be replaced. uh my hiring manager at service now Frank Slutman used to say tools are for fools as in if you say this is a software tool >> that's never a good sign and that's why he used to say that tools are for fools uh so one is products can be replaced because that's a fast uh software is a disruptive market so you want to make sure that you have a platform that you are positioning to the customer whether it's a builder here in San Francisco that is creating a brand new company for a use case all the way to a very large company that is selling to say a large bank. Okay. So one is when you position yourself as a platform maybe that increases your sales cycle versus selling hey you're using this now you can use this go ahead and replace it right so platforms are sticky because it's a thoughtful decision from a customer's perspective the second thing that >> this is actually not the general advice of many people in the startup and VC community so I want to I want to talk about this a little bit and then I want to get to your second point >> um you You know a lot of people talk about having like a wedge right and for service now you know it service desk would have been considered the wedge. Is that not the right is that like a wrong retelling of history here? I mean you need an initial use case and that initial use case has to be a killer use case because if you go in front of a large bank or a healthcare company or a manufacturing company say you are building something here uh with this great audience we have you can say okay this is a disruptive way to think of a legal use case or a finance use case or a help desk use case that's great that's your entry point but then the entry that was easy for you to get in if it was disruptive exit would be in a similar way easy uh because they have not built things around you right so that's that's the main part that okay today it will work for your maybe 0 to 100 million uh 0 to 10 10 to 100 whatever steps you want to go in but it gets harder and harder set up from 100 to billion billion to 5 billion and then 10 billion plus I mean how many companies today that are there that are more than 10 billion in software, >> 10 billion in just pure play software revenue. >> How many companies are there? It's single digits. >> Okay. Why is that? The software industry has been around for a long time created by many many smart people like yourselves. Why is it only singledigit companies are more than 10 billion in revenue? Because >> platforms are rare. >> Platforms are rare and platforms are rare. So one is your dream or aspiration as a software company should that you become a platform and once you are a platform that means you have n equals at least two you have two plus products being used by your customers from whatever you are offering they all work in unison with each other so it's sticky truly from a technology perspective and then what I would argue further all the integrations that your customer has to do with their existing systems because remember if you go to a large bank, bank has been around for some of them 100 plus years. You go to a large insurance company, they've been around for 100 plus years. If you go to a healthcare company, same thing. You look at Fortune 10, Fortune 100, Fortune 500, that's where the TAM is. Mhm. >> And if that's where the time is and you go there and if it is just a product eventually you're going to max out and then you'll have to uh add multiple things. So if you're a platform you're sticky products work with each other and then those products work with all the systems you have and I'll just make it very specific speaking to a bank on behalf of MongoDB. They run their commercial banking applications on top of MongoDB. They have built lot of other integrations. They have done all the security checks, governance, all of that stuff. And I said, \"Wow, gee, how many applications you have built on MongoDB?\" They say very critical and that matter. I said, \"How many?\" And finally the CTO tells me in London, \"30.\" I said, \"Wow, okay, that's great. 300 applications built on MongoDB.\" I said, \"Cj, don't worry. Thank you for coming to London. We are not going anywhere.\" I said, \"Can I just ask you what's the denominator? I understand the numerator is 300.\" He said, 9,000. I said 9,000 that's a great opportunity for MongoDB. Uh so >> he said I'm not going anywhere. >> I'm not going. So that's when the more they use the more sticky we get and then we are in the fabric of their infrastructure. So, so the other premise that you know some builders and buyers and and uh investors now have is those 9,000 applications uh with vibe coding being possible or engineering and some code generation uh some set of those are just going to be you know made on demand or you know in niche ways by every company. What's your take on this? and that and that's the way that this bank or whoever it is as a customer is going to get exactly what they want and they're not going to be horizontal more standardized or even vertical applications anymore. >> Yeah. But if you are you know trying to sell banks have huge budget for technology right so okay you used a V coding platform AB and C you created an app great so your app velocity has increased if you used MongoD",
    "summary": "In this episode of No Priors Live, MongoDB President and CEO CJ Desai discusses the future of software in the AI age, emphasizing the rarity and importance of platforms, the need for speed in technology transitions, and the challenges of building durable software companies that can scale to over $10 billion in revenue.",
    "entities": [
      "MongoDB",
      "Sarah Guo",
      "CJ Desai",
      "ServiceNow",
      "Salesforce",
      "AI",
      "Internet",
      "Mobile"
    ],
    "topics": [
      "Future of software",
      "Technology transitions",
      "Platform stickiness",
      "Customer relationships",
      "Durability of software companies",
      "Revenue growth in software industry"
    ],
    "credibility_score": 9,
    "podcast_channel": "No Priors: AI, Machine Learning, Tech, & Startups",
    "duration_min": 36.0,
    "is_transcript": true
  },
  {
    "title": "The Future of Warfare: How the US Department of War Thinks About AI",
    "url": "https://www.youtube.com/watch?v=djv5UDHrg7g",
    "source": "Podcast: No Priors: AI, Machine Learning, Tech, & Startups",
    "date": "",
    "content": "The military buildup in China is the biggest military buildup in world history. And so there's a real urgency on our side to ensure that we are ahead, but we stay ahead. And that's going to take a different level of investment and different type of thinking than we've had in the last 20 years. In the 80s, there were 50 defense contractors and they got merged. So there are only about five. There's a lot of room for new entrance. It's crazy to me that SpaceX and Androl and Palanteer all had to sue the Department of War for their first contract. So the idea is you don't have to sue anymore. come through the front door because no one's people are not going to fight you. We're now excited about lower cost, faster, more sophisticated options. >> Hi listeners, welcome back to No Priors. We're here today with Emil Michael, the former chief business officer of Uber, White  House fellow, and currently CTO of the Department of War. >> Emil, thanks for joining us. Welcome to New Prior. >> Good to see you guys. It's been a long time. >> Congratulations on the new role. Um, very exciting news. Could you describe a little bit more about what that role is and what's changed at the Department of War to sort of create this new war this new um role this new momentum new initiatives that you all are focused on? >> Yeah. So um for a long time at the department of war there was one organization called uh acquisition technology and logistics and it was all bunched up into one thing and then about 8 years ago they said well tech is moving faster on new kinds of weaponry and defense systems than on the old system. So we're going to split out acquisition from research and engineering they call it. So I'm the under secretary for research and engineering which is cool right because I get to work on the stuff that I used to work on when I was in Silicon Valley like with work with entrepreneurs work with new companies. I'm now responsible for DARPA which is obviously super cool because it's most some of the most advanced research that happens in the country if not the world. uh in the last few months I took over as chief AI the chief AI office in the department of war which would 3 million employees the biggest organization with the biggest budget in the world you know is is not a small thing to to think about how to do AI right for and then the defense innovation unit which is actually based in Mountain View and it's supposedly the it's supposed to be the link between defense industry and uh you know the startup community that's building commercial products may have dual use. And then last is this strategic capability office which takes kind of existing um systems and tries to modify them in strategic ways or supplement them to get you know for strategic surprise they call it. So all that all has technology underneath it and the idea is to unify that across the department because we spend you know $150 billion a year on tech in one way or the other. So you want to avoid duplication, you want to bring things to market faster. That that was a big announcement yesterday by by the secretary at uh at Starbase. >> You mentioned DIU has been involved with providing early funding to a variety of promising new defense tech and hardware companies. Could you tell us a little bit more about what they've done and how you've approached that so far? >> Yeah, DIU's had some good success with Seronic and and uh and a bunch of other companies and now it's you know we have now it's time to do more of it. So, it's more of a focus and yeah, DARPA is just, you know, they invented the internet. You go to their gift shop and they have like a a napkin that where someone drew out the internet uh sort of architecture and they sell that in the gift shop. In case you guys come to DC, we can get you get you some of those. >> This is like a I mean, it's an amazing unification of like a bunch of stuff that's clearly important to America and to, you know, global security. uh it's a very different environment than a lot and I have seen you in uh in the past right you've worked with technology companies and you know even within them as a leader had this reputation for being you know high-speed aggressive dealmaker uh if if I'm allowed to say that um how does that work with the department of war which like amazing impact importance and scale not the strongest reputation in previous decades and administrations for speed. >> Yeah. Well, I I mean hopefully part of the reason I got chosen for the job and certainly the reason I took it is to inject some of that urgency speed, you know, being a little bit impervious to barriers and trying to run them over as opposed to being stopped by them. And it kind it works with leadership in in organizations is not is somewhat transferable, right? If you can show that kind of urgency and leadership and create a culture in a company, you could certainly make some progress in a bureaucracy or a government agency, it might not be as fast, but I also have way more resources, right? And I have a a way more, you know, in some way is important mission to humanity. So those two things are different in some ways sort of give me the same abilities to sort of drive fast and people respond. I mean, I'm getting a lot of people who want to join my team or other teams at DO um and across the government and yeah, it's harder, but uh in some ways it's more impactful. So, I'm just as motivated. >> Yeah, it's been interesting to watch all the entrepreneurs kind of gathering in DC in a way that, you know, I really haven't seen in a while. So, it's it's been impressive in terms of the types of talent that you all have been able to recruit. One thing that I think is related to moving fast is prioritization. And I think in another um interview in the past, you said something like 14 priorities mean no priorities at all. And you've kind of really honed down into a handful of key areas. Do you mind walking through sort of those key areas from an overall broader innovation focus? And then maybe we can touch on one or two of them as we go. >> Yeah, I mean so just to analogize to Uber or any company that you you know we worked on together, imagine you went to a management team and you're like what's your how many product you know how's your product line going? You're a series B company. and you're like, \"Well, I've got 14 product lines.\" You say, \"Well, wait a minute. Sort of uninvestable, right?\" So, um, when I got here, there were 14 critical technology areas. So, critical critical to our national security. So, I I looked at them. I said, \"Well, number one, that if if there's that many, they can't be critical.\" But number two, hard for people to hold in their head that many priorities and wake up every morning knowing that they have to get up and execute against those priorities and make progress because you get distracted. did you get too much uh disperate you know uh separation and therefore not enough resource to any one thing. So I I cut them down to six a after studying them and I made it more actionoriented. So we put sprints behind them like you would in an engineering team. So uh you know off the top of my head the the number one is applied AI because we're not building a foundation model at the DO because there's being hundreds of billions of dollars being spent by the private sector on this. So, how do I adapt or use what's being developed in the in the private sector and apply it to the Department of War use cases? So, that's number one. Um, number two is I don't know if you've heard of hypersonic missiles, but they're sort of the newest variation of um we used to have nuclear weapons and ballistic missiles. Now, these hypersonics, Elon talks about this all the time, that could go five times the speed of sound, the minimum Mach five. And so they pose somewhat of a unique threat and they're maneuverable and so they're you know it's the new sort of threat environment um weapon that's entered the picture. Few countries can build them the Chinese, Russia, US but my uh critical technology area is to do that at scale and at reasonable price because they're all very expensive now. They're exquisite. So how do you make them like in the way and thinks about them much more producible, much less exquisite, much cheaper, more mass? So I call it scaled hypersonics. Another one is scaled directed energy. So directed energy um is you know high powered microwave or lasers to take down missiles like iron beam if you think about what the Israelis did. And now with all the drone warfare you see, it's much cheaper to take out drones with an energy zap than it is to send a missile at another at a drone, right? A $5 million missile at a $50,000 drone or something. So directed energy again is is the technology exists, but how do I do that at scale? >> I I'm a little bit curious uh on the on the drone area in particular. One of the um things that I think a lot of people believe is that we're going to move from sort of big iron giant systems into more distributed fleets of autonomous drones or vehicles. And how do you think about that in the in the context of some of the uh programs or purchasing behavior of the Department of War? You know, we're still buying aircraft carriers. We're still buying very big sort of systems and there's all sorts of uses and needs for those. But what do you view as the shift to autonomy and the shift to drones that's going to happen over time? I mean, what what proportion of hardware do you think goes there? >> I think that starting with the Russia Ukraine war, you've got this sort of renaissance in drones or robots. Think of it as robots as the new front line, right? Say in a territorial battle, if you're fighting over land, you know, less costly in human life if you have robots fighting first and then before humans come in. Without the drone warfare in in that area of the world, you probably would have seen way more human casualties. It would it's",
    "summary": "Emil Michael, former chief business officer of Uber and current CTO of the Department of War, discusses the urgent need for the US to stay ahead in military technology, particularly in AI, and the changes being made to modernize the Department of War's approach to innovation and acquisition.",
    "entities": [
      "Emil Michael",
      "Uber",
      "White House",
      "Department of War",
      "SpaceX",
      "Androl",
      "Palanteer",
      "DARPA",
      "Defense Innovation Unit (DIU)",
      "Elon Musk",
      "China"
    ],
    "topics": [
      "Military technology",
      "AI in defense",
      "Acquisition technology and logistics",
      "Research and engineering",
      "Private sector collaboration",
      "Hypersonic missiles",
      "National security"
    ],
    "credibility_score": 9,
    "podcast_channel": "No Priors: AI, Machine Learning, Tech, & Startups",
    "duration_min": 44.0,
    "is_transcript": true
  },
  {
    "title": "Adam Marblestone – AI is missing something fundamental about the brain",
    "url": "https://www.youtube.com/watch?v=_9V_Hbe-N1A",
    "source": "Podcast: Dwarkesh Patel",
    "date": "",
    "content": "The big million-dollar question that I have, that I've been trying to get the answer to through all these interviews with AI researchers: How does the brain do it? We're throwing way more data at these LLMs and they still have a small fraction of the total capabilities that a human does. So what's going on? This might be the quadrillion-dollar question or something like that. You can make an argument that this is the most important question in science. I don't claim to know the answer. I also don't think that the answer will necessarily come even from a lot of smart people thinking about it as much as they are. My overall meta-level take is that we have to empower the field of neuroscience to just make neuroscience a more powerful field technologically and otherwise, to actually be able to crack a question like this. Maybe the way that we would think about this now with modern AI, neural nets, deep learning, is that there are certain key components of that. There's the architecture. There's maybe hyperparameters of how many layers you have or properties of that architecture. There is the learning algorithm itself. How do you train it? Backprop, gradient descent, is it something else? How is it initialized? If we take the learning part of the system, it still may have some initialization of the weights. And then there are also cost functions. What is it being trained to do? What's the reward signal? What are the loss functions, supervision signals? My personal hunch within that framework is that the field has neglected the role of these very specific loss functions, very specific cost functions. Machine learning tends to like mathematically simple loss functions. Predict the next token, cross-entropy, these simple computer scientist loss functions. I think evolution may have built a lot of complexity into the loss functions actually, many different loss functions for different areas turned on at different stages of development. A lot of Python code, basically, generating a specific curriculum for what different parts of the brain need to learn. Because evolution has seen many times what was successful and unsuccessful, and evolution could encode the knowledge of the learning curriculum. In the machine learning framework, maybe we can come back and we can talk about where do the loss functions of the brain come from? Can different loss functions lead to different efficiency of learning? People say the cortex has got the universal human learning algorithm, the special sauce that humans have. What's up with that? This is a huge question and we don't know. I've seen models where the cortex… The cortex typically has this six-layered structure, layers in a slightly different sense than layers of a neural net. Any one location in the cortex has six physical layers of tissue as you go in layers of the sheet. And those areas then connect to each other and that's more like the layers of a network. I've seen versions of that where what you're trying to explain is just, \"How does it approximate backprop?\" And what is the cost function for that? What is the network being asked to do, if you are trying to say it's something like backprop? Is it doing backprop on next token prediction or is it doing backprop on classifying images or what is it doing? And no one knows. But one thought about it, one possibility about it, is that it's just this incredibly general prediction engine. So any one area of the cortex is just trying to predict… Basically can it learn to predict any subset of all the variables it sees from any other subset? Omnidirectional inference, or omnidirectional prediction. Whereas an LLM is just seeing everything in the context window and then it computes a very particular conditional probability which is, \"Given all the last thousands of things, what are the probabilities for the next token.\" But it would be weird for a large language model to say \"the quick brown fox blank blank the lazy dog\" and fill in the middle versus doing the next token, if it's doing just forward. It can learn how to do that stuff at this emergent level of the context window and everything, but natively it's just predicting the next token. What if the cortex is natively made so that any area of cortex can predict any pattern in any subset of its inputs given any other missing subset? That is a little bit more like \"probabilistic AI\". A lot of the things I'm saying, by the way, are extremely similar to what Yann LeCun would say. He's really interested in these energy-based models and something like that is like, the joint distribution of all the variables. What is the likelihood or unlikelihood of just any combination of variables? If I clamp some of them and I say that definitely these variables are in these states, then I can compute, with probabilistic sampling for example—conditioned on these being set in this state, and these could be any arbitrary subset of variables in the model—can I predict what any other subset is going to do and sample from any other subset given clamping this subset? And I could choose a totally different subset and sample from that subset. So it's omnidirectional inference. And so there could be some parts of the cortex, there might be association areas of cortex that predict vision from audition. There might be areas that predict things that the more innate part of the brain is going to do. Because remember, this whole thing is riding on top of a lizard brain and lizard body, if you will. And that thing is a thing that's worth predicting too. You're not just predicting do I see this or do I see that. Is this muscle about to tense? Am I about to have a reflex where I laugh? Is my heart rate about to go up? Am I about to activate this instinctive behavior? Based on my higher-level understanding… Like I can match somebody has told me there's a spider on my back to this lizard part that would activate if I was literally seeing a spider in front of me. You learn to associate the two so that even just from somebody hearing you say \"There's a spider on your back\" Well, let's come back to this. This is partly having to do with Steve Byrnes’ theories, which I'm recently obsessed about. But on your podcast with Ilya, he said, \"Look, I'm not aware of any good theory of how evolution encodes high-level desires or intentions.\" I think this is very connected to all of these questions about the loss functions and the cost functions that the brain would use. And it's a really profound question, right? Let's say that I am embarrassed for saying the wrong thing on your podcast because I'm imagining that Yann LeCun is listening and he says, \"That's not my theory. You described energy-based models really badly.\" That's going to activate in me innate embarrassment and shame, and I'm going to want to go hide and whatever. That's going to activate these innate reflexes. That's important because I might otherwise get killed by Yann LeCun's marauding army of other… The French AI researchers are coming for you, Adam. So it's important that I have that instinctual response. But of course, evolution has never seen Yann LeCun or known about energy-based models or known what an important scientist or a podcast is. Somehow the brain has to encode this desire to not piss off really important people in the tribe or something like this in a very robust way, without knowing in advance all the things that the Learning Subsystem of the brain, the part that is learning cortex and other parts… The cortex is going to learn this world model. It's going to include things like Yann LeCun and podcasts. And evolution has to make sure that those neurons, whatever the Yann-LeCun-being-upset-with-me neurons, get properly wired up to the shame response or this part of the reward function. And this is important, right? Because if we're going to be able to seek status in the tribe or learn from knowledgeable people, as you said, or things like that, exchange knowledge and skills with friends but not with enemies… We have to learn all this stuff. It has to be able to robustly wire these learned features of the world, learned parts of the world model, up to these innate reward functions, and then actually use that to then learn more. Because next time I'm not going to try to piss off Yann LeCun if he emails me that I got this wrong. We're going to do further learning based on that. In constructing the reward function, it has to use learned information. But how can evolution, which didn't know about Yann LeCun, do that? The basic idea that Steve Byrnes is proposing is that part of the cortex, or other areas like the amygdala that learn, what they're doing is they're modeling the Steering Subsystem. The Steering Subsystem is the part with these more innately programmed responses and the innate programming of these series of reward functions, cost functions, bootstrapping functions that exist. There are parts of the amygdala, for example, that are able to monitor what those parts do and predict what those parts do. How do you find the neurons that are important for social status? Well, you have some innate heuristics of social status, for example, or you have some innate heuristics of friendliness that the Steering Subsystem can use. And the Steering Subsystem actually has its own sensory system, which is crazy. We think of vision as being something that the cortex does. But there's also a Steering Subsystem, subcortical visual system called the superior colliculus with innate ability to detect faces, for example, or threats. So there's a visual system that has innate heuristics and the Steering Subsystem has its own responses. There'll be part of the amygdala or part of the cortex that is learning to predict those responses. What are the neurons that matter in the cortex for social status or for friendship? They're the ones that predict those innate heuristics for friendship. You train a predictor in the cortex and you say, \"Which neurons are part of the predi",
    "summary": "Adam Marblestone discusses the fundamental differences between AI and the human brain, emphasizing the need to understand the brain's complex loss functions and cost functions. He suggests that the brain may use a more general prediction engine, capable of omnidirectional inference, unlike current AI models which focus on predicting the next token. Marblestone also touches on the role of evolution in encoding high-level desires and intentions.",
    "entities": [
      "Adam Marblestone",
      "Dwarkesh Patel",
      "Yann LeCun",
      "Ilya"
    ],
    "topics": [
      "AI",
      "neuroscience",
      "loss functions",
      "cost functions",
      "evolution",
      "human brain",
      "machine learning",
      "deep learning",
      "neural nets",
      "backprop",
      "gradient descent",
      "cortex",
      "energy-based models",
      "probabilistic AI"
    ],
    "credibility_score": 8,
    "podcast_channel": "Dwarkesh Patel",
    "duration_min": 109.0,
    "is_transcript": true
  },
  {
    "title": "What are we scaling?",
    "url": "https://www.youtube.com/watch?v=_zgnSbu5GqE",
    "source": "Podcast: Dwarkesh Patel",
    "date": "",
    "content": "I'm confused why some people have super short timelines yet at the same time are bullish on scaling up reinforcement learning a top LLMs. If we're actually close to a humanlike learner, then this whole approach of training on verifiable outcomes is doomed. Now,  currently the labs are trying to bake in a bunch of skills into these models through mid-training. There's an entire supply chain of companies that are building RL environments which teach the model how to navigate a web browser or use Excel to build financial models. Now either these models will soon learn on the job in a self-directed way which will make all this freebaking pointless or they won't which means that AGI is not imminent. Humans don't have to go through the special training phase where they need to rehearse every single piece of software that they might ever need to use on the job. Baron Millig made an interesting point about this in a recent blog post he wrote. He writes, quote, \"When we see frontier models improving at various benchmarks, we should think not just about the increased scale and the clever ML research ideas, but the billions of dollars that are paid to PhDs, MDs, and other experts to write questions and provide example answers and reasoning targeting these precise capabilities. You can see this tension most vividly in robotics. In some fundamental sense, robotics is an algorithms problem, not a hardware or a data problem. With very little training, a human can learn how to tell or operate current hardware to do useful work. So if you actually had a humanlike learner, robotics would be in large part a solved problem. But the fact that we don't have such a learner makes it necessary to go out into a thousand different homes and practice a million times on how to pick up dishes or fold laundry. Now, one counter argument I've heard from the people who think we're going to have a takeoff within the next 5 years is that we have to do all this cludgy RL in service of building a superhuman AI researcher. And then the million copies of this automated Ilia can go figure out how to solve robust and efficient learning from experience. This just gives me the vibes of that old joke, we're losing money on every sale, but we'll make it up in volume. Somehow, this automated researcher is going to figure out the algorithm for AGI, which is a problem that humans have been banging their head against for the better half of a century, while not having the basic learning capabilities that children have. I find it super implausible. Besides, even if that's what you believe, it doesn't describe how the labs are approaching reinforcement learning from verifiable reward. You don't need to pre-bake in a consultant skill at crafting PowerPoint slides in order to automate Ilia. So clearly, the lab's actions hint at a worldview where these models will continue to fare poorly at generalization and on the job learning, thus making it necessary to build in the skills that we hope will be economically useful beforehand into these models. Another counter argument you can make is that even if the model could learn these skills on the job, it is just so much more efficient to build in these skills once during trading rather than again for each user and each company. And look, it makes a ton of sense to just bake influency with common tools like browsers and terminals. And indeed, one of the key advantages that AGIS will have is this greater capacity to share knowledge across copies. But people are really underrating how much company and context specific skills are required to do most jobs. And there just isn't currently a robust efficient way for AIS to pick up these skills. I was recently at a dinner with a AI researcher and a biologist. And it turned out the biologist had long timelines. And so we were asking about why she had these long timelines. And then she said, you know, one part of work recently in the lab has involved looking at slides and deciding if the dot in that slide is actually a macroofage or just looks like a macroofage. And the AI researcher, as you might anticipate, responded, look, image classification is a textbook deep learning problem. This is death center in the kind of thing that we could train these models to do. And I thought this is a very interesting exchange because it illustrated a key crux between me and the people who expect transformative economic impact within the next few years. Human workers are valuable precisely because we don't need to build in the schley training bloops for every single small part of their job. It's not net productive to build a custom training pipeline to identify what macrofages look like given the specific way that this lab prepares slides and then another training loop for the next lab specific microtask and so on. What you actually need is an AI that can learn from semantic feedback or from self-directed experience and then generalize the way a human does. Every day you have to do a 100 things that require judgment, situational awareness, and skills and context that are learned on the job. These tasks differ not just across different people but even from one day to the next for the same person. It is not possible to automate even a single job by just baking in a predefined set of skills let alone all the jobs. In fact, I think people are really underestimating how big a deal actual AI will be because they are just imagining more of this current regime. They're not thinking about billions of humanlike intelligences on a server which can copy and merge all the learnings. And to be clear, I expect this, which is to say I expect actual brain-like intelligences within the next decade or two, which is pretty crazy. Sometimes people will say that the reason that AIs are more widely deployed right now across firms and already providing lots of value outside of coding is that technology takes a long time to diffuse. And I think this is cope. I think people are using this code to gloss over the fact that these models just lack the capabilities that are necessary for broad economic value. If these models actually were like humans on a server, they'd diffuse incredibly quickly. In fact, they'd be so much easier to integrate and onboard than a normal human employee is. They could read your entire Slack and drive within minutes. And they could immediately distill all the skills that your other AI employees have. Plus,  the hiring market for humans is very much like a lemons market where it's hard to tell who the good people are beforehand. And then obviously hiring somebody who turns out to be bad is very costly. This is just not a dynamic that you would have to face or worry about if you're just spinning up another instance of a vetted hi model. So for these reasons, I expect it's going to be much easier to diffuse AI labor into firms than it is to hire a person. And companies hire people all the time. If the capabilities were actually at AGI level, people would be willing to spend trillions of dollars a year buying tokens  that these models produce. Knowledge workers across the world cumulatively earn tens of trillions of dollars a year in wages. And the reason that labs are orders of magnitude off this figure right now is that the models are nowhere near as capable as human knowledge workers. Now you might be like look how can the standard have suddenly become labs have to earn tens of trillions of dollars of revenue a year right like until recently people were saying can these models reason do these models have common sense are they just doing pattern recognition and obviously AI bulls are right to criticize AI bears for repeatedly moving these goalpost and this is very often fair it's easy to underestimate the progress that AI has made over the last decade but some amount of goalpost shifting is actually justified if If you showed me Gemini 3 in 2020, I would have been certain that it could automate half of knowledge work. And so we keep solving what we thought were the sufficient bottlenecks to AGI. We have models that have general understanding. They have few shot learning. They have reasoning. And yet we still don't have AGI. So what is a rational response to observing this? I think it's totally reasonable to look at this and say, \"Oh, actually there's much more to intelligence and labor than I previously realized.\" And while we're really close and in many ways have surpassed what I would have previously defined as AGI in the past, the fact that model companies are not making the trillions of dollars in revenue that would be implied by AGI clearly reveals that my previous definition of AGI was too narrow. And I expect this to keep happening into the future. I expect that by 2030, the labs will have made significant progress on my hobby horse of continual learning and the models will be earning hundreds of billions of dollars in revenue a year, but they won't have automated all knowledge work. And I'll be like, look, we made a lot of progress, but we haven't hit AGI yet. We also need these other capabilities. We need X, Y, and Z capabilities in these models. Models keep getting more impressive at the rate that the short timelines people predict, but more useful at the rate that the long timelines people predict. It's worth asking what are we scaling with pre-trading? We had this extremely clean and general trend in improvement in loss across multiples orders of magnitude in compute. Albeit this was on a power law which is as weak as exponential growth is strong. But people are trying to launder the prestige that three training scaling has, which is almost as predictable as a physical law of the universe to justify bullish predictions about reinforcement learning from verifiable reward for which we have no welfare publicly known trend. And when intrepid researchers do try to piece together the implications from scarce public data points, they get pretty bearish results. For example, Toby Board has a gr",
    "summary": "The podcast discusses the current state and future of reinforcement learning and large language models (LLMs) in achieving artificial general intelligence (AGI). It questions the efficiency of baking in specific skills during training and argues that true AGI would require models to learn and generalize like humans, which is not yet achievable.",
    "entities": [
      "LLMs",
      "AGI",
      "Baron Millig"
    ],
    "topics": [
      "scaling up reinforcement learning",
      "training on verifiable outcomes",
      "humanlike learners",
      "baking in skills",
      "AI diffusion in firms",
      "economic value of AI"
    ],
    "credibility_score": 8,
    "podcast_channel": "Dwarkesh Patel",
    "duration_min": 12.0,
    "is_transcript": true
  },
  {
    "title": "Gnosis: Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits [Paper club 7 Jan]",
    "url": "https://www.youtube.com/watch?v=XLW5i_-wp5U",
    "source": "Podcast: Latent Space TV (see @LatentSpacePod for Pod)",
    "date": "",
    "content": "kickoff. So, pretty underrated paper here. Also, I don't know if it's underrated, it's just not very popular. Um, it's about kind of LM as a judge and predicting hallucinations. So, the background of picking this paper, uh, we didn't have a paper this week, so I was trying to see what's popular, what's doing well. Um, one of the ones that I was going over was this like aentic memory summary paper. It's very long. It's like a 100 pages. Maybe if we have some time today, we can go over it real quick. Um the problem with that one, I got through it and like they're trying to do a lot of definitions. How do we define what memory is? What are different memories? Throw like you know try to try to explain all these tools and stuff and what classifies as memory versus rag versus all this. Uh and then uh hallucination detection and stuff came in and this was just one of the top papers from hugging face. It's really not that popular. The GitHub repo has like six stars. But um I don't know. I read through it, it seemed kind of interesting. It seemed somewhat mechany. They're doing uh internal circuits. And then when I read through it, I was like, \"Okay, I don't think this is actually going to go anywhere.\" But it makes a lot of cool points. It seems fairly like applicable. like it's a pure applicable use case of mechan but it's it's cool background research a lot of interesting abilations and stuff so I thought you know why not let's let's go through it so basically can LLM predict their own failures uh selfawareness via internal circuits the key thing here being internal circuits right so they talk about a few different ways of having um LLM as a judge to detect hallucination and stuff and the issues with those and then their proposed solution. Then they solve one very niche problem with like a whole bunch of architecture stuff which is pretty deep research and it it works pretty pretty good. Yeah, they talk about hallucination probes as well. So there's there's like four things in here that we can talk over. There's uh intrinsic that they want or where where is it? This is what they wanted to go for. But then there's there's four types. So there's like textbased confidence and self-critique. So this is where you kind of look at token probability. So how confident are you in your next probable token? Um that's that's one approach that currently exists. They talk about the cons with it and some of the latest research. I guess if you're al also interested in like hallucination detection sort of stuff from a research perspective. They cite a lot of pretty good papers. So like apricot is a cool state-of-the-art method here. It works with distillation and stuff. But uh one of the con the cons of textbased confidence is when you look at your confidence score of your next probable token um you can't detect confident hallucinations right so stuff where you're very confident where your um hallucin like you know where you're confident but you're still wrong that doesn't get detected then the other state-of-the-art is this sort of distillation apricot work um you know in that you have out of context and you have high uh training cost so it's not asicable I wonder if this can be turned into an API. That's exactly what I was expecting here. Uh the the other thing to note with this approach is that you do need model weights. So you can't just do this on like a closed source model. But this like I was looking at this I was looking at these two authors. This guy's a professor. Uh this guy actually works at Huawei now. He's been at Huawei for two years, but he's doing his PhD at University of Alberta. I was like good fire should hire this guy. This is very much wrap it in an API serve it. It works very well. It's state-of-the-art. Uh it's on hallucination detection. So we're talking about the four four approaches. One of them is that textbased confidence. Second is um internal signal based indicators with linear probes. So someone shared a linear probe example of this. Problem with this is that they're um static in where they probe. Right? A big thing that they know and they do revel at relations about this is that you need to look at the whole trajectory. So uh oftent times hallucination can happen early in the model and we need to look at all hidden states. So all attention patterns. Um so that's a con there. Probes only look at final token states. Uh that doesn't work. And the other fun thing that they can do is that they can predict early on in generation. So um another high level before kind of answering this question is what they're doing is um they're training a little 5 million parameter like model. They're adding the not a model they're they're adding 5 million parameters with this little architecture into a model. They they test four models. They test a few small quens I think a small llama and then GPToss. Uh they train in a little detector. They they have a hidden  state encoder and an attention circuit and then they concatenate these and then they have that trained as a binary classifier on hallucination. And some fun stuff that they can do is without even seeing the full sequence uh they can predict whether it's hallucinating and that's an emerging capability. They don't explicitly train that. They just notice that even when only 40% of the sequence is there, we're already 90% confident that this is hallucinated. So there's fun stuff like that. The other thing that they're doing here that's novel is since they're probing in with a circuit on attention, they're they're basically taking every attention map at every layer and that would scale quadratically, but they want this to be very computationally small. So they have some fun encoders that encode all this stuff down to a consistent linear scaling and it's irrelevant of context size, but we'll go into all that architecture stuff later. But basically the question is can we probe our own model? Can we look at the model internals and predict whether something is hallucinating or infactual just by looking at the model itself? Uh can we keep it lightweight not add in prince burden and can this thing kind of scale? Uh turns out yes yes yes it can. Some other fun stuff they find towards the end is like this can work across model families. So you can train this on a coin 1B, apply it on a coin 20B and it still works. Uh, a lot of fun stuff there later. But honestly, a lot of this the meat of this paper is their architecture research. So as opposed to highle stuff, we might dive in a bit deeper there. And you know, of course, feel free to um pause wherever and we can we can kind of slow down and go back over stuff. It's it's not the easiest to grasp, but yeah, they they introduced this method called Genosis. Uh they have a open- source repo and everything about how it works, how to do it, how to apply it, and basically what you're doing is you're adding neg negligible inference cost. You're freezing the LLM backbone. You're adding 5 million parameters and you're basically independent of the sequence length. So whether you're operating on short sequences, long sequences, uh you're able to have a binary classifier about hallucination, they test this thing. It works better than LLM as a judge using like Gemini 2.5. It works better than probing, better than self-consistency. Self-consistency is basically where you do a bunch of samples like you generate 20 samples, you see if there's an aggregate match. It works better than all that stuff. And yeah, you can kind of just deploy this uh with low training cost. The training cost for this was like 25 bucks. Um they use two A100s for like 12 hours. So low training cost doesn't mess with anything as 5 million parameters can be done before the full sequence is even generated. And yeah, all this stuff kind of works pretty well. But uh being small university research, it'll probably never get picked up or slapped into an API even though you could really slap it into an API, right? Um it's it's just fun to see. And this is happening on model level outputs not on like agent harness checking agent steps. This is just model generation. Uh the other fun stuff it generalizes the zero shot stuff. Um yeah so okay intro is kind of some of the background of what happens. So you know LLM solucinate that's an issue. Um particularly in long horizon reasoning and we want compute aware control. So some of the issues with some other approaches are that they're very computationally expensive, right? Like when you run a second model as an LLM as a judge, if you're hosting this thing offline, well now you need to host a second model, right? Self-consistency. When you do multiple generations, uh you have to, you know, you have to do more calls and stuff. Uh someone's asking if on sloth teams and stuff will bolt these into open weight models. I think it would be cool, right? Like one, the fact that you can do this if you're deploying yourself. Two, this reminds me of stuff like Llama Guard and Open AI had a version of that for GPOSS safeguard or something like why not do a small bolt-on approach like this, right? Um, if people if people extend this, it would be pretty cool to see. And you don't even need to serve it as its own API, right? You're not changing the LLM background at all. The LLM backbone is the exact same. All you're really doing is returning an additional binary classifier with confidence of is this hallucinated or not and that's output at every single token step. So you know if you pass a threshold you can just cut it there and this works better than every other approach that they've tested so far and they do a lot of fabulations at this. So okay prior work um there's textbased self-critique and confidence estimates. So this is basically where you infer correctness from generated tokens or token probabilities. So this is more so tracking uh linguistic fluency rather than reasoning validity. Right? So as you're predicting the next token, you're just",
    "summary": "The podcast discusses a paper on Large Language Models (LLMs) predicting their own failures through self-awareness via internal circuits. It explores different methods for detecting hallucinations in LLMs and introduces a new method called Genosis, which adds a negligible inference cost and can predict hallucinations before the full sequence is generated.",
    "entities": [
      "LLMs",
      "Hugging Face",
      "GitHub",
      "GPToss",
      "Gemini 2.5",
      "A100"
    ],
    "topics": [
      "Large Language Models",
      "Self-awareness",
      "Internal circuits",
      "Hallucination detection",
      "Genosis"
    ],
    "credibility_score": 9,
    "podcast_channel": "Latent Space TV (see @LatentSpacePod for Pod)",
    "duration_min": 60.0,
    "is_transcript": true
  },
  {
    "title": "Microagent Part 1: AI in Action 9 Jan 2026",
    "url": "https://www.youtube.com/watch?v=uXkBQzMuKBI",
    "source": "Podcast: Latent Space TV (see @LatentSpacePod for Pod)",
    "date": "",
    "content": "We'll give people a couple minutes, but also like I'm not terribly impressed if it's a smaller group today. >> Happy New Year, y'all. Did you end up hanging out on the second? I was out, but >> yeah, there was a couple of us. >> Nice. >> In fact, I was actually kind of like instigating um some IRL events in New York. I think I mentioned it to Scott, too. Um, yeah. It's kind of like what I used to do as a New Haven Ruby person is I would always kind of do like a DIY energy and try to like see if I could get people to be like, \"Hey, do you want to give a talk?\" Like a lightning talk, five minutes or just be like, \"Hey, like do you want to help me put together a pub night?\" And then just try and get some of that organizing energy distributed. So it wasn't like, you know, bus test problem where one person's doing all the organizing. So I've got like 50 something slides kind of like last time. Um I might just start getting into it and then if people filter in, they filter in. But honestly, like just putting this together was really helpful for me to kind of get my thoughts together about everything that I've been putting effort towards and and putting it together in like a explainable way. So, um yeah, I might just start hitting it. Um couple weeks ago, I did the Claudius Coach one where I was kind of prototyping in Claude projects this workflow that I've been doing for a few months now. Um, I'm like logging everything every day. The example that I've been using is like, you know, Rob the runner is the synthetic data of like someone's preparing for a 5K. So the idea is that you're logging information daily and then you're doing these summaries and then you do like a retrospective to kind of do a summary of summaries and that's how you're able to kind of keep context tight um while also getting like rich context that you can see uh like the memory system that Claude is using like Enthropic built a memory system into Claude AI web and it's inscrutable. There's like a background job they do once a night and the results that it gives are not amazing and there's like no auditability. Like you can edit certain memory slots, but then there's also like a giant text blob that's generated. So it's it's kind of messy, but I like having this memory system and it's been working and it continues to work without any additional build. So I was like, well, I've got this platform lockin problem. like, okay, I didn't need to build anything to prototype it and I got high quality inferencing and all that, but there's still a lot of these manual steps. I can't easily share it. Like, people can download the skill zip files and try to import them, but there's like certain parts of the workflow that with a little bit more development could actually become a little bit more helpful and remind people of stuff like a very thin layer built on top. So I started with um you know this prototype and then I was like well it's got these issues maybe I can build the smallest possible agent and what ended up happening is I've got about 1500 lines of Python and then the agent driven side of this uh or sorry the eval driven side of this is about 800 lines. Um, now it doesn't include like fixture data. That doesn't include the specifics of how like the data itself inside of there like the skill file under test like all of that isn't included in that. This is just simply like the orchestration uh sort of like the test helper RB in a way but also kind of like the actual like foo test RB although this is Python. So I there's going to be some Ruby in the mix uh later on. And I see Scott smiling. Um, so we started with cloud projects and I'm like, let's get to a local agent. So I prototyped, you know, this idea of having like easily editable, not easily editable, but like the user can manage which files are considered in context. Like I have a friend who prefers to code in web chat. So if he's working on a specific problem, he'll take the appropriate files and drop them into like chatgbt or claw.ai. And that way he knows exactly what's in context. Very manual, but like I kind of get it because that's kind of what I'm doing here where I'm treating these markdown files as you know data but also instructions because there's a bit of multi-shot learning going on as there's this accumulation of summaries. So again, Cloud Projects has like these different primitives. The workflow like you have these artifacts and then you have to save it to project and then they have their own built-in memory which I don't particularly like for my use case. Um there's also like sort of system level instructions that the user can edit. I don't know exactly how all that's packed into context exactly, but I know I was like this is this is a lot. Um this isn't even a lot. This is actually relatively simple, but I can do smaller. Um, so I figured like let's start with a CLI. We're going to build out whatever Python is necessary and give it just enough skill, just enough tool, and then I can manage tokens and then I can kind of build out a version where I own the stack because it'd be nice for me to actually use this as my day-to-day rather than relying on Claude and Anthropic. Like there's sort of like a home lab energy to it. Um, so again, like skills are basically just markdown instructions that are packed into a zip file and pulled into context as necessary when the description comes into the agent's attention. So there's sort of like a um what was the word for word? Progressive disclosure where it only knows as much as it needs to because otherwise it's just going to blow out your context window especially if have a lot of skills or a lot of instructions. So that strategy is interesting and I'm going to continue exploring it. But I was like, what's the smallest? Let's just get skills in place and let's continue with this protocol of like summary of summaries where like you know the human in the loop is good for like you're working on goals. Like you don't want to like delegate your thinking during retro to the agent, right? Like if you're a team and you're like doing a retrospective at the end of a sprint, you need to be involved in the process. You can't just sort of like p it all off on the agent because you have to be the one that changes. Like so there there's something useful about it, but the manual elements of like I have to add to project, I have to do this like manual daily summary thing. I might not even necessarily have to do as much of the retro. I might be able to kind of have it be a little bit more like agent driven where I don't have to be the one to say, \"All right, it's Sunday. I have to do retro now.\" The agent could be like, \"Hey, do you want to do retro? Like let's get through this. Like I can help you through it.\" and to sort of lower the barrier so that someone could do that process without having to expend as much cognition but still be the one making the decisions. Um, so again, I've been using this for a few months now. Um, I like that it was zero infrastructure. I like that I didn't have to really like build anything for the prototype. I like that it was cross device. Like I could just go to my laptop. I could go to my phone. I could use the native app somewhat. The web app was pretty reliable. Um and then this like this idea of like summary of summaries as the real like sort of keystone of it. Um and it was also nice to get you know all the inferencing like I actually it was kind of a pain but I did an export and I tried to analyze my chats and I was like guessing at the system prompt sizes and like what my token burn rate was. If I had been hitting an API for this project, it would have been like $1,000 for a few months. I've been I've been using it very extensively. Um, so that was nice, but then I was like, well, how much of this can I own? So, let's try and build it, right? Like there's I I can't easily do analysis. When I tried to do analysis, it was hard. They don't even actually give you the join table. They just give you all of the conversations regardless of what project they're in. Then they gave you a project like list of uh entries but there's no join table to know like this this conversation is associated with this project and I was like really guys um again there's the automation issues skills aren't isolated in cloud projects and I was like well I can't test skills against each other easily. It means I have to kind of like test in prod which is you know I can I'd like to not do that but I'm willing to be you know a little adventurous here at times but at a certain point it'd be nice to actually identify does this skill work better than that skill for the same thing. Um and also like I can't meaningfully evaluate any of it like there's no way to create a scenario that I can actually like test. So when I was doing the demonstration a couple weeks ago and I was putting together all of these like synthetic data situations and I was trying to demonstrate it in the web UI, it was very manual, very clunky. I'm not trying to write py write to hit against my claw.ai count. Like I was like that's I can do better. Um okay so minimal agent I was like what is the least amount of agent I can possibly use? And I was thinking about Carpathy's nano chat, right? Like what is, you know, for a hundred bucks worth of GPU time, you can build a full stack chatbot. Not great, not amazing, but like you can have a conversation with it and it like kind of follows. So what's the smallest thing? So I was like, well, I need get current date just because like dates are so critical to this daily logging concept and the idea of like time frames. Um, and then access to files, list the files, read a file, write a file, and then find a skill. And that's kind of like the most essential tool calls that I needed. But that did mean that the model had to support tool calls, and we'll get into that. Um, having the files accessible, right? So, I just put a sandbox directory and the",
    "summary": "The podcast discusses the use of AI in organizing events and managing workflows, with a focus on the speaker's experiences with Claude AI and their attempts to prototype a workflow for logging and summarizing daily activities. The speaker also explores the idea of building a local agent to manage tasks and reduce reliance on external platforms.",
    "entities": [
      "Latent Space TV",
      "Scott",
      "New Haven Ruby",
      "Claudius Coach",
      "Claude AI",
      "Entropic",
      "Anthropic"
    ],
    "topics": [
      "AI in event organization",
      "workflow management",
      "prototyping",
      "local agent development",
      "cross-device usage",
      "summary of summaries",
      "API inferencing"
    ],
    "credibility_score": 9,
    "podcast_channel": "Latent Space TV (see @LatentSpacePod for Pod)",
    "duration_min": 12.0,
    "is_transcript": true
  },
  {
    "title": "The non-technical PM's guide to building with Cursor | Zevi Arnovitz (Meta)",
    "url": "https://www.youtube.com/watch?v=1em64iUFt3U",
    "source": "Podcast: Lenny's Podcast",
    "date": "",
    "content": "You are a product manager shipping product without knowing how to write code, barely knowing how to review code. I have zero technical background, did music in high school ... when Sonnet 3.5 came out. I remember watching a YouTube video building apps using Bolt or Lovable. It basically felt like someone came up to me and said, \"You have superpowers now.\" These days, you're using Cursor with Claude Code. If you're non-technical like me, code is terrifying, but AI just makes it so much possible. In the next coming years, I think everyone's going to become a builder. Titles are going to collapse and responsibilities are going to collapse. The main challenge people have is reviewing the code that AI has written. It's very difficult for me to catch mistakes. What I'll do is basically /review. This tells Claude to start reviewing its own code, but what's even cooler is I have Codex as well as Cursor open. I will have each of them review the code. This comes back to this quote. I think everyone's always hearing. It's not that you will be replaced by AI. You'll be replaced by someone who's better at using AI than you. It's the best time to be a junior, contrary to what a lot of people are saying, how there's no more junior roles out there. Yeah, that's true, but also when else in history could you get out of school and just build a startup on your own? Today, my guest is Zevi Arnovitz. Zevi is a PM at Meta. Prior to that, he was a PM at Wix, and this is a truly remarkable conversation that every non-technical product person needs to hear. Zevi is super young and has no technical background, but as a smart, young, ambitious person, has learned how to use Cursor and Claude Code to build significant and real products completely on his own, and he's created his own very clever and effective workflow that everyone listening can copy. To make that copying even easier, at the top of the show notes of this episode, you can download all of the prompts and /commands and start doing all of this yourself. Zevi shows you how to work with cursor to quickly add your ideas to Linear to explore your idea with AI, how to develop your plan, how to then build the thing, and then have different LLMs review your code and update your documentation, and then use all of this as a learning opportunity to develop your own sense of how things work. I haven't stopped thinking about this conversation since we had it, and everyone needs to pay attention to what AI is unlocking for non-technical people. A huge thank you to Tal Raviv for encouraging me to meet Zevi. If you enjoy this podcast, don't forget to subscribe and follow it in your favorite podcasting app or YouTube. It helps tremendously. And if you become an annual subscriber of my newsletter, you get 19 premium products for free for an entire year including Lovable, Replit, Bolt, Gamma, n8n, Linear, Devin, PostHog, Superhuman, Descript, Wispr Flow, Perplexity, Warp, Granola, Magic Patterns, Raycast, ChatPRD, Mobbin, and Stripe Atlas. Head on over to lennysnewsletter.com and click product pass. With that, I bring you Zevi Arnovitz after a short word from our sponsors. This episode is brought to you by 10Web, the company that pioneered AI website building before ChatGPT. In the last three years, over two million websites have been generated with 10Web's vibe coding platform. 10Web's vibe coding platform is a powerful way to build websites. Think of it as lovable for WordPress, front end and backend. Users can build any website at any complexity, e-commerce, portfolios, information websites, blogs, and it comes with the WordPress admin panel and thousands of ready to use plugins. 10Web also offers website generation as an API as a service for SaaS companies, marketplaces, hosting providers, MSPs and agencies. SaaS companies can embed it via API so that users can launch AI generated sites directly inside of their platform, connected to their own data. Agencies and MSPs can get a white label dashboard to manage clients and resell under their brand. Hosting providers can self-host the API builder on their own infrastructure. Check it out at 10web.io/lenny and use code Lenny for exclusive free credits and 30% off API or white labeled solutions. That's the number 10web.io/lenny, vibe coding platform as an API. Today's episode is brought to you by DX, the developer intelligence platform designed by leading researchers. To thrive in the AI era, organizations need to adapt quickly, but many organization leaders struggle to answer pressing questions like, which tools are working? How are they being used? What's actually driving value? DX provides the data and insights that leaders need to navigate this shift. With DX, companies like Dropbox, Booking.com, Adyen, and Intercom get a deep understanding of how AI is providing value to their developers and what impact AI is having on engineering productivity. To learn more, visit DX's website at getdx.com/lenny, that's getdx.com/lenny. Zevi, thank you so much for being here and welcome to the podcast. Thanks for having me, Lenny. I'm a huge fan of the show and tons of people that I've admired most and learned the most from. I've been on here, so it's a crazy moment for me. I'm really excited for this. I really appreciate that. I want to start by reading actually a note I got about you from Tal Raviv, who is a previous podcast guest, many times newsletter collaborator. One of the most AI forward product managers that I know I've learned a ton from him. So here's what he said about you when he introduced us. Zevi is the most hands-on vibe coding PM I know, and I've personally learned so much from him. His engineers at Meta ask him to teach them how to do what he does. Every time we get coffee, I repeatedly get this feeling of everyone needs to be hearing this. That's so nice. And so that's the goal. That's the goal of this conversation is to help more people hear what you figured out. We're going to get very hands-on. We're going to do a lot of show versus tell, showing people what you've figured out about how to be a PM, a non-technical PM building stuff. I want to give people a little bit of background on you because I think this is going to inspire a lot of listeners to feel like they can also do what we're about to show you. This is going to look very advanced, but just give people a little bit of sense of just your background. I'm very non-technical. I have zero technical background. Did music in high school. A lot of Israelis do technology units in the Army. I was not in a tech unit. And basically a year ago, I was traveling with my wife for three months in Asia and we were in Japan and that was around when Sonnet 3.5 came out. And I remember watching a YouTube video. I think it was either Greg Isenberg or Riley Brown and they were basically building apps using, it was either Bolt or Lovable, just using AI. And it was like a crazy moment for me because I was watching this and it basically felt like someone came up to me and said, \"Hey Zevi, there's this cool new technology you should check out. You should really give it a try. Oh, and by the way, you have superpowers now.\" And the second I got home from Japan, I didn't even unpack my bags, ran to my computer, opened Bolt, opened an account, and for the past year I've been building. And the last thing I'll say on that is we talked about this a bit before we started recording, but I was prepping with Claude for the episode and I was trying to clarify what my goal is for this episode. And Claude said, \"If people walk away thinking how amazing you are, you failed. And if people walk away and open their computer and start building, you've succeeded.\" So I really hope that we can inspire some people to do the same. I love that so much. I feel like that should be the goal for my podcast. If you're like, \"I love that guest.\" It's less of a win. If it's just like, \"Oh, I'm so inspired to do the thing that they figured out, that is the real win.\" I love, Claude is the best. I agree. Okay. So let's dive in and give people, let's start with kind of a high level overview of how you operate and you use AI in your job. What are the core tools and just what's kind of like the frame of reference for the workflow that you figured out and how you operate? This all started where I was a project's power user. I love projects, GPT projects. ChatGPT projects? Yeah, exactly. GPT projects and Claude projects, which are basically a shared folder of chats which share both custom instructions and shared knowledge base. And I think it was around when GPT started using memory where I thought it was interesting, but it really annoyed me because I do a bunch of different things. I'm a terrible runner, I'm a PM, I was a student, psychology student, so I had all these different facets of life. And what happened was the memory feature was mixing stuff up. So like I talked to GPT about running and it would say, \"Oh yeah, after this 5K, you're going to crush all your next product reviews.\" And it's like, okay, I understand that you have that in your memory, but it's just not relevant. And projects basically allows you to compartmentalize and have things within the right context. So tracking back to the story I told when we came back from Japan, I started building this app. The first thing I noticed was that these products were built in a way where, and when I say these products, I mean Bolt and Lovable, were built in a way where they were super eager to write code. So their system prompt was you're a coding agent. So when you'd write something, they'd straight away start coding. So at the beginning of a project, this was super fun and exciting because they just go and start building your app. But later on when things got more complex, this created much more problems because planning is really important when you're implementing something technical and let's say you're implementing payments or something that's going to be a change to your ",
    "summary": "Zevi Arnovitz, a non-technical PM at Meta, discusses his journey of leveraging AI tools like Cursor and Claude Code to build products independently. He emphasizes the potential for AI to democratize software development, allowing non-technical individuals to become builders and create startups on their own.",
    "entities": [
      "Meta",
      "Wix",
      "Cursor",
      "Claude Code",
      "Codex",
      "Sonnet 3.5",
      "Bolt",
      "Lovable",
      "Tal Raviv",
      "10Web",
      "DX"
    ],
    "topics": [
      "Non-technical product management",
      "AI in software development",
      "Building products with AI",
      "Reviewing AI-generated code",
      "Democratization of software development"
    ],
    "credibility_score": 8,
    "podcast_channel": "Lenny's Podcast",
    "duration_min": 75.0,
    "is_transcript": true
  }
]