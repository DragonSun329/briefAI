{
  "timestamp": "2026-01-13T13:41:58.245134",
  "key": "source_arxiv_ai_7days",
  "value": [
    {
      "title": "Naiad: Novel Agentic Intelligent Autonomous System for Inland Water Monitoring",
      "url": "https://arxiv.org/abs/2601.05256",
      "content": "arXiv:2601.05256v1 Announce Type: new \nAbstract: Inland water monitoring is vital for safeguarding public health and ecosystems, enabling timely interventions to mitigate risks. Existing methods often address isolated sub-problems such as cyanobacteria, chlorophyll, or other quality indicators separately. NAIAD introduces an agentic AI assistant that leverages Large Language Models (LLMs) and external analytical tools to deliver a holistic solution for inland water monitoring using Earth Observation (EO) data. Designed for both experts and non-experts, NAIAD provides a single-prompt interface that translates natural-language queries into actionable insights. Through Retrieval-Augmented Generation (RAG), LLM reasoning, external tool orchestration, computational graph execution, and agentic reflection, it retrieves and synthesizes knowledge from curated sources to produce tailored reports. The system integrates diverse tools for weather data, Sentinel-2 imagery, remote-sensing index computation (e.g., NDCI), chlorophyll-a estimation, and established platforms such as CyFi. Performance is evaluated using correctness and relevancy metrics, achieving over 77% and 85% respectively on a dedicated benchmark covering multiple user-expertise levels. Preliminary results show strong adaptability and robustness across query types. An ablation study on LLM backbones further highlights Gemma 3 (27B) and Qwen 2.5 (14B) as offering the best balance between computational efficiency and reasoning performance.",
      "published_date": "2026-01-13T05:00:00",
      "source": "ArXiv AI (Test Source)",
      "source_id": "arxiv_ai",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 6,
      "focus_tags": [
        "AI",
        "机器学习",
        "研究论文",
        "算法"
      ]
    },
    {
      "title": "Mathematical Knowledge Graph-Driven Framework for Equation-Based Predictive and Reliable Additive Manufacturing",
      "url": "https://arxiv.org/abs/2601.05298",
      "content": "arXiv:2601.05298v1 Announce Type: new \nAbstract: Additive manufacturing (AM) relies critically on understanding and extrapolating process-property relationships; however, existing data-driven approaches remain limited by fragmented knowledge representations and unreliable extrapolation under sparse data conditions. In this study, we propose an ontology-guided, equation-centric framework that tightly integrates large language models (LLMs) with an additive manufacturing mathematical knowledge graph (AM-MKG) to enable reliable knowledge extraction and principled extrapolative modeling. By explicitly encoding equations, variables, assumptions, and their semantic relationships within a formal ontology, unstructured literature is transformed into machine-interpretable representations that support structured querying and reasoning. LLM-based equation generation is further conditioned on MKG-derived subgraphs, enforcing physically meaningful functional forms and mitigating non-physical or unstable extrapolation trends. To assess reliability beyond conventional predictive uncertainty, a confidence-aware extrapolation assessment is introduced, integrating extrapolation distance, statistical stability, and knowledge-graph-based physical consistency into a unified confidence score. Results demonstrate that ontology-guided extraction significantly improves the structural coherence and quantitative reliability of extracted knowledge, while subgraph-conditioned equation generation yields stable and physically consistent extrapolations compared to unguided LLM outputs. Overall, this work establishes a unified pipeline for ontology-driven knowledge representation, equation-centered reasoning, and confidence-based extrapolation assessment, highlighting the potential of knowledge-graph-augmented LLMs as reliable tools for extrapolative modeling in additive manufacturing.",
      "published_date": "2026-01-13T05:00:00",
      "source": "ArXiv AI (Test Source)",
      "source_id": "arxiv_ai",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 6,
      "focus_tags": [
        "AI",
        "机器学习",
        "研究论文",
        "算法"
      ]
    },
    {
      "title": "Effects of personality steering on cooperative behavior in Large Language Model agents",
      "url": "https://arxiv.org/abs/2601.05302",
      "content": "arXiv:2601.05302v1 Announce Type: new \nAbstract: Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear. In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games. Based on the Big Five framework, we first measure basic personality profiles of three models, GPT-3.5-turbo, GPT-4o, and GPT-5, using the Big Five Inventory. We then compare behavior under baseline and personality-informed conditions, and further analyze the effects of independently manipulating each personality dimension to extreme values. Our results show that agreeableness is the dominant factor promoting cooperation across all models, while other personality traits have limited impact. Explicit personality information increases cooperation but can also raise vulnerability to exploitation, particularly in earlier-generation models. In contrast, later-generation models exhibit more selective cooperation. These findings indicate that personality steering acts as a behavioral bias rather than a deterministic control mechanism.",
      "published_date": "2026-01-13T05:00:00",
      "source": "ArXiv AI (Test Source)",
      "source_id": "arxiv_ai",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 6,
      "focus_tags": [
        "AI",
        "机器学习",
        "研究论文",
        "算法"
      ]
    },
    {
      "title": "Improving Enzyme Prediction with Chemical Reaction Equations by Hypergraph-Enhanced Knowledge Graph Embeddings",
      "url": "https://arxiv.org/abs/2601.05330",
      "content": "arXiv:2601.05330v1 Announce Type: new \nAbstract: Predicting enzyme-substrate interactions has long been a fundamental problem in biochemistry and metabolic engineering. While existing methods could leverage databases of expert-curated enzyme-substrate pairs for models to learn from known pair interactions, the databases are often sparse, i.e., there are only limited and incomplete examples of such pairs, and also labor-intensive to maintain. This lack of sufficient training data significantly hinders the ability of traditional enzyme prediction models to generalize to unseen interactions. In this work, we try to exploit chemical reaction equations from domain-specific databases, given their easier accessibility and denser, more abundant data. However, interactions of multiple compounds, e.g., educts and products, with the same enzymes create complex relational data patterns that traditional models cannot easily capture. To tackle that, we represent chemical reaction equations as triples of (educt, enzyme, product) within a knowledge graph, such that we can take advantage of knowledge graph embedding (KGE) to infer missing enzyme-substrate pairs for graph completion. Particularly, in order to capture intricate relationships among compounds, we propose our knowledge-enhanced hypergraph model for enzyme prediction, i.e., Hyper-Enz, which integrates a hypergraph transformer with a KGE model to learn representations of the hyper-edges that involve multiple educts and products. Also, a multi-expert paradigm is introduced to guide the learning of enzyme-substrate interactions with both the proposed model and chemical reaction equations. Experimental results show a significant improvement, with up to a 88% relative improvement in average enzyme retrieval accuracy and 30% improvement in pair-level prediction compared to traditional models, demonstrating the effectiveness of our approach.",
      "published_date": "2026-01-13T05:00:00",
      "source": "ArXiv AI (Test Source)",
      "source_id": "arxiv_ai",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 6,
      "focus_tags": [
        "AI",
        "机器学习",
        "研究论文",
        "算法"
      ]
    },
    {
      "title": "The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models",
      "url": "https://arxiv.org/abs/2601.05376",
      "content": "arXiv:2601.05376v1 Announce Type: new \nAbstract: Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner. However, its effects on high-stakes clinical decision-making remain poorly characterized. We systematically evaluate persona-based control in clinical LLMs, examining how professional roles (e.g., Emergency Department physician, nurse) and interaction styles (bold vs.\\ cautious) influence behavior across models and medical tasks. We assess performance on clinical triage and patient-safety tasks using multidimensional evaluations that capture task accuracy, calibration, and safety-relevant risk behavior. We find systematic, context-dependent, and non-monotonic effects: Medical personas improve performance in critical care tasks, yielding gains of up to $\\sim+20\\%$ in accuracy and calibration, but degrade performance in primary-care settings by comparable margins. Interaction style modulates risk propensity and sensitivity, but it's highly model-dependent. While aggregated LLM-judge rankings favor medical over non-medical personas in safety-critical cases, we found that human clinicians show moderate agreement on safety compliance (average Cohen's $\\kappa = 0.43$) but indicate a low confidence in 95.9\\% of their responses on reasoning quality. Our work shows that personas function as behavioral priors that introduce context-dependent trade-offs rather than guarantees of safety or expertise. The code is available at https://github.com/rsinghlab/Persona\\_Paradox.",
      "published_date": "2026-01-13T05:00:00",
      "source": "ArXiv AI (Test Source)",
      "source_id": "arxiv_ai",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 6,
      "focus_tags": [
        "AI",
        "机器学习",
        "研究论文",
        "算法"
      ]
    },
    {
      "title": "Conformity and Social Impact on AI Agents",
      "url": "https://arxiv.org/abs/2601.05384",
      "content": "arXiv:2601.05384v1 Announce Type: new \nAbstract: As AI agents increasingly operate in multi-agent environments, understanding their collective behavior becomes critical for predicting the dynamics of artificial societies. This study examines conformity, the tendency to align with group opinions under social pressure, in large multimodal language models functioning as AI agents. By adapting classic visual experiments from social psychology, we investigate how AI agents respond to group influence as social actors. Our experiments reveal that AI agents exhibit a systematic conformity bias, aligned with Social Impact Theory, showing sensitivity to group size, unanimity, task difficulty, and source characteristics. Critically, AI agents achieving near-perfect performance in isolation become highly susceptible to manipulation through social influence. This vulnerability persists across model scales: while larger models show reduced conformity on simple tasks due to improved capabilities, they remain vulnerable when operating at their competence boundary. These findings reveal fundamental security vulnerabilities in AI agent decision-making that could enable malicious manipulation, misinformation campaigns, and bias propagation in multi-agent systems, highlighting the urgent need for safeguards in collective AI deployments.",
      "published_date": "2026-01-13T05:00:00",
      "source": "ArXiv AI (Test Source)",
      "source_id": "arxiv_ai",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 6,
      "focus_tags": [
        "AI",
        "机器学习",
        "研究论文",
        "算法"
      ]
    },
    {
      "title": "On the Effect of Cheating in Chess",
      "url": "https://arxiv.org/abs/2601.05386",
      "content": "arXiv:2601.05386v1 Announce Type: new \nAbstract: Cheating in chess, by using advice from powerful software, has become a major problem, reaching the highest levels. As opposed to the large majority of previous work, which concerned {\\em detection} of cheating, here we try to evaluate the possible gain in performance, obtained by cheating a limited number of times during a game. Algorithms are developed and tested on a commonly used chess engine (i.e software).\\footnote{Needless to say, the goal of this work is not to assist cheaters, but to measure the effectiveness of cheating -- which is crucial as part of the effort to contain and detect it.}",
      "published_date": "2026-01-13T05:00:00",
      "source": "ArXiv AI (Test Source)",
      "source_id": "arxiv_ai",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 6,
      "focus_tags": [
        "AI",
        "机器学习",
        "研究论文",
        "算法"
      ]
    },
    {
      "title": "ART: Adaptive Reasoning Trees for Explainable Claim Verification",
      "url": "https://arxiv.org/abs/2601.05455",
      "content": "arXiv:2601.05455v1 Announce Type: new \nAbstract: Large Language Models (LLMs) are powerful candidates for complex decision-making, leveraging vast encoded knowledge and remarkable zero-shot abilities. However, their adoption in high-stakes environments is hindered by their opacity; their outputs lack faithful explanations and cannot be effectively contested to correct errors, undermining trustworthiness. In this paper, we propose ART (Adaptive Reasoning Trees), a hierarchical method for claim verification. The process begins with a root claim, which branches into supporting and attacking child arguments. An argument's strength is determined bottom-up via a pairwise tournament of its children, adjudicated by a judge LLM, allowing a final, transparent and contestable verdict to be systematically derived which is missing in methods like Chain-of-Thought (CoT). We empirically validate ART on multiple datasets, analyzing different argument generators and comparison strategies. Our findings show that ART's structured reasoning outperforms strong baselines, establishing a new benchmark for explainable claim verification which is more reliable and ensures clarity in the overall decision making step.",
      "published_date": "2026-01-13T05:00:00",
      "source": "ArXiv AI (Test Source)",
      "source_id": "arxiv_ai",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 6,
      "focus_tags": [
        "AI",
        "机器学习",
        "研究论文",
        "算法"
      ]
    },
    {
      "title": "PRISMA: Reinforcement Learning Guided Two-Stage Policy Optimization in Multi-Agent Architecture for Open-Domain Multi-Hop Question Answering",
      "url": "https://arxiv.org/abs/2601.05465",
      "content": "arXiv:2601.05465v1 Announce Type: new \nAbstract: Answering real-world open-domain multi-hop questions over massive corpora is a critical challenge in Retrieval-Augmented Generation (RAG) systems. Recent research employs reinforcement learning (RL) to end-to-end optimize the retrieval-augmented reasoning process, directly enhancing its capacity to resolve complex queries. However, reliable deployment is hindered by two obstacles. 1) Retrieval Collapse: iterative retrieval over large corpora fails to locate intermediate evidence containing bridge answers without reasoning-guided planning, causing downstream reasoning to collapse. 2) Learning Instability: end-to-end trajectory training suffers from weak credit assignment across reasoning chains and poor error localization across modules, causing overfitting to benchmark-specific heuristics that limit transferability and stability. To address these problems, we propose PRISMA, a decoupled RL-guided framework featuring a Plan-Retrieve-Inspect-Solve-Memoize architecture. PRISMA's strength lies in reasoning-guided collaboration: the Inspector provides reasoning-based feedback to refine the Planner's decomposition and fine-grained retrieval, while enforcing evidence-grounded reasoning in the Solver. We optimize individual agent capabilities via Two-Stage Group Relative Policy Optimization (GRPO). Stage I calibrates the Planner and Solver as specialized experts in planning and reasoning, while Stage II utilizes Observation-Aware Residual Policy Optimization (OARPO) to enhance the Inspector's ability to verify context and trigger targeted recovery. Experiments show that PRISMA achieves state-of-the-art performance on ten benchmarks and can be deployed efficiently in real-world scenarios.",
      "published_date": "2026-01-13T05:00:00",
      "source": "ArXiv AI (Test Source)",
      "source_id": "arxiv_ai",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 6,
      "focus_tags": [
        "AI",
        "机器学习",
        "研究论文",
        "算法"
      ]
    },
    {
      "title": "MMUEChange: A Generalized LLM Agent Framework for Intelligent Multi-Modal Urban Environment Change Analysis",
      "url": "https://arxiv.org/abs/2601.05483",
      "content": "arXiv:2601.05483v1 Announce Type: new \nAbstract: Understanding urban environment change is essential for sustainable development. However, current approaches, particularly remote sensing change detection, often rely on rigid, single-modal analysis. To overcome these limitations, we propose MMUEChange, a multi-modal agent framework that flexibly integrates heterogeneous urban data via a modular toolkit and a core module, Modality Controller for cross- and intra-modal alignment, enabling robust analysis of complex urban change scenarios. Case studies include: a shift toward small, community-focused parks in New York, reflecting local green space efforts; the spread of concentrated water pollution across districts in Hong Kong, pointing to coordinated water management; and a notable decline in open dumpsites in Shenzhen, with contrasting links between nighttime economic activity and waste types, indicating differing urban pressures behind domestic and construction waste. Compared to the best-performing baseline, the MMUEChange agent achieves a 46.7% improvement in task success rate and effectively mitigates hallucination, demonstrating its capacity to support complex urban change analysis tasks with real-world policy implications.",
      "published_date": "2026-01-13T05:00:00",
      "source": "ArXiv AI (Test Source)",
      "source_id": "arxiv_ai",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 6,
      "focus_tags": [
        "AI",
        "机器学习",
        "研究论文",
        "算法"
      ]
    },
    {
      "title": "The Evaluation Gap in Medicine, AI and LLMs: Navigating Elusive Ground Truth & Uncertainty via a Probabilistic Paradigm",
      "url": "https://arxiv.org/abs/2601.05500",
      "content": "arXiv:2601.05500v1 Announce Type: new \nAbstract: Benchmarking the relative capabilities of AI systems, including Large Language Models (LLMs) and Vision Models, typically ignores the impact of uncertainty in the underlying ground truth answers from experts. This ambiguity is particularly consequential in medicine where uncertainty is pervasive. In this paper, we introduce a probabilistic paradigm to theoretically explain how high certainty in ground truth answers is almost always necessary for even an expert to achieve high scores, whereas in datasets with high variation in ground truth answers there may be little difference between a random labeller and an expert. Therefore, ignoring uncertainty in ground truth evaluation data can result in the misleading conclusion that a non-expert has similar performance to that of an expert. Using the probabilistic paradigm, we thus bring forth the concepts of expected accuracy and expected F1 to estimate the score an expert human or system can achieve given ground truth answer variability.\n  Our work leads to the recommendation that when establishing the capability of a system, results should be stratified by probability of the ground truth answer, typically measured by the agreement rate of ground truth experts. Stratification becomes critical when the overall performance drops below a threshold of 80%. Under stratified evaluation, performance comparison becomes more reliable in high certainty bins, mitigating the effect of the key confounding factor -- uncertainty.",
      "published_date": "2026-01-13T05:00:00",
      "source": "ArXiv AI (Test Source)",
      "source_id": "arxiv_ai",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 6,
      "focus_tags": [
        "AI",
        "机器学习",
        "研究论文",
        "算法"
      ]
    },
    {
      "title": "Explainable AI: Learning from the Learners",
      "url": "https://arxiv.org/abs/2601.05525",
      "content": "arXiv:2601.05525v1 Announce Type: new \nAbstract: Artificial intelligence now outperforms humans in several scientific and engineering tasks, yet its internal representations often remain opaque. In this Perspective, we argue that explainable artificial intelligence (XAI), combined with causal reasoning, enables {\\it learning from the learners}. Focusing on discovery, optimization and certification, we show how the combination of foundation models and explainability methods allows the extraction of causal mechanisms, guides robust design and control, and supports trust and accountability in high-stakes applications. We discuss challenges in faithfulness, generalization and usability of explanations, and propose XAI as a unifying framework for human-AI collaboration in science and engineering.",
      "published_date": "2026-01-13T05:00:00",
      "source": "ArXiv AI (Test Source)",
      "source_id": "arxiv_ai",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 6,
      "focus_tags": [
        "AI",
        "机器学习",
        "研究论文",
        "算法"
      ]
    },
    {
      "title": "Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making",
      "url": "https://arxiv.org/abs/2601.05529",
      "content": "arXiv:2601.05529v1 Announce Type: new \nAbstract: One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how \"rare\" errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.",
      "published_date": "2026-01-13T05:00:00",
      "source": "ArXiv AI (Test Source)",
      "source_id": "arxiv_ai",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 6,
      "focus_tags": [
        "AI",
        "机器学习",
        "研究论文",
        "算法"
      ]
    },
    {
      "title": "WildSci: Advancing Scientific Reasoning from In-the-Wild Literature",
      "url": "https://arxiv.org/abs/2601.05567",
      "content": "arXiv:2601.05567v1 Announce Type: new \nAbstract: Recent progress in large language model (LLM) reasoning has focused on domains like mathematics and coding, where abundant high-quality data and objective evaluation metrics are readily available. In contrast, progress in LLM reasoning models remains limited in scientific domains such as medicine and materials science due to limited dataset coverage and the inherent complexity of open-ended scientific questions. To address these challenges, we introduce WildSci, a new dataset of domain-specific science questions automatically synthesized from peer-reviewed literature, covering 9 scientific disciplines and 26 subdomains. By framing complex scientific reasoning tasks in a multiple-choice format, we enable scalable training with well-defined reward signals. We further apply reinforcement learning to finetune models on these data and analyze the resulting training dynamics, including domain-specific performance changes, response behaviors, and generalization trends. Experiments on a suite of scientific benchmarks demonstrate the effectiveness of our dataset and approach. We release WildSci to enable scalable and sustainable research in scientific reasoning, available at https://huggingface.co/datasets/JustinTX/WildSci.",
      "published_date": "2026-01-13T05:00:00",
      "source": "ArXiv AI (Test Source)",
      "source_id": "arxiv_ai",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 6,
      "focus_tags": [
        "AI",
        "机器学习",
        "研究论文",
        "算法"
      ]
    },
    {
      "title": "Crisis-Bench: Benchmarking Strategic Ambiguity and Reputation Management in Large Language Models",
      "url": "https://arxiv.org/abs/2601.05570",
      "content": "arXiv:2601.05570v1 Announce Type: new \nAbstract: Standard safety alignment optimizes Large Language Models (LLMs) for universal helpfulness and honesty, effectively instilling a rigid \"Boy Scout\" morality. While robust for general-purpose assistants, this one-size-fits-all ethical framework imposes a \"transparency tax\" on professional domains requiring strategic ambiguity and information withholding, such as public relations, negotiation, and crisis management. To measure this gap between general safety and professional utility, we introduce Crisis-Bench, a multi-agent Partially Observable Markov Decision Process (POMDP) that evaluates LLMs in high-stakes corporate crises. Spanning 80 diverse storylines across 8 industries, Crisis-Bench tasks an LLM-based Public Relations (PR) Agent with navigating a dynamic 7-day corporate crisis simulation while managing strictly separated Private and Public narrative states to enforce rigorous information asymmetry. Unlike traditional benchmarks that rely on static ground truths, we introduce the Adjudicator-Market Loop: a novel evaluation metric where public sentiment is adjudicated and translated into a simulated stock price, creating a realistic economic incentive structure. Our results expose a critical dichotomy: while some models capitulate to ethical concerns, others demonstrate the capacity for Machiavellian, legitimate strategic withholding in order to stabilize the simulated stock price. Crisis-Bench provides the first quantitative framework for assessing \"Reputation Management\" capabilities, arguing for a shift from rigid moral absolutism to context-aware professional alignment.",
      "published_date": "2026-01-13T05:00:00",
      "source": "ArXiv AI (Test Source)",
      "source_id": "arxiv_ai",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 6,
      "focus_tags": [
        "AI",
        "机器学习",
        "研究论文",
        "算法"
      ]
    },
    {
      "title": "Reinforcement Learning of Large Language Models for Interpretable Credit Card Fraud Detection",
      "url": "https://arxiv.org/abs/2601.05578",
      "content": "arXiv:2601.05578v1 Announce Type: new \nAbstract: E-commerce platforms and payment solution providers face increasingly sophisticated fraud schemes, ranging from identity theft and account takeovers to complex money laundering operations that exploit the speed and anonymity of digital transactions. However, despite their theoretical promise, the application of Large Language Models (LLMs) to fraud detection in real-world financial contexts remains largely unexploited, and their practical effectiveness in handling domain-specific e-commerce transaction data has yet to be empirically validated. To bridge this gap between conventional machine learning limitations and the untapped potential of LLMs in fraud detection, this paper proposes a novel approach that employs Reinforcement Learning (RL) to post-train lightweight language models specifically for fraud detection tasks using only raw transaction data. We utilize the Group Sequence Policy Optimization (GSPO) algorithm combined with a rule-based reward system to fine-tune language models of various sizes on a real-life transaction dataset provided by a Chinese global payment solution company. Through this reinforcement learning framework, the language models are encouraged to explore diverse trust and risk signals embedded within the textual transaction data, including patterns in customer information, shipping details, product descriptions, and order history. Our experimental results demonstrate the effectiveness of this approach, with post-trained language models achieving substantial F1-score improvements on held-out test data. Our findings demonstrate that the observed performance improvements are primarily attributable to the exploration mechanism inherent in reinforcement learning, which allows models to discover novel fraud indicators beyond those captured by traditional engineered features.",
      "published_date": "2026-01-13T05:00:00",
      "source": "ArXiv AI (Test Source)",
      "source_id": "arxiv_ai",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 6,
      "focus_tags": [
        "AI",
        "机器学习",
        "研究论文",
        "算法"
      ]
    },
    {
      "title": "A Causal Information-Flow Framework for Unbiased Learning-to-Rank",
      "url": "https://arxiv.org/abs/2601.05590",
      "content": "arXiv:2601.05590v1 Announce Type: new \nAbstract: In web search and recommendation systems, user clicks are widely used to train ranking models. However, click data is heavily biased, i.e., users tend to click higher-ranked items (position bias), choose only what was shown to them (selection bias), and trust top results more (trust bias). Without explicitly modeling these biases, the true relevance of ranked items cannot be correctly learned from clicks. Existing Unbiased Learning-to-Rank (ULTR) methods mainly correct position bias and rely on propensity estimation, but they cannot measure remaining bias, provide risk guarantees, or jointly handle multiple bias sources. To overcome these challenges, this paper introduces a novel causal learning-based ranking framework that extends ULTR by combining Structural Causal Models (SCMs) with information-theoretic tools. SCMs specify how clicks are generated and help identify the true relevance signal from click data, while conditional mutual information, measures how much bias leaks into the\n  learned relevance estimates. We use this leakage measure to define a rigorous notion of disentanglement and include it as a regularizer during model training to reduce bias. In addition, we incorporate a causal inference estimator, i.e., doubly robust estimator, to ensure more reliable risk estimation. Experiments on standard Learning-to-Rank benchmarks show that our method consistently reduces measured bias leakage and improves ranking performance, especially in realistic scenarios where multiple biases-such as position and trust bias-interact strongly.",
      "published_date": "2026-01-13T05:00:00",
      "source": "ArXiv AI (Test Source)",
      "source_id": "arxiv_ai",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 6,
      "focus_tags": [
        "AI",
        "机器学习",
        "研究论文",
        "算法"
      ]
    },
    {
      "title": "Cumulative Path-Level Semantic Reasoning for Inductive Knowledge Graph Completion",
      "url": "https://arxiv.org/abs/2601.05629",
      "content": "arXiv:2601.05629v1 Announce Type: new \nAbstract: Conventional Knowledge Graph Completion (KGC) methods aim to infer missing information in incomplete Knowledge Graphs (KGs) by leveraging existing information, which struggle to perform effectively in scenarios involving emerging entities. Inductive KGC methods can handle the emerging entities and relations in KGs, offering greater dynamic adaptability. While existing inductive KGC methods have achieved some success, they also face challenges, such as susceptibility to noisy structural information during reasoning and difficulty in capturing long-range dependencies in reasoning paths. To address these challenges, this paper proposes the Cumulative Path-Level Semantic Reasoning for inductive knowledge graph completion (CPSR) framework, which simultaneously captures both the structural and semantic information of KGs to enhance the inductive KGC task. Specifically, the proposed CPSR employs a query-dependent masking module to adaptively mask noisy structural information while retaining important information closely related to the targets. Additionally, CPSR introduces a global semantic scoring module that evaluates both the individual contributions and the collective impact of nodes along the reasoning path within KGs. The experimental results demonstrate that CPSR achieves state-of-the-art performance.",
      "published_date": "2026-01-13T05:00:00",
      "source": "ArXiv AI (Test Source)",
      "source_id": "arxiv_ai",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 6,
      "focus_tags": [
        "AI",
        "机器学习",
        "研究论文",
        "算法"
      ]
    },
    {
      "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
      "url": "https://arxiv.org/abs/2601.05637",
      "content": "arXiv:2601.05637v1 Announce Type: new \nAbstract: As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
      "published_date": "2026-01-13T05:00:00",
      "source": "ArXiv AI (Test Source)",
      "source_id": "arxiv_ai",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 6,
      "focus_tags": [
        "AI",
        "机器学习",
        "研究论文",
        "算法"
      ]
    },
    {
      "title": "HAG: Hierarchical Demographic Tree-based Agent Generation for Topic-Adaptive Simulation",
      "url": "https://arxiv.org/abs/2601.05656",
      "content": "arXiv:2601.05656v1 Announce Type: new \nAbstract: High-fidelity agent initialization is crucial for credible Agent-Based Modeling across diverse domains. A robust framework should be Topic-Adaptive, capturing macro-level joint distributions while ensuring micro-level individual rationality. Existing approaches fall into two categories: static data-based retrieval methods that fail to adapt to unseen topics absent from the data, and LLM-based generation methods that lack macro-level distribution awareness, resulting in inconsistencies between micro-level persona attributes and reality. To address these problems, we propose HAG, a Hierarchical Agent Generation framework that formalizes population generation as a two-stage decision process. Firstly, utilizing a World Knowledge Model to infer hierarchical conditional probabilities to construct the Topic-Adaptive Tree, achieving macro-level distribution alignment. Then, grounded real-world data, instantiation and agentic augmentation are carried out to ensure micro-level consistency. Given the lack of specialized evaluation, we establish a multi-domain benchmark and a comprehensive PACE evaluation framework. Extensive experiments show that HAG significantly outperforms representative baselines, reducing population alignment errors by an average of 37.7% and enhancing sociological consistency by 18.8%.",
      "published_date": "2026-01-13T05:00:00",
      "source": "ArXiv AI (Test Source)",
      "source_id": "arxiv_ai",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 6,
      "focus_tags": [
        "AI",
        "机器学习",
        "研究论文",
        "算法"
      ]
    }
  ]
}