{
  "timestamp": "2026-01-30T13:15:14.818459",
  "key": "source_arxiv_cs_lg_7days",
  "value": [
    {
      "title": "Rethinking LLM-Driven Heuristic Design: Generating Efficient and Specialized Solvers via Dynamics-Aware Optimization",
      "url": "https://arxiv.org/abs/2601.20868",
      "content": "arXiv:2601.20868v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have advanced the field of Combinatorial Optimization through automated heuristic generation. Instead of relying on manual design, this LLM-Driven Heuristic Design (LHD) process leverages LLMs to iteratively generate and refine solvers to achieve high performance. However, existing LHD frameworks face two critical limitations: (1) Endpoint-only evaluation, which ranks solvers solely by final quality, ignoring the convergence process and runtime efficiency; (2) High adaptation costs, where distribution shifts necessitate re-adaptation to generate specialized solvers for new instance groups. To address these issues, we propose Dynamics-Aware Solver Heuristics (DASH), a framework that co-optimizes solver search mechanisms and runtime schedules guided by a convergence-aware metric, thereby identifying efficient and high-performance solvers. Furthermore, to mitigate expensive re-adaptation, DASH incorporates Profiled Library Retrieval (PLR). PLR efficiently archives specialized solvers concurrently with the evolutionary process, enabling cost-effective warm-starts for heterogeneous distributions. Experiments on four combinatorial optimization problems demonstrate that DASH improves runtime efficiency by over 3 times, while surpassing the solution quality of state-of-the-art baselines across diverse problem scales. Furthermore, by enabling profile-based warm starts, DASH maintains superior accuracy under different distributions while cutting LLM adaptation costs by over 90%.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.LG (Machine Learning)",
      "source_id": "arxiv_cs_lg",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "machine learning",
        "deep learning",
        "neural networks"
      ]
    },
    {
      "title": "Finetune-Informed Pretraining Boosts Downstream Performance",
      "url": "https://arxiv.org/abs/2601.20884",
      "content": "arXiv:2601.20884v1 Announce Type: new \nAbstract: Multimodal pretraining is effective for building general-purpose representations, but in many practical deployments, only one modality is heavily used during downstream fine-tuning. Standard pretraining strategies treat all modalities uniformly, which can lead to under-optimized representations for the modality that actually matters. We propose Finetune-Informed Pretraining (FIP), a model-agnostic method that biases representation learning toward a designated target modality needed at fine-tuning time. FIP combines higher masking difficulty, stronger loss weighting, and increased decoder capacity for the target modality, without modifying the shared encoder or requiring additional supervision. When applied to masked modeling on constellation diagrams for wireless signals, FIP consistently improves downstream fine-tuned performance with no extra data or compute. FIP is simple to implement, architecture-compatible, and broadly applicable across multimodal masked modeling pipelines.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.LG (Machine Learning)",
      "source_id": "arxiv_cs_lg",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "machine learning",
        "deep learning",
        "neural networks"
      ]
    },
    {
      "title": "A generative machine learning model for designing metal hydrides applied to hydrogen storage",
      "url": "https://arxiv.org/abs/2601.20892",
      "content": "arXiv:2601.20892v1 Announce Type: new \nAbstract: Developing new metal hydrides is a critical step toward efficient hydrogen storage in carbon-neutral energy systems. However, existing materials databases, such as the Materials Project, contain a limited number of well-characterized hydrides, which constrains the discovery of optimal candidates. This work presents a framework that integrates causal discovery with a lightweight generative machine learning model to generate novel metal hydride candidates that may not exist in current databases. Using a dataset of 450 samples (270 training, 90 validation, and 90 testing), the model generates 1,000 candidates. After ranking and filtering, six previously unreported chemical formulas and crystal structures are identified, four of which are validated by density functional theory simulations and show strong potential for future experimental investigation. Overall, the proposed framework provides a scalable and time-efficient approach for expanding hydrogen storage datasets and accelerating materials discovery.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.LG (Machine Learning)",
      "source_id": "arxiv_cs_lg",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "machine learning",
        "deep learning",
        "neural networks"
      ]
    },
    {
      "title": "Is Parameter Isolation Better for Prompt-Based Continual Learning?",
      "url": "https://arxiv.org/abs/2601.20894",
      "content": "arXiv:2601.20894v1 Announce Type: new \nAbstract: Prompt-based continual learning methods effectively mitigate catastrophic forgetting. However, most existing methods assign a fixed set of prompts to each task, completely isolating knowledge across tasks and resulting in suboptimal parameter utilization. To address this, we consider the practical needs of continual learning and propose a prompt-sharing framework. This framework constructs a global prompt pool and introduces a task-aware gated routing mechanism that sparsely activates a subset of prompts to achieve dynamic decoupling and collaborative optimization of task-specific feature representations. Furthermore, we introduce a history-aware modulator that leverages cumulative prompt activation statistics to protect frequently used prompts from excessive updates, thereby mitigating inefficient parameter usage and knowledge forgetting. Extensive analysis and empirical results demonstrate that our approach consistently outperforms existing static allocation strategies in effectiveness and efficiency.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.LG (Machine Learning)",
      "source_id": "arxiv_cs_lg",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "machine learning",
        "deep learning",
        "neural networks"
      ]
    },
    {
      "title": "Faster Predictive Coding Networks via Better Initialization",
      "url": "https://arxiv.org/abs/2601.20895",
      "content": "arXiv:2601.20895v1 Announce Type: new \nAbstract: Research aimed at scaling up neuroscience inspired learning algorithms for neural networks is accelerating. Recently, a key research area has been the study of energy-based learning algorithms such as predictive coding, due to their versatility and mathematical grounding. However, the applicability of such methods is held back by the large computational requirements caused by their iterative nature. In this work, we address this problem by showing that the choice of initialization of the neurons in a predictive coding network matters significantly and can notably reduce the required training times. Consequently, we propose a new initialization technique for predictive coding networks that aims to preserve the iterative progress made on previous training samples. Our approach suggests a promising path toward reconciling the disparities between predictive coding and backpropagation in terms of computational efficiency and final performance. In fact, our experiments demonstrate substantial improvements in convergence speed and final test loss in both supervised and unsupervised settings.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.LG (Machine Learning)",
      "source_id": "arxiv_cs_lg",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "machine learning",
        "deep learning",
        "neural networks"
      ]
    },
    {
      "title": "TwinWeaver: An LLM-Based Foundation Model Framework for Pan-Cancer Digital Twins",
      "url": "https://arxiv.org/abs/2601.20906",
      "content": "arXiv:2601.20906v1 Announce Type: new \nAbstract: Precision oncology requires forecasting clinical events and trajectories, yet modeling sparse, multi-modal clinical time series remains a critical challenge. We introduce TwinWeaver, an open-source framework that serializes longitudinal patient histories into text, enabling unified event prediction as well as forecasting with large language models, and use it to build Genie Digital Twin (GDT) on 93,054 patients across 20 cancer types. In benchmarks, GDT significantly reduces forecasting error, achieving a median Mean Absolute Scaled Error (MASE) of 0.87 compared to 0.97 for the strongest time-series baseline (p<0.001). Furthermore, GDT improves risk stratification, achieving an average concordance index (C-index) of 0.703 across survival, progression, and therapy switching tasks, surpassing the best baseline of 0.662. GDT also generalizes to out-of-distribution clinical trials, matching trained baselines at zero-shot and surpassing them with fine-tuning, achieving a median MASE of 0.75-0.88 and outperforming the strongest baseline in event prediction with an average C-index of 0.672 versus 0.648. Finally, TwinWeaver enables an interpretable clinical reasoning extension, providing a scalable and transparent foundation for longitudinal clinical modeling.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.LG (Machine Learning)",
      "source_id": "arxiv_cs_lg",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "machine learning",
        "deep learning",
        "neural networks"
      ]
    },
    {
      "title": "Noisy but Valid: Robust Statistical Evaluation of LLMs with Imperfect Judges",
      "url": "https://arxiv.org/abs/2601.20913",
      "content": "arXiv:2601.20913v1 Announce Type: new \nAbstract: Reliable certification of Large Language Models (LLMs)-verifying that failure rates are below a safety threshold-is critical yet challenging. While \"LLM-as-a-Judge\" offers scalability, judge imperfections, noise, and bias can invalidate statistical guarantees. We introduce a \"Noisy but Valid\" hypothesis testing framework to address this. By leveraging a small human-labelled calibration set to estimate the judge's True Positive and False Positive Rates (TPR/FPR), we derive a variance-corrected critical threshold applied to a large judge-labelled dataset. Crucially, our framework theoretically guarantees finite-sample Type-I error control (validity) despite calibration uncertainty. This distinguishes our work from Prediction-Powered Inference (PPI), positioning our method as a diagnostic tool that explicitly models judge behavior rather than a black-box estimator. Our contributions include: (1) Theoretical Guarantees: We derive the exact conditions under which noisy testing yields higher statistical power than direct evaluation; (2) Empirical Validation: Experiments on Jigsaw Comment, Hate Speech and SafeRLHF confirm our theory; (3) The Oracle Gap: We reveal a significant performance gap between practical methods and the theoretical \"Oracle\" (perfectly known judge parameters), quantifying the cost of estimation. Specifically, we provide the first systematic treatment of the imperfect-judge setting, yielding interpretable diagnostics of judge reliability and clarifying how evaluation power depends on judge quality, dataset size, and certification levels. Together, these results sharpen understanding of statistical evaluation with LLM judges, and highlight trade-offs among competing inferential tools.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.LG (Machine Learning)",
      "source_id": "arxiv_cs_lg",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "machine learning",
        "deep learning",
        "neural networks"
      ]
    },
    {
      "title": "Noninvasive Intracranial Pressure Estimation Using Subspace System Identification and Bespoke Machine Learning Algorithms: A Learning-to-Rank Approach",
      "url": "https://arxiv.org/abs/2601.20916",
      "content": "arXiv:2601.20916v1 Announce Type: new \nAbstract: Objective: Accurate noninvasive estimation of intracranial pressure (ICP) remains a major challenge in critical care. We developed a bespoke machine learning algorithm that integrates system identification and ranking-constrained optimization to estimate mean ICP from noninvasive signals. Methods: A machine learning framework was proposed to obtain accurate mean ICP values using arbitrary noninvasive signals. The subspace system identification algorithm is employed to identify cerebral hemodynamics models for ICP simulation using arterial blood pressure (ABP), cerebral blood velocity (CBv), and R-wave to R-wave interval (R-R interval) signals in a comprehensive database. A mapping function to describe the relationship between the features of noninvasive signals and the estimation errors is learned using innovative ranking constraints through convex optimization. Patients across multiple clinical settings were randomly split into testing and training datasets for performance evaluation of the mapping function. Results: The results indicate that about 31.88% of testing entries achieved estimation errors within 2 mmHg and 34.07% of testing entries between 2 mmHg to 6 mmHg from the nonlinear mapping with constraints. Conclusion: Our results demonstrate the feasibility of the proposed noninvasive ICP estimation approach. Significance: Further validation and technical refinement are required before clinical deployment, but this work lays the foundation for safe and broadly accessible ICP monitoring in patients with acute brain injury and related conditions.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.LG (Machine Learning)",
      "source_id": "arxiv_cs_lg",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "machine learning",
        "deep learning",
        "neural networks"
      ]
    },
    {
      "title": "A Theory of Universal Agnostic Learning",
      "url": "https://arxiv.org/abs/2601.20961",
      "content": "arXiv:2601.20961v1 Announce Type: new \nAbstract: We provide a complete theory of optimal universal rates for binary classification in the agnostic setting. This extends the realizable-case theory of Bousquet, Hanneke, Moran, van Handel, and Yehudayoff (2021) by removing the realizability assumption on the distribution. We identify a fundamental tetrachotomy of optimal rates: for every concept class, the optimal universal rate of convergence of the excess error rate is one of $e^{-n}$, $e^{-o(n)}$, $o(n^{-1/2})$, or arbitrarily slow. We further identify simple combinatorial structures which determine which of these categories any given concept class falls into.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.LG (Machine Learning)",
      "source_id": "arxiv_cs_lg",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "machine learning",
        "deep learning",
        "neural networks"
      ]
    },
    {
      "title": "Monotone Optimisation with Learned Projections",
      "url": "https://arxiv.org/abs/2601.20983",
      "content": "arXiv:2601.20983v1 Announce Type: new \nAbstract: Monotone optimisation problems admit specialised global solvers such as the Polyblock Outer Approximation (POA) algorithm, but these methods typically require explicit objective and constraint functions. In many applications, these functions are only available through data, making POA difficult to apply directly. We introduce an algorithm-aware learning approach that integrates learned models into POA by directly predicting its projection primitive via the radial inverse, avoiding the costly bisection procedure used in standard POA. We propose Homogeneous-Monotone Radial Inverse (HM-RI) networks, structured neural architectures that enforce key monotonicity and homogeneity properties, enabling fast projection estimation. We provide a theoretical characterisation of radial inverse functions and show that, under mild structural conditions, a HM-RI predictor corresponds to the radial inverse of a valid set of monotone constraints. To reduce training overhead, we further develop relaxed monotonicity conditions that remain compatible with POA. Across multiple monotone optimisation benchmarks (indefinite quadratic programming, multiplicative programming, and transmit power optimisation), our approach yields substantial speed-ups in comparison to direct function estimation while maintaining strong solution quality, outperforming baselines that do not exploit monotonic structure.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.LG (Machine Learning)",
      "source_id": "arxiv_cs_lg",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "machine learning",
        "deep learning",
        "neural networks"
      ]
    },
    {
      "title": "Distributional Active Inference",
      "url": "https://arxiv.org/abs/2601.20985",
      "content": "arXiv:2601.20985v1 Announce Type: new \nAbstract: Optimal control of complex environments with robotic systems faces two complementary and intertwined challenges: efficient organization of sensory state information and far-sighted action planning. Because the reinforcement learning framework addresses only the latter, it tends to deliver sample-inefficient solutions. Active inference is the state-of-the-art process theory that explains how biological brains handle this dual problem. However, its applications to artificial intelligence have thus far been limited to extensions of existing model-based approaches. We present a formal abstraction of reinforcement learning algorithms that spans model-based, distributional, and model-free approaches. This abstraction seamlessly integrates active inference into the distributional reinforcement learning framework, making its performance advantages accessible without transition dynamics modeling.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.LG (Machine Learning)",
      "source_id": "arxiv_cs_lg",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "machine learning",
        "deep learning",
        "neural networks"
      ]
    },
    {
      "title": "Pre-trained Encoders for Global Child Development: Transfer Learning Enables Deployment in Data-Scarce Settings",
      "url": "https://arxiv.org/abs/2601.20987",
      "content": "arXiv:2601.20987v1 Announce Type: new \nAbstract: A large number of children experience preventable developmental delays each year, yet the deployment of machine learning in new countries has been stymied by a data bottleneck: reliable models require thousands of samples, while new programs begin with fewer than 100. We introduce the first pre-trained encoder for global child development, trained on 357,709 children across 44 countries using UNICEF survey data. With only 50 training samples, the pre-trained encoder achieves an average AUC of 0.65 (95% CI: 0.56-0.72), outperforming cold-start gradient boosting at 0.61 by 8-12% across regions. At N=500, the encoder achieves an AUC of 0.73. Zero-shot deployment to unseen countries achieves AUCs up to 0.84. We apply a transfer learning bound to explain why pre-training diversity enables few-shot generalization. These results establish that pre-trained encoders can transform the feasibility of ML for SDG 4.2.1 monitoring in resource-constrained settings.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.LG (Machine Learning)",
      "source_id": "arxiv_cs_lg",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "machine learning",
        "deep learning",
        "neural networks"
      ]
    },
    {
      "title": "Top-k on a Budget: Adaptive Ranking with Weak and Strong Oracles",
      "url": "https://arxiv.org/abs/2601.20989",
      "content": "arXiv:2601.20989v1 Announce Type: new \nAbstract: Identifying the top-$k$ items is fundamental but often prohibitive when exact valuations are expensive. We study a two-oracle setting with a fast, noisy weak oracle and a scarce, high-fidelity strong oracle (e.g., human expert verification or expensive simulation). We first analyze a simple screen-then-certify baseline (STC) and prove it makes at most $m(4\\varepsilon_{\\max})$ strong calls given jointly valid weak confidence intervals with maximum radius $\\varepsilon_{\\max}$, where $m(\\cdot)$ denotes the near-tie mass around the top-$k$ threshold. We establish a conditional lower bound of $\\Omega(m(\\varepsilon_{\\max}))$ for any algorithm given the same weak uncertainty. Our main contribution is ACE, an adaptive certification algorithm that focuses strong queries on critical boundary items, achieving the same $O(m(4\\varepsilon_{\\max}))$ bound while reducing strong calls in practice. We then introduce ACE-W, a fully adaptive two-phase method that allocates weak budget adaptively before running ACE, further reducing strong costs.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.LG (Machine Learning)",
      "source_id": "arxiv_cs_lg",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "machine learning",
        "deep learning",
        "neural networks"
      ]
    },
    {
      "title": "The Depth Delusion: Why Transformers Should Be Wider, Not Deeper",
      "url": "https://arxiv.org/abs/2601.20994",
      "content": "arXiv:2601.20994v1 Announce Type: new \nAbstract: Neural scaling laws describe how language model loss decreases with parameters and data, but treat architecture as interchangeable--a billion parameters could arise from a shallow-wide model (10 layers & 8,192 hidden dimension) or a deep-narrow one (80 layers & 2,048 hidden dimension). We propose architecture-conditioned scaling laws decomposing this dependence, finding that optimal depth scales as D* ~ C^0.12 while optimal width scales as W* ~ C^0.34, meaning width should grow 2.8x faster than depth. We discover a critical depth phenomenon: beyond D_crit ~ W^0.44 (sublinear in W), adding layers increases loss despite adding parameters--the Depth Delusion. Empirically, we validate these findings across 30 transformer architectures spanning 17M to 7B parameters, each trained on representative high-compute samples, achieving R^2 = 0.922. Our central finding: at 7B scale, a 64-layer model (6.38B params) underperforms a 32-layer model (6.86B params) by 0.12 nats, despite being significantly deeper. This demonstrates that optimal depth-width tradeoffs persist at the production scale.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.LG (Machine Learning)",
      "source_id": "arxiv_cs_lg",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "machine learning",
        "deep learning",
        "neural networks"
      ]
    },
    {
      "title": "MADE: Benchmark Environments for Closed-Loop Materials Discovery",
      "url": "https://arxiv.org/abs/2601.20996",
      "content": "arXiv:2601.20996v1 Announce Type: new \nAbstract: Existing benchmarks for computational materials discovery primarily evaluate static predictive tasks or isolated computational sub-tasks. While valuable, these evaluations neglect the inherently iterative and adaptive nature of scientific discovery. We introduce MAterials Discovery Environments (MADE), a novel framework for benchmarking end-to-end autonomous materials discovery pipelines. MADE simulates closed-loop discovery campaigns in which an agent or algorithm proposes, evaluates, and refines candidate materials under a constrained oracle budget, capturing the sequential and resource-limited nature of real discovery workflows. We formalize discovery as a search for thermodynamically stable compounds relative to a given convex hull, and evaluate efficacy and efficiency via comparison to baseline algorithms. The framework is flexible; users can compose discovery agents from interchangeable components such as generative models, filters, and planners, enabling the study of arbitrary workflows ranging from fixed pipelines to fully agentic systems with tool use and adaptive decision making. We demonstrate this by conducting systematic experiments across a family of systems, enabling ablation of components in discovery pipelines, and comparison of how methods scale with system complexity.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.LG (Machine Learning)",
      "source_id": "arxiv_cs_lg",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "machine learning",
        "deep learning",
        "neural networks"
      ]
    },
    {
      "title": "Solver-in-the-Loop: MDP-Based Benchmarks for Self-Correction and Behavioral Rationality in Operations Research",
      "url": "https://arxiv.org/abs/2601.21008",
      "content": "arXiv:2601.21008v1 Announce Type: new \nAbstract: Operations Research practitioners routinely debug infeasible models through an iterative process: analyzing Irreducible Infeasible Subsystems (\\IIS{}), identifying constraint conflicts, and systematically repairing formulations until feasibility is achieved. Yet existing LLM benchmarks evaluate OR as one-shot translation -- given a problem description, generate solver code -- ignoring this diagnostic loop entirely. We introduce two benchmarks that place the \\textbf{solver in the evaluation loop}. \\textbf{\\ORDebug{}} evaluates iterative self-correction through 5,000+ problems spanning 9 error types; each repair action triggers solver re-execution and \\IIS{} recomputation, providing deterministic, verifiable feedback. \\textbf{\\ORBias{}} evaluates behavioral rationality through 2,000 newsvendor instances (1,000 ID + 1,000 OOD), measuring systematic deviations from closed-form optimal policies. Across 26 models and 12,000+ samples, we find that domain-specific RLVR training enables an 8B model to surpass frontier APIs: 95.3\\% vs 86.2\\% recovery rate (+9.1\\%), 62.4\\% vs 47.8\\% diagnostic accuracy (+14.6\\%), and 2.25 vs 3.78 steps to resolution (1.7$\\times$ faster). On \\ORBias{}, curriculum training achieves the only negative ID$\\rightarrow$OOD bias drift among models evaluated (-9.6\\%), reducing systematic bias by 48\\% (from 20.0\\% to 10.4\\%). These results demonstrate that process-level evaluation with verifiable oracles enables targeted training that outperforms scale.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.LG (Machine Learning)",
      "source_id": "arxiv_cs_lg",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "machine learning",
        "deep learning",
        "neural networks"
      ]
    },
    {
      "title": "Order-Aware Test-Time Adaptation: Leveraging Temporal Dynamics for Robust Streaming Inference",
      "url": "https://arxiv.org/abs/2601.21012",
      "content": "arXiv:2601.21012v1 Announce Type: new \nAbstract: Test-Time Adaptation (TTA) enables pre-trained models to adjust to distribution shift by learning from unlabeled test-time streams. However, existing methods typically treat these streams as independent samples, overlooking the supervisory signal inherent in temporal dynamics. To address this, we introduce Order-Aware Test-Time Adaptation (OATTA). We formulate test-time adaptation as a gradient-free recursive Bayesian estimation task, using a learned dynamic transition matrix as a temporal prior to refine the base model's predictions. To ensure safety in weakly structured streams, we introduce a likelihood-ratio gate (LLR) that reverts to the base predictor when temporal evidence is absent. OATTA is a lightweight, model-agnostic module that incurs negligible computational overhead. Extensive experiments across image classification, wearable and physiological signal analysis, and language sentiment analysis demonstrate its universality; OATTA consistently boosts established baselines, improving accuracy by up to 6.35%. Our findings establish that modeling temporal dynamics provides a critical, orthogonal signal beyond standard order-agnostic TTA approaches.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.LG (Machine Learning)",
      "source_id": "arxiv_cs_lg",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "machine learning",
        "deep learning",
        "neural networks"
      ]
    },
    {
      "title": "Conditional Denoising Model as a Physical Surrogate Model",
      "url": "https://arxiv.org/abs/2601.21021",
      "content": "arXiv:2601.21021v1 Announce Type: new \nAbstract: Surrogate modeling for complex physical systems typically faces a trade-off between data-fitting accuracy and physical consistency. Physics-consistent approaches typically treat physical laws as soft constraints within the loss function, a strategy that frequently fails to guarantee strict adherence to the governing equations, or rely on post-processing corrections that do not intrinsically learn the underlying solution geometry. To address these limitations, we introduce the {Conditional Denoising Model (CDM)}, a generative model designed to learn the geometry of the physical manifold itself. By training the network to restore clean states from noisy ones, the model learns a vector field that points continuously towards the valid solution subspace. We introduce a time-independent formulation that transforms inference into a deterministic fixed-point iteration, effectively projecting noisy approximations onto the equilibrium manifold. Validated on a low-temperature plasma physics and chemistry benchmark, the CDM achieves higher parameter and data efficiency than physics-consistent baselines. Crucially, we demonstrate that the denoising objective acts as a powerful implicit regularizer: despite never seeing the governing equations during training, the model adheres to physical constraints more strictly than baselines trained with explicit physics losses.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.LG (Machine Learning)",
      "source_id": "arxiv_cs_lg",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "machine learning",
        "deep learning",
        "neural networks"
      ]
    },
    {
      "title": "SIGMA-PPG: Statistical-prior Informed Generative Masking Architecture for PPG Foundation Model",
      "url": "https://arxiv.org/abs/2601.21031",
      "content": "arXiv:2601.21031v1 Announce Type: new \nAbstract: Current foundation model for photoplethysmography (PPG) signals is challenged by the intrinsic redundancy and noise of the signal. Standard masked modeling often yields trivial solutions while contrastive methods lack morphological precision. To address these limitations, we propose a Statistical-prior Informed Generative Masking Architecture (SIGMA-PPG), a generative foundation model featuring a Prior-Guided Adversarial Masking mechanism, where a reinforcement learning-driven teacher leverages statistical priors to create challenging learning paths that prevent overfitting to noise. We also incorporate a semantic consistency constraint via vector quantization to ensure that physiologically identical waveforms (even those altered by recording artifacts or minor perturbations) map to shared indices. This enhances codebook semantic density and eliminates redundant feature structures. Pre-trained on over 120,000 hours of data, SIGMA-PPG achieves superior average performance compared to five state-of-the-art baselines across 12 diverse downstream tasks. The code is available at https://github.com/ZonghengGuo/SigmaPPG.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.LG (Machine Learning)",
      "source_id": "arxiv_cs_lg",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "machine learning",
        "deep learning",
        "neural networks"
      ]
    },
    {
      "title": "Predict-Project-Renoise: Sampling Diffusion Models under Hard Constraints",
      "url": "https://arxiv.org/abs/2601.21033",
      "content": "arXiv:2601.21033v1 Announce Type: new \nAbstract: Neural emulators based on diffusion models show promise for scientific applications, but vanilla models cannot guarantee physical accuracy or constraint satisfaction. We address this by introducing a constrained sampling framework that enforces hard constraints, such as physical laws or observational consistency, at generation time. Our approach defines a constrained forward process that diffuses only over the feasible set of constraint-satisfying samples, inducing constrained marginal distributions. To reverse this, we propose Predict-Project-Renoise (PPR), an iterative algorithm that samples from the constrained marginals by alternating between denoising predictions, projecting onto the feasible set, and renoising. Experiments on 2D distributions, PDEs, and global weather forecasting demonstrate that PPR reduces constraint violations by over an order of magnitude while improving sample consistency and better matching the true constrained distribution compared to baselines.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.LG (Machine Learning)",
      "source_id": "arxiv_cs_lg",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "machine learning",
        "deep learning",
        "neural networks"
      ]
    }
  ]
}