{
  "timestamp": "2026-01-30T13:14:56.625515",
  "key": "source_arxiv_cs_cl_7days",
  "value": [
    {
      "title": "DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents",
      "url": "https://arxiv.org/abs/2601.20975",
      "content": "arXiv:2601.20975v1 Announce Type: new \nAbstract: We introduce DeepSearchQA, a 900-prompt benchmark for evaluating agents on difficult multi-step information-seeking tasks across 17 different fields. Unlike traditional benchmarks that target single answer retrieval or broad-spectrum factuality, DeepSearchQA features a dataset of challenging, handcrafted tasks designed to evaluate an agent's ability to execute complex search plans to generate exhaustive answer lists. This shift in design explicitly tests three critical, yet under-evaluated capabilities: 1) systematic collation of fragmented information from disparate sources, 2) de-duplication and entity resolution to ensure precision, and 3) the ability to reason about stopping criteria within an open-ended search space. Each task is structured as a causal chain, where discovering information for one step is dependent on the successful completion of the previous one, stressing long-horizon planning and context retention. All tasks are grounded in the open web with objectively verifiable answer sets. Our comprehensive evaluation of state-of-the-art agent architectures reveals significant performance limitations: even the most advanced models struggle to balance high recall with precision. We observe distinct failure modes ranging from premature stopping (under-retrieval) to hedging behaviors, where agents cast an overly wide net of low-confidence answers to artificially boost recall. These findings highlight critical headroom in current agent designs and position DeepSearchQA as an essential diagnostic tool for driving future research toward more robust, deep-research capabilities.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.CL (NLP)",
      "source_id": "arxiv_cs_cl",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "NLP",
        "LLM",
        "language models",
        "transformers"
      ]
    },
    {
      "title": "asr_eval: Algorithms and tools for multi-reference and streaming speech recognition evaluation",
      "url": "https://arxiv.org/abs/2601.20992",
      "content": "arXiv:2601.20992v1 Announce Type: new \nAbstract: We propose several improvements to the speech recognition evaluation. First, we propose a string alignment algorithm that supports both multi-reference labeling, arbitrary-length insertions and better word alignment. This is especially useful for non-Latin languages, those with rich word formation, to label cluttered or longform speech. Secondly, we collect a novel test set DiverseSpeech-Ru of longform in-the-wild Russian speech with careful multi-reference labeling. We also perform multi-reference relabeling of popular Russian tests set and study fine-tuning dynamics on its corresponding train set. We demonstrate that the model often adopts to dataset-specific labeling, causing an illusion of metric improvement. Based on the improved word alignment, we develop tools to evaluate streaming speech recognition and to align multiple transcriptions to compare them visually. Additionally, we provide uniform wrappers for many offline and streaming speech recognition models. Our code will be made publicly available.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.CL (NLP)",
      "source_id": "arxiv_cs_cl",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "NLP",
        "LLM",
        "language models",
        "transformers"
      ]
    },
    {
      "title": "UrduBench: An Urdu Reasoning Benchmark using Contextually Ensembled Translations with Human-in-the-Loop",
      "url": "https://arxiv.org/abs/2601.21000",
      "content": "arXiv:2601.21000v1 Announce Type: new \nAbstract: Recent advances in large language models (LLMs) have led to strong reasoning capabilities; however, evaluating such models in low-resource languages remains challenging due to the lack of standardized benchmarks. In particular, Urdu reasoning evaluation has been limited by the sensitivity of machine translation and an emphasis on general language tasks rather than reasoning benchmarks. In this paper, we propose a contextually ensembled translation framework with human-in-the-loop validation that leverages multiple translation systems to develop Urdu reasoning benchmarks while preserving contextual and structural integrity. Using this framework, we translate widely adopted reasoning and question-answering benchmarks, including MGSM, MATH-500, CommonSenseQA, and OpenBookQA, into Urdu, collectively referred to as UrduBench, and conduct a comprehensive evaluation of both reasoning-oriented and instruction-tuned LLMs across multiple prompting strategies. Our analysis reveals performance differences across (1) four datasets, (2) five task difficulty levels, (3) diverse model architectures, (4) multiple model scaling settings, and (5) language consistency tests. We find that multi-step and symbolic reasoning tasks pose significant challenges in Urdu, and that stable language alignment is a critical prerequisite for robust reasoning. Overall, our work establishes a scalable methodology for standardized reasoning evaluation in Urdu and provides empirical insights into multilingual reasoning failures. This experimental setup is also broadly applicable to other low-resource languages. The code and datasets will be publicly released.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.CL (NLP)",
      "source_id": "arxiv_cs_cl",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "NLP",
        "LLM",
        "language models",
        "transformers"
      ]
    },
    {
      "title": "Position-invariant Fine-tuning of Speech Enhancement Models with Self-supervised Speech Representations",
      "url": "https://arxiv.org/abs/2601.21084",
      "content": "arXiv:2601.21084v1 Announce Type: new \nAbstract: Integrating front-end speech enhancement (SE) models with self-supervised learning (SSL)-based speech models is effective for downstream tasks in noisy conditions. SE models are commonly fine-tuned using SSL representations with mean squared error (MSE) loss between enhanced and clean speech. However, MSE is prone to exploiting positional embeddings in SSL models, allowing the objective to be minimised through positional correlations instead of content-related information. This work frames the problem as a general limitation of self-supervised representation fine-tuning and investigates it through representation-guided SE. Two strategies are considered: (1) zero-padding, previously explored in SSL pre-training but here examined in the fine-tuning setting, and (2) speed perturbations with a soft-DTW loss. Experiments show that the soft-DTW-based approach achieves faster convergence and improved downstream performance, underscoring the importance of position-invariant fine-tuning in SSL-based speech modelling.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.CL (NLP)",
      "source_id": "arxiv_cs_cl",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "NLP",
        "LLM",
        "language models",
        "transformers"
      ]
    },
    {
      "title": "ChunkWise LoRA: Adaptive Sequence Partitioning for Memory-Efficient Low-Rank Adaptation and Accelerated LLM Inference",
      "url": "https://arxiv.org/abs/2601.21109",
      "content": "arXiv:2601.21109v1 Announce Type: new \nAbstract: Recent advances in low-rank adaptation (LoRA) have enabled efficient fine-tuning of large language models (LLMs) with minimal additional parameters. However, existing LoRA methods apply static rank configurations uniformly across all input tokens, ignoring variation in token complexity and computational requirements. In this work, we propose ChunkWise LoRA, a dynamic and adaptive approach that partitions sequences into variable-length chunks based on token complexity and assigns each chunk a tailored low-rank configuration. Our system introduces a runtime scheduler that estimates token difficulty, performs adaptive chunking, and selects per-chunk LoRA rank and scaling using a rank-ladder mechanism. To preserve output consistency, we further introduce a boundary-safe composition module and integrate policy-driven KV-cache strategies. Experiments on benchmark datasets such as Wikitext-103 and SQuAD demonstrate that ChunkWise LoRA achieves up to 34\\% lower latency and 38% memory reduction compared to baseline LoRA, while maintaining or improving task performance metrics like BLEU, EM, and perplexity. The proposed framework remains fully compatible with existing transformer architectures and inference frameworks, providing a practical solution for real-world deployment of parameter-efficient LLMs.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.CL (NLP)",
      "source_id": "arxiv_cs_cl",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "NLP",
        "LLM",
        "language models",
        "transformers"
      ]
    },
    {
      "title": "Multi-task Code LLMs: Data Mix or Model Merge?",
      "url": "https://arxiv.org/abs/2601.21115",
      "content": "arXiv:2601.21115v1 Announce Type: new \nAbstract: Recent research advocates deploying smaller, specialized code LLMs in agentic frameworks alongside frontier models, sparking interest in efficient strategies for multi-task learning that balance performance, constraints, and costs. We compare two approaches for creating small, multi-task code LLMs: data mixing versus model merging. We conduct extensive experiments across two model families (Qwen Coder and DeepSeek Coder) at two scales (2B and 7B parameters), fine-tuning them for code generation and code summarization tasks. Our evaluation on HumanEval, MBPP, and CodeXGlue benchmarks reveals that model merging achieves the best overall performance at larger scale across model families, retaining 96% of specialized model performance on code generation tasks while maintaining summarization capabilities. Notably, merged models can even surpass individually fine-tuned models, with our best configuration of Qwen Coder 2.5 7B model achieving 92.7% Pass@1 on HumanEval compared to 90.9% for its task-specific fine-tuned equivalent. At a smaller scale we find instead data mixing to be a preferred strategy. We further introduce a weight analysis technique to understand how different tasks affect model parameters and their implications for merging strategies. The results suggest that careful merging and mixing strategies can effectively combine task-specific capabilities without significant performance degradation, making them ideal for resource-constrained deployment scenarios.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.CL (NLP)",
      "source_id": "arxiv_cs_cl",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "NLP",
        "LLM",
        "language models",
        "transformers"
      ]
    },
    {
      "title": "Large Language Models Naively Recover Ethnicity from Individual Records",
      "url": "https://arxiv.org/abs/2601.21132",
      "content": "arXiv:2601.21132v1 Announce Type: new \nAbstract: I demonstrate that large language models can infer ethnicity from names with accuracy exceeding that of Bayesian Improved Surname Geocoding (BISG) without additional training data, enabling inference outside the United States and to contextually appropriate classification categories. Using stratified samples from Florida and North Carolina voter files with self-reported race, LLM-based classification achieves up to 84.7% accuracy, outperforming BISG (68.2%) on balanced samples. I test six models including Gemini 3 Flash, GPT-4o, and open-source alternatives such as DeepSeek v3.2 and GLM-4.7. Enabling extended reasoning can improve accuracy by 1-3 percentage points, though effects vary across contexts; including metadata such as party registration reaches 86.7%. LLM classification also reduces the income bias inherent in BISG, where minorities in wealthier neighborhoods are systematically misclassified as White. I further validate using Lebanese voter registration with religious sect (64.3% accuracy), Indian MPs from reserved constituencies (99.2%), and Indian land records with caste classification (74.0%). Aggregate validation across India, Uganda, Nepal, Armenia, Chile, and Costa Rica using original full-count voter rolls demonstrates that the method recovers known population distributions where naming conventions are distinctive. For large-scale applications, small transformer models fine-tuned on LLM labels exceed BISG accuracy while enabling local deployment at no cost.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.CL (NLP)",
      "source_id": "arxiv_cs_cl",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "NLP",
        "LLM",
        "language models",
        "transformers"
      ]
    },
    {
      "title": "EnsembleLink: Accurate Record Linkage Without Training Data",
      "url": "https://arxiv.org/abs/2601.21138",
      "content": "arXiv:2601.21138v1 Announce Type: new \nAbstract: Record linkage, the process of matching records that refer to the same entity across datasets, is essential to empirical social science but remains methodologically underdeveloped. Researchers treat it as a preprocessing step, applying ad hoc rules without quantifying the uncertainty that linkage errors introduce into downstream analyses. Existing methods either achieve low accuracy or require substantial labeled training data. I present EnsembleLink, a method that achieves high accuracy without any training labels. EnsembleLink leverages pre-trained language models that have learned semantic relationships (e.g., that \"South Ozone Park\" is a neighborhood in \"New York City\" or that \"Lutte ouvriere\" refers to the Trotskyist \"Workers' Struggle\" party) from large text corpora. On benchmarks spanning city names, person names, organizations, multilingual political parties, and bibliographic records, EnsembleLink matches or exceeds methods requiring extensive labeling. The method runs locally on open-source models, requiring no external API calls, and completes typical linkage tasks in minutes.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.CL (NLP)",
      "source_id": "arxiv_cs_cl",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "NLP",
        "LLM",
        "language models",
        "transformers"
      ]
    },
    {
      "title": "Output-Space Search: Targeting LLM Generations in a Frozen Encoder-Defined Output Space",
      "url": "https://arxiv.org/abs/2601.21169",
      "content": "arXiv:2601.21169v1 Announce Type: new \nAbstract: We introduce Output-Space Search (OS-Search), which turns LLM generation into endpoint search. An outer loop selects a target z* in a frozen encoder-defined 3D output space Z, and a retrieval-grounded policy trained with sequence-level RL generates outputs whose coordinates land near z* under standard autoregressive decoding. This enables parallel sweeps and black-box optimization in Z without path-dependent token/program search. On stories, sweeping Z (text) yields 3.1x higher LLM-scored diversity than prompt-chaining. On code, Bayesian optimization over Z (code) improves an objective withheld from the controller under matched inference budgets while preserving validity.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.CL (NLP)",
      "source_id": "arxiv_cs_cl",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "NLP",
        "LLM",
        "language models",
        "transformers"
      ]
    },
    {
      "title": "From Linear Input to Hierarchical Structure: Function Words as Statistical Cues for Language Learning",
      "url": "https://arxiv.org/abs/2601.21191",
      "content": "arXiv:2601.21191v1 Announce Type: new \nAbstract: What statistical conditions support learning hierarchical structure from linear input? In this paper, we address this question by focusing on the statistical distribution of function words. Function words have long been argued to play a crucial role in language acquisition due to their distinctive distributional properties, including high frequency, reliable association with syntactic structure, and alignment with phrase boundaries. We use cross-linguistic corpus analysis to first establish that all three properties are present across 186 studied languages. Next, we use a combination of counterfactual language modeling and ablation experiments to show that language variants preserving all three properties are more easily acquired by neural learners, with frequency and structural association contributing more strongly than boundary alignment. Follow-up probing and ablation analyses further reveal that different learning conditions lead to systematically different reliance on function words, indicating that similar performance can arise from distinct internal mechanisms.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.CL (NLP)",
      "source_id": "arxiv_cs_cl",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "NLP",
        "LLM",
        "language models",
        "transformers"
      ]
    },
    {
      "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
      "url": "https://arxiv.org/abs/2601.21204",
      "content": "arXiv:2601.21204v1 Announce Type: new \nAbstract: While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.CL (NLP)",
      "source_id": "arxiv_cs_cl",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "NLP",
        "LLM",
        "language models",
        "transformers"
      ]
    },
    {
      "title": "Multilingual Dysarthric Speech Assessment Using Universal Phone Recognition and Language-Specific Phonemic Contrast Modeling",
      "url": "https://arxiv.org/abs/2601.21205",
      "content": "arXiv:2601.21205v1 Announce Type: new \nAbstract: The growing prevalence of neurological disorders associated with dysarthria motivates the need for automated intelligibility assessment methods that are applicalbe across languages. However, most existing approaches are either limited to a single language or fail to capture language-specific factors shaping intelligibility. We present a multilingual phoneme-production assessment framework that integrates universal phone recognition with language-specific phoneme interpretation using contrastive phonological feature distances for phone-to-phoneme mapping and sequence alignment. The framework yields three metrics: phoneme error rate (PER), phonological feature error rate (PFER), and a newly proposed alignment-free measure, phoneme coverage (PhonCov). Analysis on English, Spanish, Italian, and Tamil show that PER benefits from the combination of mapping and alignment, PFER from alignment alone, and PhonCov from mapping. Further analyses demonstrate that the proposed framework captures clinically meaningful patterns of intelligibility degradation consistent with established observations of dysarthric speech.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.CL (NLP)",
      "source_id": "arxiv_cs_cl",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "NLP",
        "LLM",
        "language models",
        "transformers"
      ]
    },
    {
      "title": "Scaling Reasoning Hop Exposes Weaknesses: Demystifying and Improving Hop Generalization in Large Language Models",
      "url": "https://arxiv.org/abs/2601.21214",
      "content": "arXiv:2601.21214v1 Announce Type: new \nAbstract: Chain-of-thought (CoT) reasoning has become the standard paradigm for enabling Large Language Models (LLMs) to solve complex problems. However, recent studies reveal a sharp performance drop in reasoning hop generalization scenarios, where the required number of reasoning steps exceeds training distributions while the underlying algorithm remains unchanged. The internal mechanisms driving this failure remain poorly understood. In this work, we conduct a systematic study on tasks from multiple domains, and find that errors concentrate at token positions of a few critical error types, rather than being uniformly distributed. Closer inspection reveals that these token-level erroneous predictions stem from internal competition mechanisms: certain attention heads, termed erroneous processing heads (ep heads), tip the balance by amplifying incorrect reasoning trajectories while suppressing correct ones. Notably, removing individual ep heads during inference can often restore the correct predictions. Motivated by these insights, we propose test-time correction of reasoning, a lightweight intervention method that dynamically identifies and deactivates ep heads in the reasoning process. Extensive experiments across different tasks and LLMs show that it consistently improves reasoning hop generalization, highlighting both its effectiveness and potential.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.CL (NLP)",
      "source_id": "arxiv_cs_cl",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "NLP",
        "LLM",
        "language models",
        "transformers"
      ]
    },
    {
      "title": "Parametric Knowledge is Not All You Need: Toward Honest Large Language Models via Retrieval of Pretraining Data",
      "url": "https://arxiv.org/abs/2601.21218",
      "content": "arXiv:2601.21218v1 Announce Type: new \nAbstract: Large language models (LLMs) are highly capable of answering questions, but they are often unaware of their own knowledge boundary, i.e., knowing what they know and what they don't know. As a result, they can generate factually incorrect responses on topics they do not have enough knowledge of, commonly known as hallucination. Rather than hallucinating, a language model should be more honest and respond with \"I don't know\" when it does not have enough knowledge about a topic. Many methods have been proposed to improve LLM honesty, but their evaluations lack robustness, as they do not take into account the knowledge that the LLM has ingested during its pretraining. In this paper, we propose a more robust evaluation benchmark dataset for LLM honesty by utilizing Pythia, a truly open LLM with publicly available pretraining data. In addition, we also propose a novel method for harnessing the pretraining data to build a more honest LLM.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.CL (NLP)",
      "source_id": "arxiv_cs_cl",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "NLP",
        "LLM",
        "language models",
        "transformers"
      ]
    },
    {
      "title": "MGSM-Pro: A Simple Strategy for Robust Multilingual Mathematical Reasoning Evaluation",
      "url": "https://arxiv.org/abs/2601.21225",
      "content": "arXiv:2601.21225v1 Announce Type: new \nAbstract: Large language models have made substantial progress in mathematical reasoning. However, benchmark development for multilingual evaluation has lagged behind English in both difficulty and recency. Recently, GSM-Symbolic showed a strong evidence of high variance when models are evaluated on different instantiations of the same question; however, the evaluation was conducted only in English. In this paper, we introduce MGSM-Pro, an extension of MGSM dataset with GSM-Symbolic approach. Our dataset provides five instantiations per MGSM question by varying names, digits and irrelevant context. Evaluations across nine languages reveal that many low-resource languages suffer large performance drops when tested on digit instantiations different from those in the original test set. We further find that some proprietary models, notably Gemini 2.5 Flash and GPT-4.1, are less robust to digit instantiation, whereas Claude 4.0 Sonnet is more robust. Among open models, GPT-OSS 120B and DeepSeek V3 show stronger robustness. Based on these findings, we recommend evaluating each problem using at least five digit-varying instantiations to obtain a more robust and realistic assessment of math reasoning.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.CL (NLP)",
      "source_id": "arxiv_cs_cl",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "NLP",
        "LLM",
        "language models",
        "transformers"
      ]
    },
    {
      "title": "SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models",
      "url": "https://arxiv.org/abs/2601.21235",
      "content": "arXiv:2601.21235v1 Announce Type: new \nAbstract: Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar scores, thereby obscuring distributional structure, cross-dimensional interactions, and worst-case behavior. This paper introduces Social Harm Analysis via Risk Profiles (SHARP), a framework for multidimensional, distribution-aware evaluation of social harm. SHARP models harm as a multivariate random variable and integrates explicit decomposition into bias, fairness, ethics, and epistemic reliability with a union-of-failures aggregation reparameterized as additive cumulative log-risk. The framework further employs risk-sensitive distributional statistics, with Conditional Value at Risk (CVaR95) as a primary metric, to characterize worst-case model behavior. Application of SHARP to eleven frontier LLMs, evaluated on a fixed corpus of n=901 socially sensitive prompts, reveals that models with similar average risk can exhibit more than twofold differences in tail exposure and volatility. Across models, dimension-wise marginal tail behavior varies systematically across harm dimensions, with bias exhibiting the strongest tail severities, epistemic and fairness risks occupying intermediate regimes, and ethical misalignment consistently lower; together, these patterns reveal heterogeneous, model-dependent failure structures that scalar benchmarks conflate. These findings indicate that responsible evaluation and governance of LLMs require moving beyond scalar averages toward multidimensional, tail-sensitive risk profiling.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.CL (NLP)",
      "source_id": "arxiv_cs_cl",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "NLP",
        "LLM",
        "language models",
        "transformers"
      ]
    },
    {
      "title": "MoCo: A One-Stop Shop for Model Collaboration Research",
      "url": "https://arxiv.org/abs/2601.21257",
      "content": "arXiv:2601.21257v1 Announce Type: new \nAbstract: Advancing beyond single monolithic language models (LMs), recent research increasingly recognizes the importance of model collaboration, where multiple LMs collaborate, compose, and complement each other. Existing research on this topic has mostly been disparate and disconnected, from different research communities, and lacks rigorous comparison. To consolidate existing research and establish model collaboration as a school of thought, we present MoCo: a one-stop Python library of executing, benchmarking, and comparing model collaboration algorithms at scale. MoCo features 26 model collaboration methods, spanning diverse levels of cross-model information exchange such as routing, text, logit, and model parameters. MoCo integrates 25 evaluation datasets spanning reasoning, QA, code, safety, and more, while users could flexibly bring their own data. Extensive experiments with MoCo demonstrate that most collaboration strategies outperform models without collaboration in 61.0% of (model, data) settings on average, with the most effective methods outperforming by up to 25.8%. We further analyze the scaling of model collaboration strategies, the training/inference efficiency of diverse methods, highlight that the collaborative system solves problems where single LMs struggle, and discuss future work in model collaboration, all made possible by MoCo. We envision MoCo as a valuable toolkit to facilitate and turbocharge the quest for an open, modular, decentralized, and collaborative AI future.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.CL (NLP)",
      "source_id": "arxiv_cs_cl",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "NLP",
        "LLM",
        "language models",
        "transformers"
      ]
    },
    {
      "title": "CausalEmbed: Auto-Regressive Multi-Vector Generation in Latent Space for Visual Document Embedding",
      "url": "https://arxiv.org/abs/2601.21262",
      "content": "arXiv:2601.21262v1 Announce Type: new \nAbstract: Although Multimodal Large Language Models (MLLMs) have shown remarkable potential in Visual Document Retrieval (VDR) through generating high-quality multi-vector embeddings, the substantial storage overhead caused by representing a page with thousands of visual tokens limits their practicality in real-world applications. To address this challenge, we propose an auto-regressive generation approach, CausalEmbed, for constructing multi-vector embeddings. By incorporating iterative margin loss during contrastive training, CausalEmbed encourages the embedding models to learn compact and well-structured representations. Our method enables efficient VDR tasks using only dozens of visual tokens, achieving a 30-155x reduction in token count while maintaining highly competitive performance across various backbones and benchmarks. Theoretical analysis and empirical results demonstrate the unique advantages of auto-regressive embedding generation in terms of training efficiency and scalability at test time. As a result, CausalEmbed introduces a flexible test-time scaling strategy for multi-vector VDR representations and sheds light on the generative paradigm within multimodal document retrieval.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.CL (NLP)",
      "source_id": "arxiv_cs_cl",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "NLP",
        "LLM",
        "language models",
        "transformers"
      ]
    },
    {
      "title": "Qwen3-ASR Technical Report",
      "url": "https://arxiv.org/abs/2601.21337",
      "content": "arXiv:2601.21337v1 Announce Type: new \nAbstract: In this report, we introduce Qwen3-ASR family, which includes two powerful all-in-one speech recognition models and a novel non-autoregressive speech forced alignment model. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B are ASR models that support language identification and ASR for 52 languages and dialects. Both of them leverage large-scale speech training data and the strong audio understanding ability of their foundation model Qwen3-Omni. We conduct comprehensive internal evaluation besides the open-sourced benchmarks as ASR models might differ little on open-sourced benchmark scores but exhibit significant quality differences in real-world scenarios. The experiments reveal that the 1.7B version achieves SOTA performance among open-sourced ASR models and is competitive with the strongest proprietary APIs while the 0.6B version offers the best accuracy-efficiency trade-off. Qwen3-ASR-0.6B can achieve an average TTFT as low as 92ms and transcribe 2000 seconds speech in 1 second at a concurrency of 128. Qwen3-ForcedAligner-0.6B is an LLM based NAR timestamp predictor that is able to align text-speech pairs in 11 languages. Timestamp accuracy experiments show that the proposed model outperforms the three strongest force alignment models and takes more advantages in efficiency and versatility. To further accelerate the community research of ASR and audio understanding, we release these models under the Apache 2.0 license.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.CL (NLP)",
      "source_id": "arxiv_cs_cl",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "NLP",
        "LLM",
        "language models",
        "transformers"
      ]
    },
    {
      "title": "Self-Improving Pretraining: using post-trained models to pretrain better models",
      "url": "https://arxiv.org/abs/2601.21343",
      "content": "arXiv:2601.21343v1 Announce Type: new \nAbstract: Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model's core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.",
      "published_date": "2026-01-30T05:00:00",
      "source": "arXiv cs.CL (NLP)",
      "source_id": "arxiv_cs_cl",
      "language": "en",
      "credibility_score": 10,
      "relevance_weight": 9,
      "focus_tags": [
        "NLP",
        "LLM",
        "language models",
        "transformers"
      ]
    }
  ]
}