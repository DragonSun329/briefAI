# AI Briefing Agent - Implementation Guide for Claude Code

This guide provides step-by-step instructions for building the AI briefing agent system using Claude Code and Cursor.

---

## Prerequisites

### Required Tools:
- Python 3.10 or higher
- Claude Code CLI
- Cursor IDE (or VS Code with Cursor)
- Anthropic API key

### Required Python Packages:
```txt
anthropic>=0.25.0
requests>=2.31.0
beautifulsoup4>=4.12.0
lxml>=4.9.0
feedparser>=6.0.10
playwright>=1.40.0
python-dateutil>=2.8.2
markdownify>=0.11.6
jinja2>=3.1.2
pyyaml>=6.0
python-dotenv>=1.0.0
```

---

## Phase 1: Project Setup (Day 1)

### Step 1.1: Initialize Project Structure

```bash
# Create project directory
mkdir ai_briefing_agent
cd ai_briefing_agent

# Create subdirectories
mkdir -p config modules utils data/{cache,reports} logs

# Initialize git
git init
```

### Step 1.2: Create Configuration Files

**File: `.env`**
```env
ANTHROPIC_API_KEY=your_api_key_here
DEFAULT_CATEGORIES=Â§ßÊ®°Âûã,AIÂ∫îÁî®,ÊîøÁ≠ñÁõëÁÆ°
REPORT_OUTPUT_DIR=./data/reports
CACHE_DIR=./data/cache
LOG_LEVEL=INFO
```

**File: `config/sources.json`**
```json
{
  "sources": [
    {
      "id": "jiqizhixin",
      "name": "Êú∫Âô®‰πãÂøÉ",
      "url": "https://www.jiqizhixin.com",
      "type": "rss",
      "rss_url": "https://www.jiqizhixin.com/rss",
      "enabled": true,
      "categories": ["Â§ßÊ®°Âûã", "AIÂ∫îÁî®", "Á†îÁ©∂Á™ÅÁ†¥"],
      "language": "zh-CN",
      "credibility_score": 9
    },
    {
      "id": "qbitai",
      "name": "ÈáèÂ≠ê‰Ωç",
      "url": "https://www.qbitai.com",
      "type": "web",
      "enabled": true,
      "categories": ["Â§ßÊ®°Âûã", "AIÂ∫îÁî®", "‰ºÅ‰∏öÂä®ÊÄÅ"],
      "language": "zh-CN",
      "credibility_score": 8
    }
  ]
}
```

**File: `config/categories.json`**
```json
{
  "categories": [
    {
      "id": "llm",
      "name_zh": "Â§ßÊ®°Âûã",
      "name_en": "Large Language Models",
      "keywords": ["GPT", "Claude", "Â§ßÊ®°Âûã", "ËØ≠Ë®ÄÊ®°Âûã", "LLM", "Gemini"],
      "priority_default": 9
    },
    {
      "id": "application",
      "name_zh": "AIÂ∫îÁî®",
      "name_en": "AI Applications",
      "keywords": ["Â∫îÁî®", "ËêΩÂú∞", "‰∫ßÂìÅ", "Â∑•ÂÖ∑", "Â∫îÁî®Âú∫ÊôØ"],
      "priority_default": 8
    },
    {
      "id": "infrastructure",
      "name_zh": "AIÂü∫Á°ÄËÆæÊñΩ",
      "name_en": "AI Infrastructure",
      "keywords": ["ËäØÁâá", "GPU", "ÁÆóÂäõ", "‰∫ëËÆ°ÁÆó", "ËÆ≠ÁªÉ"],
      "priority_default": 7
    },
    {
      "id": "policy",
      "name_zh": "ÊîøÁ≠ñÁõëÁÆ°",
      "name_en": "Policy & Regulation",
      "keywords": ["ÊîøÁ≠ñ", "ÁõëÁÆ°", "Ê≥ïËßÑ", "‰º¶ÁêÜ", "Ê≤ªÁêÜ"],
      "priority_default": 7
    },
    {
      "id": "funding",
      "name_zh": "Ë°å‰∏öËûçËµÑ",
      "name_en": "Industry Funding",
      "keywords": ["ËûçËµÑ", "ÊäïËµÑ", "Êî∂Ë¥≠", "IPO", "‰º∞ÂÄº"],
      "priority_default": 6
    },
    {
      "id": "research",
      "name_zh": "Á†îÁ©∂Á™ÅÁ†¥",
      "name_en": "Research Breakthroughs",
      "keywords": ["ËÆ∫Êñá", "Á†îÁ©∂", "Á™ÅÁ†¥", "Â≠¶ÊúØ", "ÊäÄÊúØ"],
      "priority_default": 8
    },
    {
      "id": "company",
      "name_zh": "‰ºÅ‰∏öÂä®ÊÄÅ",
      "name_en": "Company Developments",
      "keywords": ["ÂÖ¨Âè∏", "‰ºÅ‰∏ö", "Âêà‰Ωú", "ÊàòÁï•", "‰∫∫‰∫ã"],
      "priority_default": 6
    }
  ]
}
```

**File: `config/report_template.md`**
```markdown
# AIË°å‰∏öÂä®ÊÄÅÂë®Êä•
**Êä•ÂëäÊó•Êúü**: {{ report_date }}
**Êä•ÂëäÂë®Êúü**: {{ week_start }} - {{ week_end }}

---

## üìä Êú¨Âë®Ê¶ÇËßà

{{ executive_summary }}

---

{% for category in categories %}
## {{ category.icon }} {{ category.name }}

{% for article in category.articles %}
### {{ loop.index }}. {{ article.title }}
**Êù•Ê∫ê**: {{ article.source }} | **Êó•Êúü**: {{ article.date }}

{{ article.summary }}

**ÂéüÊñáÈìæÊé•**: [Êü•ÁúãËØ¶ÊÉÖ]({{ article.url }})

---

{% endfor %}
{% endfor %}

## üí° ÂÖ≥ÈîÆÊ¥ûÂØü

{{ key_insights }}

---

## üîó Âª∂‰º∏ÈòÖËØª

{% for article in extended_reading %}
- [{{ article.title }}]({{ article.url }}) - {{ article.source }}
{% endfor %}

---

*Êú¨Êä•ÂëäÁî±AIÊô∫ËÉΩ‰ΩìËá™Âä®ÁîüÊàêÂíåÊï¥ÁêÜ | ÁîüÊàêÊó∂Èó¥: {{ generation_time }}*
```

---

## Phase 2: Build Core Modules (Days 2-5)

### Step 2.1: Claude API Client (`utils/claude_client.py`)

**Purpose**: Wrapper for Anthropic Claude API with retry logic and caching

**Key Functions**:
- `call_claude(prompt, system_prompt, model='claude-sonnet-4.5')` 
- `call_claude_with_retry()`
- `parse_json_response()`

**Implementation Notes**:
```python
import anthropic
import os
from typing import Dict, Any, Optional
import json
import time

class ClaudeClient:
    def __init__(self):
        self.client = anthropic.Anthropic(
            api_key=os.getenv('ANTHROPIC_API_KEY')
        )
        self.default_model = 'claude-sonnet-4-5-20250929'
    
    def call_claude(
        self, 
        prompt: str, 
        system_prompt: str = "",
        max_tokens: int = 4000,
        temperature: float = 0.7
    ) -> str:
        """Call Claude API with standard parameters"""
        # Implement with retry logic
        # Add caching for system prompts
        # Log all calls for debugging
        pass
    
    def call_with_json_response(
        self,
        prompt: str,
        system_prompt: str = ""
    ) -> Dict[Any, Any]:
        """Call Claude and parse JSON response"""
        # Add JSON parsing with validation
        # Handle malformed JSON gracefully
        pass
```

**Testing**:
```python
# Test file: tests/test_claude_client.py
def test_basic_call():
    client = ClaudeClient()
    response = client.call_claude("Say hello in Chinese")
    assert "‰Ω†Â•Ω" in response or "ÊÇ®Â•Ω" in response
```

---

### Step 2.2: Category Selector Module (`modules/category_selector.py`)

**Purpose**: Parse user preferences and select relevant categories

**Key Functions**:
- `select_categories(user_input: str) -> List[Dict]`
- `parse_preferences(input: str) -> Dict`
- `get_default_categories() -> List[Dict]`

**Implementation Steps**:
1. Load categories from `config/categories.json`
2. Use Claude to interpret user input (use prompt from templates doc)
3. Return structured category list with priorities
4. Handle empty/vague input with sensible defaults

**Example Usage**:
```python
from modules.category_selector import CategorySelector

selector = CategorySelector()
result = selector.select_categories(
    "ÊàëÊÉ≥‰∫ÜËß£Â§ßÊ®°ÂûãÁöÑÊúÄÊñ∞ËøõÂ±ïÂíåAIÂ∫îÁî®ÁöÑËêΩÂú∞ÊÉÖÂÜµ"
)

# Returns:
# [
#   {'id': 'llm', 'name': 'Â§ßÊ®°Âûã', 'priority': 10, 'keywords': [...]},
#   {'id': 'application', 'name': 'AIÂ∫îÁî®', 'priority': 9, 'keywords': [...]}
# ]
```

**Testing Scenarios**:
- Clear input: "Â§ßÊ®°ÂûãÂíåÊîøÁ≠ñÁõëÁÆ°"
- Vague input: "ÊúÄËøëAIÊúâ‰ªÄ‰πàÊñ∞Èóª"
- English input: "LLM developments and AI applications"
- Empty input: ""

---

### Step 2.3: Web Scraper Module (`modules/web_scraper.py`)

**Purpose**: Collect articles from configured sources

**Key Classes**:
- `BaseScraper` (abstract base class)
- `RSSScaper` (for RSS feeds)
- `WebScraper` (for HTML scraping)
- `ScraperOrchestrator` (manages all scrapers)

**Implementation Priority**:

**Phase 1 - RSS Scraper (Easiest)**:
```python
import feedparser
from datetime import datetime, timedelta

class RSSScraper:
    def scrape(self, source_config: Dict) -> List[Dict]:
        """
        Scrape RSS feed and return article list
        
        Returns format:
        [
            {
                'title': str,
                'url': str,
                'source': str,
                'published_date': datetime,
                'content': str,
                'summary': str
            }
        ]
        """
        feed = feedparser.parse(source_config['rss_url'])
        articles = []
        
        for entry in feed.entries:
            # Filter by date (last 7 days)
            # Extract content
            # Clean HTML
            articles.append({...})
        
        return articles
```

**Phase 2 - Web Scraper (More Complex)**:
- Use `requests` + `BeautifulSoup4`
- Implement selectors for each source
- Handle pagination if needed
- Respect robots.txt and rate limits

**Phase 3 - Dynamic Content Scraper**:
- Use `playwright` for JavaScript-heavy sites
- More complex but handles modern web apps

**Scraper Orchestrator**:
```python
class ScraperOrchestrator:
    def __init__(self):
        self.sources = self.load_sources()
        self.rss_scraper = RSSScraper()
        self.web_scraper = WebScraper()
    
    def scrape_all(
        self, 
        categories: List[str],
        days_back: int = 7
    ) -> List[Dict]:
        """
        Scrape all enabled sources for given categories
        """
        all_articles = []
        
        for source in self.sources:
            if not source['enabled']:
                continue
            
            # Check if source covers requested categories
            if not any(cat in source['categories'] for cat in categories):
                continue
            
            # Use appropriate scraper
            if source['type'] == 'rss':
                articles = self.rss_scraper.scrape(source)
            else:
                articles = self.web_scraper.scrape(source)
            
            all_articles.extend(articles)
        
        return self.deduplicate(all_articles)
```

**Caching Strategy**:
```python
import hashlib
import json
from pathlib import Path

class ArticleCache:
    def __init__(self, cache_dir='./data/cache'):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
    
    def get_cache_key(self, url: str) -> str:
        return hashlib.md5(url.encode()).hexdigest()
    
    def is_cached(self, url: str) -> bool:
        cache_file = self.cache_dir / f"{self.get_cache_key(url)}.json"
        return cache_file.exists()
    
    def get(self, url: str) -> Optional[Dict]:
        if not self.is_cached(url):
            return None
        cache_file = self.cache_dir / f"{self.get_cache_key(url)}.json"
        return json.loads(cache_file.read_text(encoding='utf-8'))
    
    def set(self, url: str, article: Dict):
        cache_file = self.cache_dir / f"{self.get_cache_key(url)}.json"
        cache_file.write_text(
            json.dumps(article, ensure_ascii=False, indent=2),
            encoding='utf-8'
        )
```

**Error Handling**:
- Log failed scrapes to `logs/scraper_errors.log`
- Continue with partial results
- Implement exponential backoff for retries
- Handle encoding issues (especially for Chinese content)

---

### Step 2.4: News Evaluator Module (`modules/news_evaluator.py`)

**Purpose**: Score and rank articles by importance

**Key Functions**:
- `evaluate_article(article: Dict, context: Dict) -> Dict`
- `rank_articles(articles: List[Dict]) -> List[Dict]`
- `get_top_articles(articles: List[Dict], n: int = 15) -> List[Dict]`

**Implementation**:
```python
class NewsEvaluator:
    def __init__(self, claude_client: ClaudeClient):
        self.claude = claude_client
        self.system_prompt = self.load_evaluator_prompt()
    
    def evaluate_article(
        self, 
        article: Dict,
        ceo_interests: List[str],
        company_context: str = ""
    ) -> Dict:
        """
        Evaluate single article and return scores
        
        Returns:
        {
            'article_url': str,
            'scores': {
                'impact': int,
                'relevance': int,
                'recency': int,
                'credibility': int
            },
            'composite_score': float,
            'rationale': str,
            'key_takeaway': str,
            'priority': str,  # high/medium/low
            'tags': List[str]
        }
        """
        prompt = self.build_evaluation_prompt(
            article, ceo_interests, company_context
        )
        
        response = self.claude.call_with_json_response(
            prompt, self.system_prompt
        )
        
        # Calculate composite score
        scores = response['scores']
        composite = (
            scores['impact'] * 0.4 +
            scores['relevance'] * 0.3 +
            scores['recency'] * 0.2 +
            scores['credibility'] * 0.1
        )
        
        response['composite_score'] = composite
        response['article_url'] = article['url']
        
        return response
    
    def batch_evaluate(
        self,
        articles: List[Dict],
        ceo_interests: List[str],
        company_context: str = ""
    ) -> List[Dict]:
        """Evaluate multiple articles with progress tracking"""
        evaluations = []
        
        for i, article in enumerate(articles):
            print(f"Evaluating article {i+1}/{len(articles)}...")
            eval_result = self.evaluate_article(
                article, ceo_interests, company_context
            )
            evaluations.append(eval_result)
            
            # Rate limiting - space out API calls
            time.sleep(0.5)
        
        return evaluations
    
    def rank_articles(
        self,
        evaluations: List[Dict]
    ) -> List[Dict]:
        """Sort by composite score, descending"""
        return sorted(
            evaluations,
            key=lambda x: x['composite_score'],
            reverse=True
        )
```

**Optimization Tips**:
- Batch multiple articles in one API call if content is short
- Cache evaluations by article URL
- Skip re-evaluation if article was evaluated recently

---

### Step 2.5: Article Paraphraser Module (`modules/article_paraphraser.py`)

**Purpose**: Condense articles into executive summaries

**Key Functions**:
- `paraphrase_article(article: Dict, evaluation: Dict) -> str`
- `verify_accuracy(original: Dict, paraphrased: str) -> Dict`
- `batch_paraphrase(articles: List[Dict]) -> List[Dict]`

**Implementation**:
```python
class ArticleParaphraser:
    def __init__(self, claude_client: ClaudeClient):
        self.claude = claude_client
        self.paraphrase_prompt = self.load_paraphrase_prompt()
        self.quality_checker_prompt = self.load_quality_prompt()
    
    def paraphrase_article(
        self,
        article: Dict,
        target_length: int = 200  # Chinese characters
    ) -> Dict:
        """
        Paraphrase article into executive summary
        
        Returns:
        {
            'original_title': str,
            'paraphrased_title': str,  # May be cleaned up
            'summary': str,  # The paraphrased content
            'source': str,
            'date': str,
            'url': str,
            'word_count': int
        }
        """
        # Build paraphrase prompt
        prompt = self.paraphrase_prompt.format(
            title=article['title'],
            source=article['source'],
            date=article['published_date'].strftime('%YÂπ¥%mÊúà%dÊó•'),
            full_content=article['content'][:5000]  # Truncate if too long
        )
        
        # Get paraphrased content
        paraphrased = self.claude.call_claude(
            prompt,
            self.system_prompt,
            max_tokens=1000
        )
        
        # Verify quality
        quality_check = self.verify_accuracy(article, paraphrased)
        
        if quality_check['approval_status'] != 'approved':
            # Retry with corrections
            paraphrased = self.retry_with_corrections(
                article, paraphrased, quality_check
            )
        
        return {
            'original_title': article['title'],
            'paraphrased_title': self.clean_title(article['title']),
            'summary': paraphrased,
            'source': article['source'],
            'date': article['published_date'].strftime('%mÊúà%dÊó•'),
            'url': article['url'],
            'word_count': len(paraphrased)
        }
    
    def verify_accuracy(
        self,
        original_article: Dict,
        paraphrased_content: str
    ) -> Dict:
        """Use Claude to verify paraphrase quality"""
        # Extract key facts from original
        key_facts = self.extract_key_facts(original_article)
        
        # Build verification prompt
        prompt = self.quality_checker_prompt.format(
            original_key_points=key_facts,
            paraphrased_content=paraphrased_content
        )
        
        quality_result = self.claude.call_with_json_response(
            prompt,
            system_prompt="You verify content quality and accuracy."
        )
        
        return quality_result
```

**Quality Assurance**:
- Always verify paraphrased content doesn't hallucinate
- Check that key numbers and names are preserved
- Ensure output is in proper Mandarin
- Validate length is within target range

---

### Step 2.6: Report Formatter Module (`modules/report_formatter.py`)

**Purpose**: Compile everything into final report

**Key Functions**:
- `generate_report(articles: List[Dict], metadata: Dict) -> str`
- `generate_executive_summary(articles: List[Dict]) -> str`
- `generate_key_insights(articles: List[Dict]) -> str`
- `export_to_html(markdown: str) -> str`
- `export_to_pdf(markdown: str, output_path: str)`

**Implementation**:
```python
from jinja2 import Template
from datetime import datetime

class ReportFormatter:
    def __init__(self, claude_client: ClaudeClient):
        self.claude = claude_client
        self.template = self.load_template()
    
    def generate_report(
        self,
        articles_by_category: Dict[str, List[Dict]],
        week_start: datetime,
        week_end: datetime
    ) -> str:
        """
        Generate complete weekly report
        
        Args:
            articles_by_category: {
                'Â§ßÊ®°Âûã': [article1, article2, ...],
                'AIÂ∫îÁî®': [article3, article4, ...]
            }
        """
        # Generate executive summary
        exec_summary = self.generate_executive_summary(
            articles_by_category
        )
        
        # Generate key insights
        key_insights = self.generate_key_insights(
            articles_by_category
        )
        
        # Format categories with icons
        formatted_categories = []
        category_icons = {
            'Â§ßÊ®°Âûã': 'ü§ñ',
            'AIÂ∫îÁî®': 'üíº',
            'AIÂü∫Á°ÄËÆæÊñΩ': '‚öôÔ∏è',
            'ÊîøÁ≠ñÁõëÁÆ°': 'üìã',
            'Ë°å‰∏öËûçËµÑ': 'üí∞',
            'Á†îÁ©∂Á™ÅÁ†¥': 'üî¨',
            '‰ºÅ‰∏öÂä®ÊÄÅ': 'üè¢'
        }
        
        for category_name, articles in articles_by_category.items():
            formatted_categories.append({
                'name': category_name,
                'icon': category_icons.get(category_name, 'üìå'),
                'articles': articles
            })
        
        # Render template
        report = self.template.render(
            report_date=datetime.now().strftime('%YÂπ¥%mÊúà%dÊó•'),
            week_start=week_start.strftime('%mÊúà%dÊó•'),
            week_end=week_end.strftime('%mÊúà%dÊó•'),
            executive_summary=exec_summary,
            categories=formatted_categories,
            key_insights=key_insights,
            extended_reading=self.get_extended_reading(articles_by_category),
            generation_time=datetime.now().strftime('%Y-%m-%d %H:%M')
        )
        
        return report
    
    def generate_executive_summary(
        self,
        articles_by_category: Dict[str, List[Dict]]
    ) -> str:
        """Use Claude to generate 2-3 sentence overview"""
        # Collect all article titles and key takeaways
        summary_data = []
        for category, articles in articles_by_category.items():
            for article in articles:
                summary_data.append(
                    f"- {category}: {article['paraphrased_title']}"
                )
        
        prompt = f"""Âü∫‰∫éÊú¨Âë®ÈÄâ‰∏≠ÁöÑËøô‰∫õAIÊñ∞ÈóªÊñáÁ´†ÔºåÂÜô‰∏Ä‰∏™2-3Âè•ËØùÁöÑÈ´òÂ±ÇÊ¨°ÊÄªÁªìÔºö

{chr(10).join(summary_data)}

Ë¶ÅÊ±ÇÔºö
- ËØÜÂà´Êú¨Âë®ÁöÑ‰∏ªË¶ÅË∂ãÂäøÂíåÈáçË¶ÅÂèëÂ±ï
- Èù¢ÂêëCEOÔºåÈ´òÂ∫¶Ê¶ÇÊã¨Ôºå‰∏çË¶ÅÁªÜËäÇ
- Áî®Ê∏ÖÊô∞„ÄÅ‰∏ì‰∏öÁöÑ‰∏≠Êñá
- ÂÖ∑‰ΩìÊåáÂá∫ÂèëÂ±ïÂÜÖÂÆπÔºå‰∏çË¶ÅÁ©∫Ê≥õ

ËæìÂá∫Ê†ºÂºèÔºö
Êú¨Âë®AIË°å‰∏öÂëàÁé∞[‰∏ªË¶ÅË∂ãÂäø]ÁöÑÁâπÁÇπ„ÄÇ[ÂÖ∑‰ΩìÂèëÂ±ï1]ÂºïËµ∑ÂπøÊ≥õÂÖ≥Ê≥®ÔºåÂêåÊó∂[ÂÖ∑‰ΩìÂèëÂ±ï2]‰πüÊ†áÂøóÁùÄ[Êüê‰∏™ÊñπÂêë]ÁöÑÈáçË¶ÅËøõÂ±ï„ÄÇ
"""
        
        summary = self.claude.call_claude(
            prompt,
            system_prompt="You craft executive summaries for CEO briefings."
        )
        
        return summary.strip()
    
    def generate_key_insights(
        self,
        articles_by_category: Dict[str, List[Dict]]
    ) -> str:
        """Generate 3-5 strategic insights connecting the dots"""
        # Build context from all articles
        articles_summary = []
        for category, articles in articles_by_category.items():
            for article in articles:
                articles_summary.append(
                    f"{category} - {article['paraphrased_title']}: "
                    f"{article['summary'][:200]}..."
                )
        
        prompt = f"""Âü∫‰∫éÊú¨Âë®ÁöÑAIÊñ∞ÈóªÔºåÁîüÊàê3-5‰∏™ÂÖ≥ÈîÆÊàòÁï•Ê¥ûÂØüÔºö

**Êú¨Âë®ÊñáÁ´†**:
{chr(10).join(articles_summary)}

ÊØè‰∏™Ê¥ûÂØüÂ∫îËØ•Ôºö
- 2-3Âè•ËØùÁöÑ‰∏≠Êñá
- ËøûÊé•Â§ö‰∏™ÂèëÂ±ïÊàñËØÜÂà´Ë∂ãÂäø
- ÂØπÂïÜ‰∏öÈ¢ÜÂØºËÄÖÊúâÊàòÁï•Áõ∏ÂÖ≥ÊÄß
- ÂÆûË¥®ÊÄßÁöÑÔºå‰∏çÊòØË°®Èù¢ËßÇÂØü

ËæìÂá∫Ê†ºÂºèÔºö
1. [Ê¥ûÂØüÊ†áÈ¢ò]Ôºö[2-3Âè•Ëß£Èáä]

2. [Ê¥ûÂØüÊ†áÈ¢ò]Ôºö[2-3Âè•Ëß£Èáä]

...

Âü∫‰∫éÊú¨Âë®ÂÆûÈôÖÊñ∞ÈóªÁîüÊàêÂÖ∑‰ΩìÊ¥ûÂØüÔºå‰∏çÊòØÊ≥õÊ≥õÁöÑAIË°å‰∏öËßÇÂØü„ÄÇ
"""
        
        insights = self.claude.call_claude(
            prompt,
            system_prompt="You synthesize strategic insights from news."
        )
        
        return insights.strip()
```

---

## Phase 3: Main Orchestrator (Day 6)

### Step 3.1: Main Execution Script (`main.py`)

**Purpose**: Orchestrate the entire workflow

```python
#!/usr/bin/env python3
"""
AI Briefing Agent - Main Orchestrator
"""

import argparse
from datetime import datetime, timedelta
from pathlib import Path
import json

from modules.category_selector import CategorySelector
from modules.web_scraper import ScraperOrchestrator
from modules.news_evaluator import NewsEvaluator
from modules.article_paraphraser import ArticleParaphraser
from modules.report_formatter import ReportFormatter
from utils.claude_client import ClaudeClient
from utils.logger import setup_logger

def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description='Generate AI industry weekly briefing'
    )
    parser.add_argument(
        '--interactive',
        action='store_true',
        help='Interactive mode - ask for category preferences'
    )
    parser.add_argument(
        '--categories',
        type=str,
        help='Comma-separated categories (e.g., "Â§ßÊ®°Âûã,AIÂ∫îÁî®")'
    )
    parser.add_argument(
        '--days',
        type=int,
        default=7,
        help='Number of days to look back (default: 7)'
    )
    parser.add_argument(
        '--top-n',
        type=int,
        default=15,
        help='Number of top articles to include (default: 15)'
    )
    parser.add_argument(
        '--output',
        type=str,
        help='Output file path (default: auto-generated)'
    )
    
    args = parser.parse_args()
    
    # Setup logging
    logger = setup_logger()
    logger.info("Starting AI Briefing Agent...")
    
    # Initialize components
    claude_client = ClaudeClient()
    category_selector = CategorySelector(claude_client)
    scraper = ScraperOrchestrator()
    evaluator = NewsEvaluator(claude_client)
    paraphraser = ArticleParaphraser(claude_client)
    formatter = ReportFormatter(claude_client)
    
    # Step 1: Select categories
    logger.info("Step 1: Selecting categories...")
    if args.interactive:
        print("\nÊ¨¢Ëøé‰ΩøÁî®AIË°å‰∏öÂä®ÊÄÅÁÆÄÊä•ÁîüÊàêÂô®ÔºÅ")
        print("ËØ∑ÊèèËø∞ÊÇ®ÊÉ≥‰∫ÜËß£Âì™‰∫õÊñπÈù¢ÁöÑAIËµÑËÆØÔºö")
        user_input = input("> ")
        categories = category_selector.select_categories(user_input)
    elif args.categories:
        # Parse comma-separated categories
        cat_list = [c.strip() for c in args.categories.split(',')]
        categories = category_selector.select_categories(
            f"ÊàëÊÉ≥‰∫ÜËß£{'„ÄÅ'.join(cat_list)}ÊñπÈù¢ÁöÑËµÑËÆØ"
        )
    else:
        # Use default categories
        categories = category_selector.get_default_categories()
    
    logger.info(f"Selected categories: {[c['name'] for c in categories]}")
    
    # Step 2: Scrape articles
    logger.info("Step 2: Scraping articles from sources...")
    category_names = [c['name'] for c in categories]
    articles = scraper.scrape_all(
        categories=category_names,
        days_back=args.days
    )
    logger.info(f"Collected {len(articles)} articles")
    
    if len(articles) == 0:
        logger.error("No articles found. Exiting.")
        return
    
    # Step 3: Evaluate articles
    logger.info("Step 3: Evaluating article importance...")
    ceo_interests = [c['name'] for c in categories]
    evaluations = evaluator.batch_evaluate(
        articles,
        ceo_interests=ceo_interests,
        company_context=""  # Can be customized
    )
    
    # Rank and select top articles
    ranked = evaluator.rank_articles(evaluations)
    top_articles = ranked[:args.top_n]
    logger.info(f"Selected top {len(top_articles)} articles")
    
    # Step 4: Paraphrase articles
    logger.info("Step 4: Paraphrasing articles...")
    paraphrased_articles = []
    for i, eval_result in enumerate(top_articles):
        # Find original article
        original = next(
            a for a in articles if a['url'] == eval_result['article_url']
        )
        
        logger.info(f"Paraphrasing {i+1}/{len(top_articles)}: {original['title']}")
        paraphrased = paraphraser.paraphrase_article(original)
        
        # Merge evaluation data with paraphrased content
        paraphrased['evaluation'] = eval_result
        paraphrased['category'] = self.determine_category(
            original, categories
        )
        
        paraphrased_articles.append(paraphrased)
    
    # Group by category
    articles_by_category = {}
    for article in paraphrased_articles:
        cat = article['category']
        if cat not in articles_by_category:
            articles_by_category[cat] = []
        articles_by_category[cat].append(article)
    
    # Step 5: Generate report
    logger.info("Step 5: Generating report...")
    week_end = datetime.now()
    week_start = week_end - timedelta(days=args.days)
    
    report_markdown = formatter.generate_report(
        articles_by_category,
        week_start,
        week_end
    )
    
    # Save report
    if args.output:
        output_path = Path(args.output)
    else:
        output_dir = Path('./data/reports')
        output_dir.mkdir(parents=True, exist_ok=True)
        filename = f"ai_briefing_{datetime.now().strftime('%Y%m%d')}.md"
        output_path = output_dir / filename
    
    output_path.write_text(report_markdown, encoding='utf-8')
    logger.info(f"Report saved to: {output_path}")
    
    print(f"\n‚úÖ Êä•ÂëäÁîüÊàêÂÆåÊàêÔºÅ")
    print(f"üìÑ Êñá‰ª∂‰ΩçÁΩÆ: {output_path}")
    print(f"üìä ÂåÖÂê´ÊñáÁ´†: {len(paraphrased_articles)} ÁØá")
    print(f"üìÇ Ê∂µÁõñÂàÜÁ±ª: {', '.join(articles_by_category.keys())}")

if __name__ == '__main__':
    main()
```

---

## Phase 4: Testing & Refinement (Day 7)

### Step 4.1: Unit Tests

Create tests for each module:

```bash
tests/
‚îú‚îÄ‚îÄ test_category_selector.py
‚îú‚îÄ‚îÄ test_web_scraper.py
‚îú‚îÄ‚îÄ test_news_evaluator.py
‚îú‚îÄ‚îÄ test_article_paraphraser.py
‚îî‚îÄ‚îÄ test_report_formatter.py
```

### Step 4.2: Integration Test

```python
# tests/test_integration.py

def test_end_to_end_small():
    """Test complete workflow with minimal data"""
    # Mock 5 sample articles
    # Run through entire pipeline
    # Verify output format
    pass
```

### Step 4.3: Manual Quality Checks

- [ ] Verify facts in paraphrased content
- [ ] Check Mandarin language quality
- [ ] Validate report formatting
- [ ] Test with different category combinations
- [ ] Ensure links work
- [ ] Check for hallucinations

---

## Phase 5: Deployment & Automation (Day 8+)

### Step 5.1: Schedule Weekly Execution

**Option 1: Cron (Linux/Mac)**
```bash
# Edit crontab
crontab -e

# Add line to run every Monday at 9 AM
0 9 * * 1 cd /path/to/ai_briefing_agent && /path/to/python main.py --use-saved-preferences
```

**Option 2: Windows Task Scheduler**
- Create scheduled task
- Run `main.py` weekly

**Option 3: GitHub Actions** (if code is in GitHub)
```yaml
# .github/workflows/weekly_briefing.yml
name: Generate Weekly Briefing

on:
  schedule:
    - cron: '0 9 * * 1'  # Every Monday 9 AM UTC
  workflow_dispatch:  # Allow manual trigger

jobs:
  generate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Generate report
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: python main.py --use-saved-preferences
      - name: Upload report
        uses: actions/upload-artifact@v2
        with:
          name: weekly-report
          path: data/reports/*.md
```

### Step 5.2: Email Integration (Optional)

```python
# utils/email_sender.py

import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

def send_report_email(
    report_content: str,
    recipient: str,
    subject: str = "AIË°å‰∏öÂä®ÊÄÅÂë®Êä•"
):
    """Send report via email"""
    # Convert Markdown to HTML
    html_content = markdown_to_html(report_content)
    
    # Create email
    msg = MIMEMultipart('alternative')
    msg['Subject'] = subject
    msg['From'] = 'your_email@example.com'
    msg['To'] = recipient
    
    # Attach HTML
    html_part = MIMEText(html_content, 'html')
    msg.attach(html_part)
    
    # Send via SMTP
    with smtplib.SMTP('smtp.gmail.com', 587) as server:
        server.starttls()
        server.login('your_email', 'your_password')
        server.send_message(msg)
```

---

## Troubleshooting Guide

### Common Issues:

**Issue 1: API Rate Limits**
- Solution: Add delays between API calls (`time.sleep(1)`)
- Use caching aggressively
- Consider upgrading API tier

**Issue 2: Scraping Failures**
- Solution: Implement robust retry logic
- Use multiple fallback sources
- Handle encoding issues with `chardet`

**Issue 3: Poor Paraphrase Quality**
- Solution: Refine prompts with examples
- Increase max_tokens if summaries are cut off
- Add quality verification step

**Issue 4: Chinese Encoding Issues**
- Solution: Always use `encoding='utf-8'`
- Verify source encoding with `chardet`
- Test with Chinese text in prompts

**Issue 5: Slow Execution**
- Solution: Implement parallel processing for scraping
- Cache aggressively
- Consider using faster Claude model for evaluation

---

## Performance Optimization

### Caching Strategy:
- Cache scraped articles for 24 hours
- Cache evaluations for 7 days
- Cache paraphrased content indefinitely

### Parallel Processing:
```python
from concurrent.futures import ThreadPoolExecutor

def batch_evaluate_parallel(articles, max_workers=5):
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [
            executor.submit(evaluate_article, article)
            for article in articles
        ]
        results = [f.result() for f in futures]
    return results
```

---

## Cost Estimation

### API Usage per Run:
- Category selection: 1 call (~500 tokens)
- Article evaluation: 50 calls (~25,000 tokens)
- Paraphrasing: 15 calls (~30,000 tokens)
- Report generation: 3 calls (~5,000 tokens)

**Total: ~60,000 tokens per weekly run**

At Claude Sonnet pricing (~$3 per million input tokens, ~$15 per million output tokens):
- **Cost per report: ~$0.50-$1.00**
- **Monthly cost: ~$2-$4**

Very affordable for weekly CEO briefings!

---

## Maintenance & Improvements

### Weekly Tasks:
- Review generated reports for quality
- Update source configurations if sites change
- Refine prompts based on output quality

### Monthly Tasks:
- Analyze which sources provide best content
- Add new high-quality sources
- Update category taxonomy if needed

### Quarterly Tasks:
- Review and optimize API usage
- Consider adding new features (email, PDF export)
- Gather CEO feedback and adjust

---

## Next Steps After MVP

1. **Add more sources** - Expand beyond initial 2-3 sources
2. **Web dashboard** - Build simple UI for easier interaction
3. **PDF export** - Generate professional PDF reports
4. **Preference learning** - Remember CEO's preferred topics
5. **Comparative analysis** - Track trends week-over-week
6. **Multi-language support** - Include English sources with translation

---

This implementation guide provides everything needed to build the AI briefing agent step by step. Start with Phase 1, build incrementally, test thoroughly, and iterate based on results.